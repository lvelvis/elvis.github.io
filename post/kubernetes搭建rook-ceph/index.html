<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
<meta name="keywords" content="lvelvis个人博客">
<meta name="description" content="时光,浓淡相宜;人心,远近相安;这就是最好的生活">
<meta name="theme-color" content="#000">
<title>kubernetes搭建rook-ceph | lvelvis</title>
<link rel="shortcut icon" href="/favicon.ico?v=1601281035249">
<link rel="stylesheet" href="/styles/main.css">
<link rel="stylesheet" href="/media/css/mist.css">

<link rel="stylesheet" href="/media/fonts/font-awesome.css">
<link
  href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Rosario:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext"
  rel="stylesheet" type="text/css">

<link href="/media/hljs/styles/androidstudio.css"
  rel="stylesheet">

<script src="/media/hljs/highlight.js"></script>
<script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.0/velocity.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.0/velocity.ui.min.js"></script>

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



  <meta name="description" content="kubernetes搭建rook-ceph" />
  <meta name="keywords" content="k8s,rook-ceph,ceph" />
</head>

<body>
  <div class="head-top-line"></div>
  <div class="header-box">
    
<div class="mist">
  <header class="header bg-color ">
    <div class="blog-header box-shadow-wrapper  " id="header">
      <div class="nav-toggle" id="nav_toggle">
        <div class="toggle-box">
          <div class="line line-top"></div>
          <div class="line line-center"></div>
          <div class="line line-bottom"></div>
        </div>
      </div>
      <div class="site-meta">       
        <div class="site-title">
          
            <a href="/" class="">
              <span class="logo-line-before">
                <i class=""></i>
              </span>
              <span class="main-title">lvelvis</span>
              <span class="logo-line-after">
                <i class=""></i>
              </span>
            </a>  
          
        </div>
        
      </div>
      <nav class="site-nav" id="site_nav">
        <ul id="nav_ul">
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/" target="_self">
                  <i class="fa fa-home"></i> 首页
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/archives/" target="_self">
                  <i class="fa fa-archive"></i> 归档
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/tags/" target="_self">
                  <i class="fa fa-tags"></i> 标签
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/post/about/" target="_self">
                  <i class="fa fa-user"></i> 关于
                </a>
              
            </li>
          
          
            
              <li class="nav-item ">
                <a href="/friends/" target="_self">
                  
                    <i class="fa fa-address-book"></i> 友情链接
                  
                </a>
              </li>
            
          
        </ul>
      </nav>
    </div>
  </header>
</div>

<script type="text/javascript"> 
 
  let showNav = true;

  let navToggle = document.querySelector('#nav_toggle'),
  siteNav = document.querySelector('#site_nav');
  
  function navClick() {
    let sideBar = document.querySelector('.sidebar');
    let navUl = document.querySelector('#nav_ul');
    navToggle.classList.toggle('nav-toggle-active');
    siteNav.classList.toggle('nav-menu-active');
    if (siteNav.classList.contains('nav-menu-active')) {
      siteNav.style = "height: " + (navUl.children.length * 42) +"px !important";
    } else {
      siteNav.style = "";
    }
  }

  navToggle.addEventListener('click',navClick);  
</script>
  </div>
  <div class="main-continer">
    
    <div
      class="section-layout mist bg-color">
      <div class="section-layout-wrapper">
        

<div class="sidebar">
  
    <div class="sidebar-box box-shadow-wrapper  right-motion" id="sidebar">
      
        <div class="post-list-sidebar">
          <div class="sidebar-title">
            <span id="tocSideBar" class="sidebar-title-item sidebar-title-active">文章目录</span>
            <span id="metaSideBar" class="sidebar-title-item">站点概览</span>
          </div>
        </div>
      
      <div class="sidebar-body mist" id="sidebar_body">
        
          
            <div class="post-side-meta" id="post_side_meta">
              
<div class="sidebar-wrapper box-shadow-wrapper ">
  <div class="sidebar-item">
    <img class="site-author-image right-motion" src="/images/avatar.png"/>
    <p class="site-author-name">lvelvis</p>
    
    <div class="site-description right-motion">
      
        <p id="binft">时光,浓淡相宜;人心,远近相安;这就是最好的生活</p>
      
    </div>
    
  </div>
  <div class="sidebar-item side-item-stat right-motion">
    <div class="sidebar-item-box">
      <a href="/archives/">
        
        <span class="site-item-stat-count">22</span>
        <span class="site-item-stat-name">文章</span>
      </a>
    </div>
    <div class="sidebar-item-box">
      <a href="">
        <span class="site-item-stat-count">22</span>
        <span class="site-item-stat-name">分类</span>
      </a>
    </div>
    <div class="sidebar-item-box">
      <a href="/tags/">
        <span class="site-item-stat-count">22</span>
        <span class="site-item-stat-name">标签</span>
      </a>
    </div>
  </div>
  
  
    <div class="sidebar-item sidebar-item-social">
      <div class="social-item">
        
          
            <a href="https://github.com/lvelvis">
              <i class="fa fa-github-alt" title="github"></i>
            </a>
          
            <a href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=421220622&amp;website=www.oicqzone.com&#34;&gt;">
              <i class="fa fa-qq" title="QQ"></i>
            </a>
          
        
        
          
            <a class="social-img" href="#">
              <img src="\media\images\custom-array-imgSocials-1588146527513-socialImg.jpg" />
              <i class="fa fa-wechat" title="weixin" ></i>
            </a>
          
        
      </div>
    </div>
  


</div>
            </div>
            <div class="post-toc sidebar-body-active" id="post_toc" style="opacity: 1;">
              <div class="toc-box right-motion">
  <div class="toc-wrapper auto-number auto"
    id="toc_wrapper">
    <ul class="markdownIt-TOC">
<li><a href="#%E7%AE%80%E4%BB%8B">简介</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83">环境</a></li>
<li><a href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C">准备工作</a></li>
<li><a href="#%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2operator">开始部署Operator</a>
<ul>
<li><a href="#%E9%83%A8%E7%BD%B2rook-operator">部署Rook Operator</a></li>
<li><a href="#%E6%9F%A5%E7%9C%8Boperator%E7%9A%84%E7%8A%B6%E6%80%81">查看Operator的状态</a></li>
<li><a href="#%E7%BB%99%E8%8A%82%E7%82%B9%E6%89%93%E6%A0%87%E7%AD%BE">给节点打标签</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEclusteryaml%E6%96%87%E4%BB%B6">配置cluster.yaml文件</a></li>
<li><a href="#%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2ceph">开始部署ceph</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEceph-dashboard">配置ceph dashboard</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEceph%E4%B8%BAstorageclass">配置ceph为storageclass</a></li>
<li><a href="#%E5%88%9B%E5%BB%BAstorageclass">创建StorageClass</a></li>
<li><a href="#%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9%E8%BF%9B%E5%85%A5%E9%9B%86%E7%BE%A4">添加新节点进入集群</a></li>
<li><a href="#%E5%88%A0%E9%99%A4%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9">删除一个节点</a></li>
</ul>
</li>
</ul>

  </div>
</div>

<script>

  let lastTop = 0, lList = [], hList = [], postBody, lastIndex = -1;
  let active = 'active-show', activeClass = 'active-current';
  let tocWrapper = document.querySelector('#toc_wrapper');
  let tocContent = tocWrapper.children[0];
  let autoNumber = tocWrapper && tocWrapper.classList.contains('auto-number');

  function addTocNumber(elem, deep) {
    if (!elem) {
      return;
    }
    let prop = elem.__proto__;

    if (prop === HTMLUListElement.prototype) {
      for (let i = 0; i < elem.children.length; i++) {
        addTocNumber(elem.children[i], deep + (i + 1) + '.');
      }
    } else if (prop === HTMLLIElement.prototype) {
      // 保存li元素
      if (elem.children[0].__proto__ === HTMLAnchorElement.prototype) {
        lList.push(elem);
      }
      for (let i = 0; i < elem.children.length; i++) {
        let cur = elem.children[i];
        if (cur.__proto__ === HTMLAnchorElement.prototype) {
          if (autoNumber) {
            cur.text = deep + ' ' + cur.text;
          }
        } else if (cur.__proto__ === HTMLUListElement.prototype) {
          addTocNumber(cur, deep);
        }
      }
    }
  }

  function removeParentActiveClass() {
    let parents = tocContent.querySelectorAll('.' + active)
    parents.forEach(function (elem) {
      elem.classList.remove(active);
    });
  }

  function addActiveClass(index) {
    if (index >= 0 && index < hList.length) {
      lList[index].classList.add(activeClass);
    }
  }

  function removeActiveClass(index) {
    if (index >= 0 && index < hList.length) {
      lList[index].classList.remove(activeClass);
    }
  }

  function addActiveLiElemment(elem, parent) {
    if (!elem || elem === parent) {
      return;
    } else {
      if (elem.__proto__ === HTMLLIElement.prototype) {
        elem.classList.add(active);
      }
      addActiveLiElemment(elem.parentElement, parent);
    }
  }

  function showToc() {
    if (tocWrapper) {
      postBody = document.querySelector('#post_body');
      for (let i = 0; i < postBody.children.length; i++) {
        if (postBody.children[i].__proto__ === HTMLHeadingElement.prototype) {
          hList.push(postBody.children[i]);
        }
      }
      if (tocWrapper.classList.contains('compress')) {
        tocContent.classList.add('closed');
      } else if (tocWrapper.classList.contains('no_compress')) {
        tocContent.classList.add('expanded');
      } else {
        if (hList.length > 10) {
          active = 'active-hidden'
          tocContent.classList.add('closed');
        } else {
          tocContent.classList.add('expanded');
        }
      }
    }
  }

  (function () {
    // 处理不是从#一级标题开始目录
    if (tocContent.children.length === 1 && tocContent.children[0].__proto__ === HTMLLIElement.prototype) {
      let con = tocContent.children[0].children[0];
      tocContent.innerHTML = con.innerHTML;
    }
    let markdownItTOC = document.querySelector('.markdownIt-TOC');
    let innerHeight = window.innerHeight;
    markdownItTOC.style = `max-height: ${innerHeight - 80 > 0 ? innerHeight - 80 : innerHeight}px`
    addTocNumber(tocContent, '');
  })();

  document.addEventListener('scroll', function (e) {
    if (lList.length <= 0) {
      return;
    }
    let scrollTop = document.scrollingElement.scrollTop + 10;
    let dir;

    if (lastTop - scrollTop > 0) {
      dir = 'up';
    } else {
      dir = 'down';
    }

    lastTop = scrollTop;
    if (scrollTop <= 0) {
      if (lastIndex >= 0 && lastIndex < hList.length) {
        lList[lastIndex].classList.remove(activeClass);
      }
      return;
    }

    let current = 0, hasFind = false;
    for (let i = 0; i < hList.length; i++) {
      if (hList[i].offsetTop > scrollTop) {
        current = i;
        hasFind = true;
        break;
      }
    }
    if (!hasFind && scrollTop > lList[lList.length - 1].offsetTop) {
      current = hList.length - 1;
    } else {
      current--;
    }
    if (dir === 'down') {
      if (current > lastIndex) {
        addActiveClass(current);
        removeActiveClass(lastIndex)
        lastIndex = current;
        removeParentActiveClass();
        lList[current] && addActiveLiElemment(lList[current].parentElement, tocContent);
      }
    } else {
      if (current < lastIndex) {
        addActiveClass(current);
        removeActiveClass(lastIndex);
        lastIndex = current;
        removeParentActiveClass();
        lList[current] && addActiveLiElemment(lList[current].parentElement, tocContent);
      }
    }
  });


  window.addEventListener('load', function () {
    showToc();
    document.querySelector('#sidebar').style = 'display: block;';
    tocWrapper.classList.add('toc-active');
    setTimeout(function () {
      if ("createEvent" in document) {
        let evt = document.createEvent("HTMLEvents");
        evt.initEvent("scroll", false, true);
        document.dispatchEvent(evt);
      }
      else {
        document.fireEvent("scroll");
      }
    }, 500)
  })

</script>
            </div>
          
        
      </div>
    </div>
  
</div>
<script>
  const SIDEBAR_TITLE_ACTIVE = 'sidebar-title-active';
  const SIDEBAR_BODY_ACTIVE = 'sidebar-body-active';
  const SLIDE_UP_IN = 'slide-up-in';

  let sidebar = document.querySelector('#sidebar'),
  tocSideBar = document.querySelector('#tocSideBar'),
  metaSideBar = document.querySelector('#metaSideBar'),
  postToc = document.querySelector('#post_toc'),
  postSiteMeta = document.querySelector('#post_side_meta'),
  sidebarTitle = document.querySelector('.sidebar-title'),
  sidebarBody = document.querySelector('#sidebar_body');

  tocSideBar && tocSideBar.addEventListener('click', (e) => {
    toggleSidebar(e);
  });

  metaSideBar && metaSideBar.addEventListener('click', (e) => {
    toggleSidebar(e);
  });

  function toggleSidebar(e) {
    let currentTitle = document.querySelector("."+SIDEBAR_TITLE_ACTIVE);
    if (currentTitle == e.srcElement) {
      return ;
    }
    let current, showElement, hideElement;
    if (e.srcElement == metaSideBar) {
      showElement = postSiteMeta;
      hideElement = postToc;
    } else if (e.srcElement == tocSideBar){
      showElement = postToc;
      hideElement = postSiteMeta;
    }
    currentTitle.classList.remove(SIDEBAR_TITLE_ACTIVE);
    e.srcElement.classList.add(SIDEBAR_TITLE_ACTIVE);

    window.Velocity(hideElement, 'stop');
    window.Velocity(hideElement, 'transition.slideUpOut', {
      display: 'none',
      duration: 200,
      complete: function () {
        window.Velocity(showElement, 'transition.slideDownIn', {
          duration: 200
        });
      }
    })
    hideElement.classList.remove(SIDEBAR_BODY_ACTIVE);
    showElement.classList.add(SIDEBAR_BODY_ACTIVE);
  }

  postToc && postToc.addEventListener('transitionend', function() {
    this.classList.remove(SLIDE_UP_IN);
  });

  if (sidebarBody) {
    if (sidebarBody.classList.contains('pisces') || sidebarBody.classList.contains('gemini')) {
      let hasFix = false;
      let scrollEl = document.querySelector('.main-continer');
      let limitTop = document.querySelector('#nav_ul').children.length * 42 + 162;
      window.addEventListener('scroll', function(e) {
        if (document.scrollingElement.scrollTop >= limitTop) {
          if (!hasFix) {
            sidebar.classList.add('sidebar-fixed');
            hasFix = true;
          }
        } else {
          if (hasFix) {
            sidebar.classList.remove('sidebar-fixed');
            hasFix = false;
          }
        }
      });
    }
  }
  
</script>
        <div class="section-box box-shadow-wrapper">
          <div class="section bg-color post post-page">
            <header class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="http://lvelvis.github.io/post/kubernetes搭建rook-ceph/">
      kubernetes搭建rook-ceph
    </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span>发布于</span>
      <span>2019-12-12</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-folder-o"></i>
      <span class="pc-show">分类于</span>
      
      
      <a href="http://lvelvis.github.io/tag/Q617Y3Kh2/">
        <span>k8s</span>
      </a>、
      
      
      
      <a href="http://lvelvis.github.io/tag/hu0mNDfuy/">
        <span>rook-ceph</span>
      </a>、
      
      
      
      <a href="http://lvelvis.github.io/tag/MkN4-Vurh-/">
        <span>ceph</span>
      </a>
      
      
    </span>
    <span class="post-meta-divider">|</span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span>13分钟</span>
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span>2524<span class="pc-show">字数</span></span>
    </span>
    
    
    
    <span id="/post/kubernetes搭建rook-ceph/" data-flag-title="kubernetes搭建rook-ceph" class="meta-item pc-show leancloud_visitors">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-eye"></i>
      <span>浏览量：<span class="leancloud-visitors-count"></span></span>
    </span>
    
  </div>
</header>
            <div class="post-body next-md-body" id="post_body">
              <h1 id="简介">简介</h1>
<p>Rook官网：https://rook.io<br>
Rook是云原生计算基金会(CNCF)的孵化级项目.<br>
Rook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。<br>
至于CEPH，官网在这：https://ceph.com/<br>
ceph官方提供的helm部署，至今我没成功过，所以转向使用rook提供的方案<br>
有道笔记原文：http://note.youdao.com/noteshare?id=281719f1f0374f787effc90067e0d5ad&amp;sub=0B59EA339D4A4769B55F008D72C1A4C0</p>
<h1 id="环境">环境</h1>
<pre><code>centos 7.5
kernel 4.18.7-1.el7.elrepo.x86_64
docker 18.06
kubernetes v1.12.2
    kubeadm部署：
        网络: canal
        DNS: coredns
    集群成员：    
    192.168.1.1 kube-master
    192.168.1.2 kube-node1
    192.168.1.3 kube-node2
    192.168.1.4 kube-node3
    192.168.1.5 kube-node4

所有node节点准备一块200G的磁盘：/dev/sdb
kubernetes搭建rook-ceph
</code></pre>
<figure data-type="image" tabindex="1"><img src="http://lvelvis.github.io/post-images/1588233297037.png" alt="" loading="lazy"></figure>
<h1 id="准备工作">准备工作</h1>
<p>所有节点开启ip_forward</p>
<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
</code></pre>
<p>sysctl -p</p>
<h1 id="开始部署operator">开始部署Operator</h1>
<h2 id="部署rook-operator">部署Rook Operator</h2>
<pre><code>cd $HOME
git clone https://github.com/rook/rook.git

cd rook
cd cluster/examples/kubernetes/ceph
kubectl apply -f operator.yaml
</code></pre>
<figure data-type="image" tabindex="2"><img src="http://lvelvis.github.io/post-images/1588233371696.png" alt="" loading="lazy"></figure>
<h2 id="查看operator的状态">查看Operator的状态</h2>
<p>执行apply之后稍等一会<br>
operator会在集群内的每个主机创建两个pod:rook-discover,rook-ceph-agent</p>
<p>kubectl -n rook-ceph-system get pod -o wide<br>
<img src="http://lvelvis.github.io/post-images/1588233523024.png" alt="" loading="lazy"></p>
<h2 id="给节点打标签">给节点打标签</h2>
<p>运行ceph-mon的节点打上：ceph-mon=enabled<br>
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-mon=enabled<br>
运行ceph-osd的节点，也就是存储节点，打上：ceph-osd=enabled<br>
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-osd=enabled<br>
运行ceph-mgr的节点，打上：ceph-mgr=enabled<br>
mgr只能支持一个节点运行，这是ceph跑k8s里的局限<br>
kubectl label nodes kube-node1 ceph-mgr=enabled</p>
<h2 id="配置clusteryaml文件">配置cluster.yaml文件</h2>
<p>官方配置文件详解：https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html<br>
文件中有几个地方要注意：</p>
<ul>
<li>dataDirHostPath: 这个路径是会在宿主机上生成的，保存的是ceph的相关的配置文件，再重新生成*集群的时候要确保这个目录为空，否则mon会无法启动</li>
<li>useAllDevices: 使用所有的设备，建议为false，否则会把宿主机所有可用的磁盘都干掉</li>
<li>useAllNodes：使用所有的node节点，建议为false，肯定不会用k8s集群内的所有node来搭建ceph的</li>
<li>databaseSizeMB和journalSizeMB：当磁盘大于100G的时候，就注释这俩项就行了<br>
本次实验用到的 cluster.yaml 文件内容如下：</li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;configmaps&quot;]
  verbs: [ &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;delete&quot; ]
---
# Allow the operator to create resources in this cluster's namespace
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster-mgmt
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-cluster-mgmt
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph-system
---
# Allow the pods in this namespace to work with configmaps
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-cluster
subjects:
- kind: ServiceAccount
  name: rook-ceph-cluster
  namespace: rook-ceph
---
apiVersion: ceph.rook.io/v1beta1
kind: Cluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
    # v12 is luminous, v13 is mimic, and v14 is nautilus.
    # RECOMMENDATION: In production, use a specific version tag instead of the general v13 flag, which pulls the latest release and could result in different
    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
    image: ceph/ceph:v13
    # Whether to allow unsupported versions of Ceph. Currently only luminous and mimic are supported.
    # After nautilus is released, Rook will be updated to support nautilus.
    # Do not set to true in production.
    allowUnsupported: false
  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).
  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
  # In Minikube, the '/data' directory is configured to persist across reboots. Use &quot;/data/rook&quot; in Minikube environment.
  dataDirHostPath: /var/lib/rook
  # The service account under which to run the daemon pods in this cluster if the default account is not sufficient (OSDs)
  serviceAccount: rook-ceph-cluster
  # set the amount of mons to be started
  # count可以定义ceph-mon运行的数量，这里默认三个就行了
  mon:
    count: 3
    allowMultiplePerNode: true
  # enable the ceph dashboard for viewing cluster status
  # 开启ceph资源面板
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
  network:
    # toggle to use hostNetwork
    # 使用宿主机的网络进行通讯
    # 使用宿主机的网络貌似可以让集群外的主机挂载ceph
    # 但是我没试过，有兴趣的兄弟可以试试改成true
    # 反正这里只是集群内用，我就不改了
    hostNetwork: false
  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
  # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
  # tolerate taints with a key of 'storage-node'.
  placement:
#    all:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: role
#              operator: In
#              values:
#              - storage-node
#      podAffinity:
#      podAntiAffinity:
#      tolerations:
#      - key: storage-node
#        operator: Exists
# The above placement information can also be specified for mon, osd, and mgr components
#    mon:
#    osd:
#    mgr:
# nodeAffinity：通过选择标签的方式，可以限制pod被调度到特定的节点上
# 建议限制一下，为了让这几个pod不乱跑
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mon
              operator: In
              values:
              - enabled
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-osd
              operator: In
              values:
              - enabled
    mgr:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mgr
              operator: In
              values:
              - enabled
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
#    mgr:
#      limits:
#        cpu: &quot;500m&quot;
#        memory: &quot;1024Mi&quot;
#      requests:
#        cpu: &quot;500m&quot;
#        memory: &quot;1024Mi&quot;
# The above example requests/limits can also be added to the mon and osd components
#    mon:
#    osd:
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    deviceFilter:
    location:
    config:
      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.
      # Set the storeType explicitly only if it is required not to use the default.
      # storeType: bluestore
      # databaseSizeMB: &quot;1024&quot; # this value can be removed for environments with normal sized disks (100 GB or larger)
      # journalSizeMB: &quot;1024&quot;  # this value can be removed for environments with normal sized disks (20 GB or larger)
# Cluster level list of directories to use for storage. These values will be set for all nodes that have no `directories` set.
#    directories:
#    - path: /rook/storage-dir
# Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
# nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
#建议磁盘配置方式如下：
#name: 选择一个节点，节点名字为kubernetes.io/hostname的标签，也就是kubectl get nodes看到的名字
#devices: 选择磁盘设置为OSD
# - name: &quot;sdb&quot;:将/dev/sdb设置为osd
    nodes:
    - name: &quot;kube-node1&quot;
      devices:
      - name: &quot;sdb&quot;
    - name: &quot;kube-node2&quot;
      devices:
      - name: &quot;sdb&quot;
    - name: &quot;kube-node3&quot;
      devices:
      - name: &quot;sdb&quot;

#      directories: # specific directories to use for storage can be specified for each node
#      - path: &quot;/rook/storage-dir&quot;
#      resources:
#        limits:
#          cpu: &quot;500m&quot;
#          memory: &quot;1024Mi&quot;
#        requests:
#          cpu: &quot;500m&quot;
#          memory: &quot;1024Mi&quot;
#    - name: &quot;172.17.4.201&quot;
#      devices: # specific devices to use for storage can be specified for each node
#      - name: &quot;sdb&quot;
#      - name: &quot;sdc&quot;
#      config: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: filestore
#    - name: &quot;172.17.4.301&quot;
#      deviceFilter: &quot;^sd.&quot;
</code></pre>
<h2 id="开始部署ceph">开始部署ceph</h2>
<p>部署ceph<br>
kubectl apply -f cluster.yaml<br>
cluster会在rook-ceph这个namesapce创建资源<br>
看到所有的pod都Running就行了<br>
注意看一下pod分布的宿主机，跟我们打标签的主机是一致的<br>
kubectl -n rook-ceph get pod -o wide<br>
<img src="http://lvelvis.github.io/post-images/1588233756856.png" alt="" loading="lazy"></p>
<p>切换到其他主机看一下磁盘</p>
<p>切换到kube-node1<br>
lsblk<br>
<img src="http://lvelvis.github.io/post-images/1588233803458.png" alt="" loading="lazy"></p>
<h2 id="配置ceph-dashboard">配置ceph dashboard</h2>
<p>看一眼dashboard在哪个service上<br>
kubectl -n rook-ceph get service<br>
可以看到dashboard监听了8443端口<br>
<img src="http://lvelvis.github.io/post-images/1588233853544.png" alt="" loading="lazy"></p>
<p>创建个nodeport类型的service以便集群外部访问<br>
kubectl apply -f dashboard-external-https.yaml</p>
<p>查看一下nodeport在哪个端口<br>
kubectl -n rook-ceph get service<br>
<img src="http://lvelvis.github.io/post-images/1588233909644.png" alt="" loading="lazy"></p>
<p>找出Dashboard的登陆账号和密码<br>
MGR_POD=<code>kubectl get pod -n rook-ceph | grep mgr | awk '{print $1}'</code><br>
kubectl -n rook-ceph logs $MGR_POD | grep password<br>
<img src="http://lvelvis.github.io/post-images/1588233955731.png" alt="" loading="lazy"></p>
<p>打开浏览器输入任意一个Node的IP+nodeport端口<br>
这里我的就是：https://192.168.1.2:30290<br>
<img src="http://lvelvis.github.io/post-images/1588234024005.png" alt="" loading="lazy"><br>
<img src="http://lvelvis.github.io/post-images/1588234065656.png" alt="" loading="lazy"></p>
<h2 id="配置ceph为storageclass">配置ceph为storageclass</h2>
<p>官方给了一个样本文件：storageclass.yaml<br>
这个文件使用的是 RBD 块存储<br>
pool创建详解：https://rook.io/docs/rook/v0.8/ceph-pool-crd.html</p>
<pre><code>apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  #这个name就是创建成ceph pool之后的pool名字
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 1
  # size 池中数据的副本数,1就是不保存任何副本
  failureDomain: osd
  #  failureDomain：数据块的故障域，
  #  值为host时，每个数据块将放置在不同的主机上
  #  值为osd时，每个数据块将放置在不同的osd上
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: ceph
   # StorageClass的名字，pvc调用时填的名字
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use `rook` as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: xfs
# 设置回收策略默认为：Retain
reclaimPolicy: Retain
</code></pre>
<h2 id="创建storageclass">创建StorageClass</h2>
<p>kubectl apply -f storageclass.yaml<br>
kubectl get storageclasses.storage.k8s.io  -n rook-ceph<br>
kubectl describe storageclasses.storage.k8s.io  -n rook-ceph<br>
kubernetes搭建rook-ceph</p>
<p>创建个nginx pod尝试挂载</p>
<pre><code>cat &lt;&lt; EOF &gt; nginx.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: ceph

---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports: 
  - port: 80
    name: nginx-port
    targetPort: 80
    protocol: TCP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /html
          name: http-file
      volumes:
      - name: http-file
        persistentVolumeClaim:
          claimName: nginx-pvc
EOF
</code></pre>
<p>kubectl apply -f nginx.yaml<br>
查看pv,pvc是否创建了<br>
kubectl get pv,pvc</p>
<p>看一下nginx这个pod也运行了<br>
kubectl get pod</p>
<p>删除这个pod,看pv是否还存在<br>
kubectl delete -f nginx.yaml</p>
<p>kubectl get pv,pvc<br>
可以看到，pod和pvc都已经被删除了，但是pv还在！！！</p>
<h2 id="添加新节点进入集群">添加新节点进入集群</h2>
<p>这次我们要把node4添加进集群，先打标签<br>
kubectl label nodes kube-node4 ceph-osd=enabled<br>
重新编辑cluster.yaml文件<br>
原来的基础上添加node4的信息</p>
<p>cd $HOME/rook/cluster/examples/kubernetes/ceph/<br>
vi cluster.yam<br>
<img src="http://lvelvis.github.io/post-images/1588234207475.png" alt="" loading="lazy"></p>
<p>apply一下cluster.yaml文件<br>
kubectl apply -f cluster.yaml</p>
<p>盯着rook-ceph名称空间,集群会自动添加node4进来<br>
kubectl -n rook-ceph get pod -o wide -w<br>
kubectl -n rook-ceph get pod -o wide<br>
<img src="http://lvelvis.github.io/post-images/1588234283568.png" alt="" loading="lazy"><br>
<img src="http://lvelvis.github.io/post-images/1588234307875.png" alt="" loading="lazy"><br>
去node4节点看一下磁盘<br>
lsblk<br>
<img src="http://lvelvis.github.io/post-images/1588234334204.png" alt="" loading="lazy"></p>
<h2 id="删除一个节点">删除一个节点</h2>
<p>去掉node3的标签<br>
kubectl label nodes kube-node3 ceph-osd-<br>
重新编辑cluster.yaml文件<br>
删除node3的信息<br>
cd $HOME/rook/cluster/examples/kubernetes/ceph/<br>
vi cluster.yam<br>
<img src="http://lvelvis.github.io/post-images/1588234380826.png" alt="" loading="lazy"></p>
<p>apply一下cluster.yaml文件<br>
kubectl apply -f cluster.yaml<br>
<img src="http://lvelvis.github.io/post-images/1588234493936.png" alt="" loading="lazy"></p>
<p>kubectl -n rook-ceph get pod -o wide<br>
<img src="http://lvelvis.github.io/post-images/1588234548709.png" alt="" loading="lazy"><br>
<img src="http://lvelvis.github.io/post-images/1588234579591.png" alt="" loading="lazy"><br>
最后记得删除宿主机的/var/lib/rook文件夹</p>

            </div>
            <div class="post-footer">
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>本文作者：</strong>
      lvelvis
    </li>
    <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://lvelvis.github.io/post/kubernetes搭建rook-ceph/" title="kubernetes搭建rook-ceph">http://lvelvis.github.io/post/kubernetes搭建rook-ceph/</a>
    </li>
    <li class="post-copyright-license">
      <strong>版权声明： </strong>
      本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！
    </li>
  </ul>
  <div class="tags">
    
      <a href="http://lvelvis.github.io/tag/Q617Y3Kh2/"># k8s</a>
    
      <a href="http://lvelvis.github.io/tag/hu0mNDfuy/"># rook-ceph</a>
    
      <a href="http://lvelvis.github.io/tag/MkN4-Vurh-/"># ceph</a>
    
  </div>
  <div class="nav">
    <div class="nav-prev">
      
        <i class="fa fa-chevron-left"></i>
        <a class="nav-pc-next" title="kubernetes Tekton-CI/CD 持续集成流水线" href="http://lvelvis.github.io/post/kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian/">kubernetes Tekton-CI/CD 持续集成流水线</a class="nav-pc-next">
        <a class="nav-mobile-prev" title="kubernetes Tekton-CI/CD 持续集成流水线" href="http://lvelvis.github.io/post/kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian/">上一篇</a>
      
    </div>
    <div class="nav-next">
      
    </div>
  </div>
</div>
            
  <script src="https://cdn.jsdelivr.net/npm/valine@1.4.4/dist/Valine.min.js"></script>
<div id="vcomments" style="padding: 10px 0px 0px 0px"></div>

<style>
  .v .veditor {
    min-height: 10rem;
    background-image: url('https://upimage.alexhchu.com/2020/04/21/47eda59424daa.gif');
    background-size: contain;
    background-repeat: no-repeat;
    background-position: right;
    background-color: rgba(255, 255, 255, 0);
    resize: none;
  }

  .v .vwrap {
    border: 1px solid #000 !important;
  }

  .v .vbtn {
    padding: .4rem 1.2rem !important;
    border-color: #fff !important;
    background-color: #49b1f5 !important;
    color: #fff !important;
    font-size: .7rem !important;
  }

  .v .vcards .vcard .vh .vmeta .vat {
    padding: 0 .8rem !important;
    border: 1px solid #00c4b6 !important;
    border-radius: 5px !important;
    color: #00c4b6 !important;
  }
</style>
<script>
  new Valine({
    el: '#vcomments',
    appId: 'q6RNtDXsSlHgt7Wml6Krh1My-gzGzoHsz',
    appKey: 'XVXX46bo5q1RnjaGrExJEXOm',
    avatar: 'identicon',
    placeholder: '',
    pageSize: '10',
    lang: 'zh-cn',
    visitor: 'true' === 'true',
    highlight: 'true' === 'true',
    avatarForce: 'true' === 'true',
  });
</script>

          </div>
        </div>
      </div>
    </div>
    <div class="footer-box">
  <footer class="footer">
    <div class="copyright">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | © 2020 Theme By elvis</a>
    </div>
    <div class="poweredby">
      <center><iframe width="280" scrolling="no" height="25" frameborder="0" allowtransparency="true" src="http://i.tianqi.com/index.php?c=code&id=34&icon=1&num=3"></iframe></center><marquee scrollamount="5" ><h2 class="wow fadeIn animated" data-wow-delay="1250ms" style="visibility: visible; animation-delay: 1250ms; animation-name: fadeIn;"><font face="Orbitron" style="color:#CC0707;text-shadow: 0 0 0.9em #6EC101,0 0 0.9em
Yellow;"><b><?php echo $conf['xbmz']?></b></font></h2></marquee>
  <script type="text/javascript"> 
  document.onkeydown = function(event){
   if ((event.ctrlKey)&&(event.keyCode==115 || event.keyCode==83)){
    event.returnValue=false;
    return;
   } 
  }
 </script>
<script async src="https://api.ly522.com/js/jilei.pure.mini.js"></script>
<span id="jilei_container_site_pv">总访问量<span id="jilei_value_site_pv"></span>次</span>
<span class="post-meta-divider">|</span>
<span id="jilei_container_site_uv">在线人数<span id="jilei_value_site_uv"></span>人</span></p>
    </div>
  </footer>
  
  
    <div class="drawer-box left" id="drawer_box">
      <span class="muse-line muse-line-first"></span>
      <span class="muse-line muse-line-middle"></span>
      <span class="muse-line muse-line-last"></span>
    </div>
  
  <div class="mist back-to-top" id="back_to_top">
    <i class="fa fa-arrow-up"></i>
    
  </div>
  
  
    
<link rel="stylesheet" href="/media/live2d/css/live2d.css" />
<div class="box-scale">
  <div id="landlord" style="left: 5px;bottom: px;"
    data-key="">
    <canvas id="live2d" width="500" height="560" class="live2d"></canvas>
    

      <div class="message" style="opacity:0"></div>
      <div class="live_talk_input_body">
        <div class="live_talk_input_name_body">
          <input name="name" type="text" class="live_talk_name white_input" id="AIuserName" autocomplete="off"
            placeholder="你的名字" />
        </div>
        <div class="live_talk_input_text_body">
          <input name="talk" type="text" class="live_talk_talk white_input" id="AIuserText" autocomplete="off"
            placeholder="要和我聊什么呀？" />
          <button type="button" class="live_talk_send_btn" id="talk_send">发送</button>
        </div>
      </div>
      <input name="live_talk" id="live_talk" value="1" type="hidden" />
      <div class="live_ico_box">
        <div class="live_ico_item type_info" id="showInfoBtn"></div>
        <div class="live_ico_item type_talk" id="showTalkBtn"></div>
        
        <div class="live_ico_item type_music" id="musicButton"></div>
        
        <div class="live_ico_item type_youdu" id="youduButton"></div>
        <div class="live_ico_item type_quit" id="hideButton"></div>
        <input name="live_statu_val" id="live_statu_val" value="0" type="hidden" />
        <audio src="" style="display:none;" id="live2d_bgm" data-bgm="0" preload="none"></audio>
        <input id="duType" value="douqilai" type="hidden">
        
        <input name="live2dBGM" value="" type="hidden">
        
      </div>
    
  </div>
</div>
<div id="open_live2d">召唤看板娘</div>
<script src="https://libs.baidu.com/jquery/2.0.0/jquery.min.js"></script>
<script>
  var message_Path = 'https://cdn.jsdelivr.net/gh/hsxyhao/live2d.github.io@master/';
  let landlord = document.querySelector('#landlord');
  var apiKey = landlord.dataset.key;
</script>
<script type="text/javascript" src="/media/live2d/js/live2d.js"></script>
<script>
	var home_Path = document.location.protocol + '//' + window.document.location.hostname + ":" + window.document.location.port + '/';
	var userAgent = window.navigator.userAgent.toLowerCase();
	var norunAI = ["android", "iphone", "ipod", "ipad", "windows phone", "mqqbrowser", "msie", "trident/7.0"];
	var norunFlag = false;

	for (var i = 0; i < norunAI.length; i++) {
		if (userAgent.indexOf(norunAI[i]) > -1) {
			norunFlag = true;
			break;
		}
	}

	if (!window.WebGLRenderingContext) {
		norunFlag = true;
	}

	if (!norunFlag) {
		var hitFlag = false;
		var AIFadeFlag = false;
		var liveTlakTimer = null;
		var sleepTimer_ = null;
		var AITalkFlag = false;
		var talkNum = 0;
		(function () {
			function renderTip(template, context) {
				var tokenReg = /(\\)?\{([^\{\}\\]+)(\\)?\}/g;
				return template.replace(tokenReg, function (word, slash1, token, slash2) {
					if (slash1 || slash2) {
						return word.replace('\\', '');
					}
					var variables = token.replace(/\s/g, '').split('.');
					var currentObject = context;
					var i, length, variable;
					for (i = 0, length = variables.length; i < length; ++i) {
						variable = variables[i];
						currentObject = currentObject[variable];
						if (currentObject === undefined || currentObject === null) return '';
					}
					return currentObject;
				});
			}

			String.prototype.renderTip = function (context) {
				return renderTip(this, context);
			};

			var re = /x/;
			re.toString = function () {
				showMessage('哈哈，你打开了控制台，是想要看看我的秘密吗？', 5000);
				return '';
			};

			$(document).on('copy', function () {
				showMessage('你都复制了些什么呀，转载要记得加上出处哦~~', 5000);
			});

			function initTips() {
				$.ajax({
					cache: true,
					url: message_Path + 'message.json',
					dataType: "json",
					success: function (result) {
						$.each(result.mouseover, function (index, tips) {
							$(tips.selector).mouseover(function () {
								var text = tips.text;
								if (Array.isArray(tips.text)) text = tips.text[Math.floor(Math.random() * tips.text.length + 1) - 1];
								text = text.renderTip({ text: $(this).text() });
								showMessage(text, 3000);
								talkValTimer();
								clearInterval(liveTlakTimer);
								liveTlakTimer = null;
							});
							$(tips.selector).mouseout(function () {
								showHitokoto();
								if (liveTlakTimer == null) {
									liveTlakTimer = window.setInterval(function () {
										showHitokoto();
									}, 15000);
								};
							});
						});
						$.each(result.click, function (index, tips) {
							$(tips.selector).click(function () {
								if (hitFlag) {
									return false
								}
								hitFlag = true;
								setTimeout(function () {
									hitFlag = false;
								}, 8000);
								var text = tips.text;
								if (Array.isArray(tips.text)) text = tips.text[Math.floor(Math.random() * tips.text.length + 1) - 1];
								text = text.renderTip({ text: $(this).text() });
								showMessage(text, 3000);
							});
							clearInterval(liveTlakTimer);
							liveTlakTimer = null;
							if (liveTlakTimer == null) {
								liveTlakTimer = window.setInterval(function () {
									showHitokoto();
								}, 15000);
							};
						});
					}
				});
			}
			initTips();

			var text;
			if (document.referrer !== '') {
				var referrer = document.createElement('a');
				referrer.href = document.referrer;
				text = '嗨！来自 <span style="color:#0099cc;">' + referrer.hostname + '</span> 的朋友！';
				var domain = referrer.hostname.split('.')[1];
				if (domain == 'baidu') {
					text = '嗨！ 来自 百度搜索 的朋友！<br>欢迎访问<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				} else if (domain == 'so') {
					text = '嗨！ 来自 360搜索 的朋友！<br>欢迎访问<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				} else if (domain == 'google') {
					text = '嗨！ 来自 谷歌搜索 的朋友！<br>欢迎访问<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				}
			} else {
				if (window.location.href == home_Path) { //主页URL判断，需要斜杠结尾
					var now = (new Date()).getHours();
					if (now > 23 || now <= 5) {
						text = '你是夜猫子呀？这么晚还不睡觉，明天起的来嘛？';
					} else if (now > 5 && now <= 7) {
						text = '早上好！一日之计在于晨，美好的一天就要开始了！';
					} else if (now > 7 && now <= 11) {
						text = '上午好！工作顺利嘛，不要久坐，多起来走动走动哦！';
					} else if (now > 11 && now <= 14) {
						text = '中午了，工作了一个上午，现在是午餐时间！';
					} else if (now > 14 && now <= 17) {
						text = '午后很容易犯困呢，今天的运动目标完成了吗？';
					} else if (now > 17 && now <= 19) {
						text = '傍晚了！窗外夕阳的景色很美丽呢，最美不过夕阳红~~';
					} else if (now > 19 && now <= 21) {
						text = '晚上好，今天过得怎么样？';
					} else if (now > 21 && now <= 23) {
						text = '已经这么晚了呀，早点休息吧，晚安~~';
					} else {
						text = '嗨~ 快来逗我玩吧！';
					}
				} else {
					text = '欢迎阅读<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				}
			}
			showMessage(text, 12000);
		})();

		liveTlakTimer = setInterval(function () {
			showHitokoto();
		}, 15000);

		function showHitokoto() {
			if (sessionStorage.getItem("Sleepy") !== "1") {
				if (!AITalkFlag) {
					$.getJSON('https://v1.hitokoto.cn/', function (result) {
						talkValTimer();
						showMessage(result.hitokoto, 0);
					});
				}
			} else {
				hideMessage(0);
				if (sleepTimer_ == null) {
					sleepTimer_ = setInterval(function () {
						checkSleep();
					}, 200);
				}
			}
		}

		function checkSleep() {
			var sleepStatu = sessionStorage.getItem("Sleepy");
			if (sleepStatu !== '1') {
				talkValTimer();
				showMessage('你回来啦~', 0);
				clearInterval(sleepTimer_);
				sleepTimer_ = null;
			}
		}

		function showMessage(text, timeout) {
			if (Array.isArray(text)) text = text[Math.floor(Math.random() * text.length + 1) - 1];
			$('.message').stop();
			$('.message').html(text);
			$('.message').fadeTo(200, 1);
			//if (timeout === null) timeout = 5000;
			//hideMessage(timeout);
		}
		function talkValTimer() {
			$('#live_talk').val('1');
		}

		function hideMessage(timeout) {
			//$('.message').stop().css('opacity',1);
			if (timeout === null) timeout = 5000;
			$('.message').delay(timeout).fadeTo(200, 0);
		}

		function initLive2d() {
			$('#hideButton').on('click', function () {
				if (AIFadeFlag) {
					return false;
				} else {
					AIFadeFlag = true;
					localStorage.setItem("live2dhidden", "0");
					$('#landlord').fadeOut(200);
					$('#open_live2d').delay(200).fadeIn(200);
					setTimeout(function () {
						AIFadeFlag = false;
					}, 300);
				}
			});
			$('#open_live2d').on('click', function () {
				if (AIFadeFlag) {
					return false;
				} else {
					AIFadeFlag = true;
					localStorage.setItem("live2dhidden", "1");
					$('#open_live2d').fadeOut(200);
					$('#landlord').delay(200).fadeIn(200);
					setTimeout(function () {
						AIFadeFlag = false;
					}, 300);
				}
			});
			$('#youduButton').on('click', function () {
				if ($('#youduButton').hasClass('doudong')) {
					var typeIs = $('#youduButton').attr('data-type');
					$('#youduButton').removeClass('doudong');
					$('body').removeClass(typeIs);
					$('#youduButton').attr('data-type', '');
				} else {
					var duType = $('#duType').val();
					var duArr = duType.split(",");
					var dataType = duArr[Math.floor(Math.random() * duArr.length)];

					$('#youduButton').addClass('doudong');
					$('#youduButton').attr('data-type', dataType);
					$('body').addClass(dataType);
				}
			});
			if (apiKey) {
				$('#showInfoBtn').on('click', function () {
					var live_statu = $('#live_statu_val').val();
					if (live_statu == "0") {
						return
					} else {
						$('#live_statu_val').val("0");
						$('.live_talk_input_body').fadeOut(500);
						AITalkFlag = false;
						showHitokoto();
						$('#showTalkBtn').show();
						$('#showInfoBtn').hide();
					}
				});
				$('#showTalkBtn').on('click', function () {
					var live_statu = $('#live_statu_val').val();
					if (live_statu == "1") {
						return
					} else {
						$('#live_statu_val').val("1");
						$('.live_talk_input_body').fadeIn(500);
						AITalkFlag = true;
						$('#showTalkBtn').hide();
						$('#showInfoBtn').show();

					}
				});
				$('#talk_send').on('click', function () {
					var info_ = $('#AIuserText').val();
					var userid_ = $('#AIuserName').val();
					if (info_ == "") {
						showMessage('写点什么吧！', 0);
						return;
					}
					if (userid_ == "") {
						showMessage('聊之前请告诉我你的名字吧！', 0);
						return;
					}
					showMessage('思考中~', 0);
					let protocol = window.location.protocol.indexOf("s") > 0 ? "https" : "http";
					$.ajax({
						type: "get",
						url: `${protocol}://www.tuling123.com/openapi/api?key=${apiKey}&info=${info_}`,
						dataType: "json",
						success: function (res) {
							talkValTimer();
							showMessage(res.text, 0);
							$('#AIuserText').val("");
							sessionStorage.setItem("live2duser", userid_);
						},
						error: function (e) {
							talkValTimer();
							showMessage('似乎有什么错误，请和站长联系！', 0);
						}
					});
				});
			} else {
				$('#showInfoBtn').hide();
				$('#showTalkBtn').hide();
			}
			//获取音乐信息初始化
			var bgmListInfo = $('input[name=live2dBGM]');
			if (bgmListInfo.length == 0) {
				$('#musicButton').hide();
			} else {
				var bgmPlayNow = parseInt($('#live2d_bgm').attr('data-bgm'));
				var bgmPlayTime = 0;
				var live2dBGM_Num = sessionStorage.getItem("live2dBGM_Num");
				var live2dBGM_PlayTime = sessionStorage.getItem("live2dBGM_PlayTime");
				if (live2dBGM_Num) {
					if (live2dBGM_Num <= $('input[name=live2dBGM]').length - 1) {
						bgmPlayNow = parseInt(live2dBGM_Num);
					}
				}
				if (live2dBGM_PlayTime) {
					bgmPlayTime = parseInt(live2dBGM_PlayTime);
				}
				var live2dBGMSrc = bgmListInfo.eq(bgmPlayNow).val();
				$('#live2d_bgm').attr('data-bgm', bgmPlayNow);
				$('#live2d_bgm').attr('src', live2dBGMSrc);
				$('#live2d_bgm')[0].currentTime = bgmPlayTime;
				$('#live2d_bgm')[0].volume = 0.5;
				var live2dBGM_IsPlay = sessionStorage.getItem("live2dBGM_IsPlay");
				var live2dBGM_WindowClose = sessionStorage.getItem("live2dBGM_WindowClose");
				if (live2dBGM_IsPlay == '0' && live2dBGM_WindowClose == '0') {
					$('#live2d_bgm')[0].play();
					$('#musicButton').addClass('play');
				}
				sessionStorage.setItem("live2dBGM_WindowClose", '1');
				$('#musicButton').on('click', function () {
					if ($('#musicButton').hasClass('play')) {
						$('#live2d_bgm')[0].pause();
						$('#musicButton').removeClass('play');
						sessionStorage.setItem("live2dBGM_IsPlay", '1');
					} else {
						$('#live2d_bgm')[0].play();
						$('#musicButton').addClass('play');
						sessionStorage.setItem("live2dBGM_IsPlay", '0');
					}
				});
				window.onbeforeunload = function () {
					sessionStorage.setItem("live2dBGM_WindowClose", '0');
					if ($('#musicButton').hasClass('play')) {
						sessionStorage.setItem("live2dBGM_IsPlay", '0');
					}
				}
				document.getElementById('live2d_bgm').addEventListener("timeupdate", function () {
					var live2dBgmPlayTimeNow = document.getElementById('live2d_bgm').currentTime;
					sessionStorage.setItem("live2dBGM_PlayTime", live2dBgmPlayTimeNow);
				});
				document.getElementById('live2d_bgm').addEventListener("ended", function () {
					var listNow = parseInt($('#live2d_bgm').attr('data-bgm'));
					listNow++;
					if (listNow > $('input[name=live2dBGM]').length - 1) {
						listNow = 0;
					}
					var listNewSrc = $('input[name=live2dBGM]').eq(listNow).val();
					sessionStorage.setItem("live2dBGM_Num", listNow);
					$('#live2d_bgm').attr('src', listNewSrc);
					$('#live2d_bgm')[0].play();
					$('#live2d_bgm').attr('data-bgm', listNow);
				});
				document.getElementById('live2d_bgm').addEventListener("error", function () {
					$('#live2d_bgm')[0].pause();
					$('#musicButton').removeClass('play');
					showMessage('音乐似乎加载不出来了呢！', 0);
				});
			}
			//获取用户名
			var live2dUser = sessionStorage.getItem("live2duser");
			if (live2dUser !== null) {
				$('#AIuserName').val(live2dUser);
			}
			//获取位置
			var landL = sessionStorage.getItem("historywidth");
			var landB = sessionStorage.getItem("historyheight");
			if (landL == null || landB == null) {
				landL = '5px'
				landB = '0px'
			}
			$('#landlord').css('left', landL + 'px');
			$('#landlord').css('bottom', landB + 'px');
			//移动
			function getEvent() {
				return window.event || arguments.callee.caller.arguments[0];
			}
			var smcc = document.getElementById("landlord");
			var moveX = 0;
			var moveY = 0;
			var moveBottom = 0;
			var moveLeft = 0;
			var moveable = false;
			var docMouseMoveEvent = document.onmousemove;
			var docMouseUpEvent = document.onmouseup;
			smcc.onmousedown = function () {
				var ent = getEvent();
				moveable = true;
				moveX = ent.clientX;
				moveY = ent.clientY;
				var obj = smcc;
				moveBottom = parseInt(obj.style.bottom);
				moveLeft = parseInt(obj.style.left);
				if (isFirefox = navigator.userAgent.indexOf("Firefox") > 0) {
					window.getSelection().removeAllRanges();
				}
				document.onmousemove = function () {
					if (moveable) {
						var ent = getEvent();
						var x = moveLeft + ent.clientX - moveX;
						var y = moveBottom + (moveY - ent.clientY);
						obj.style.left = x + "px";
						obj.style.bottom = y + "px";
					}
				};
				document.onmouseup = function () {
					if (moveable) {
						var historywidth = obj.style.left;
						var historyheight = obj.style.bottom;
						historywidth = historywidth.replace('px', '');
						historyheight = historyheight.replace('px', '');
						sessionStorage.setItem("historywidth", historywidth);
						sessionStorage.setItem("historyheight", historyheight);
						document.onmousemove = docMouseMoveEvent;
						document.onmouseup = docMouseUpEvent;
						moveable = false;
						moveX = 0;
						moveY = 0;
						moveBottom = 0;
						moveLeft = 0;
					}
				};
			};
		}
		$(document).ready(function () {
			var AIimgSrc = [];
			let chooseLive2d = 'hijiki'
			if (chooseLive2d === 'histoire') {
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_00.png");
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_01.png");
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_02.png");
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_03.png");
			} else if (chooseLive2d === 'rem') {
				AIimgSrc.push(message_Path + "model/rem/remu2048/texture_00.png");
			} else if (chooseLive2d === 'Aoba') {
				AIimgSrc.push(message_Path + "model/Aoba/textures/texture_00.png");
			} else if (chooseLive2d === 'hijiki') {
				AIimgSrc.push(message_Path + "model/hijiki/moc/hijiki.2048/texture_00.png");
			} else if (chooseLive2d === 'tororo') {
				AIimgSrc.push(message_Path + "model/tororo/moc/tororo.2048/texture_00.png");
			}
			var images = [];
			var imgLength = AIimgSrc.length;
			var loadingNum = 0;
			for (var i = 0; i < imgLength; i++) {
				images[i] = new Image();
				images[i].src = AIimgSrc[i];
				images[i].onload = function () {
					loadingNum++;
					if (loadingNum === imgLength) {
						var live2dhidden = localStorage.getItem("live2dhidden");
						if (live2dhidden === "0") {
							setTimeout(function () {
								$('#open_live2d').fadeIn(200);
							}, 1300);
						} else {
							setTimeout(function () {
								$('#landlord').fadeIn(200);
							}, 1300);
						}
						let model = '';
						if (chooseLive2d === 'histoire') {
							model = message_Path + "model/histoire/model.json";
						} else if (chooseLive2d === 'rem') {
							model = message_Path + "model/rem/model.json";
						} else if (chooseLive2d === 'Aoba') {
							model = message_Path + "model/Aoba/model.json";
						} else if (chooseLive2d === 'hijiki') {
							model = message_Path + "model/hijiki/hijiki.model.json";
						} else if (chooseLive2d === 'tororo') {
							model = message_Path + "model/tororo/tororo.model.json";
						}
						setTimeout(function () {
							loadlive2d("live2d", model);
						}, 1000);
						initLive2d();
						images = null;
					}
				}
			}
		});
	}
</script>
  
  
</div>
<script>

  let sideBarOpen = 'sidebar-open';
  let body = document.body;
  let back2Top = document.querySelector('#back_to_top'),
  back2TopText = document.querySelector('#back_to_top_text'),
  drawerBox = document.querySelector('#drawer_box'),
  rightSideBar = document.querySelector('.sidebar'),
  viewport = document.querySelector('body');

  function scrollAnimation(currentY, targetY) {
   
    let needScrollTop = targetY - currentY
    let _currentY = currentY
    setTimeout(() => {
      const dist = Math.ceil(needScrollTop / 10)
      _currentY += dist
      window.scrollTo(_currentY, currentY)
      if (needScrollTop > 10 || needScrollTop < -10) {
        scrollAnimation(_currentY, targetY)
      } else {
        window.scrollTo(_currentY, targetY)
      }
    }, 1)
  }

  back2Top.addEventListener("click", function(e) {
    scrollAnimation(document.scrollingElement.scrollTop, 0);
    e.stopPropagation();
    return false;
  });
  
  window.addEventListener('scroll', function(e) {
    let percent = document.scrollingElement.scrollTop / (document.scrollingElement.scrollHeight - document.scrollingElement.clientHeight) * 100;
    if (percent > 1 && !back2Top.classList.contains('back-top-active')) {
      back2Top.classList.add('back-top-active');
    }
    if (percent == 0) {
      back2Top.classList.remove('back-top-active');
    }
    if (back2TopText) {
      back2TopText.textContent = Math.floor(percent);
    }
  });

  
  let hasCacu = false;
  window.onresize = function() {
    if (window.width > 991) {
      calcuHeight();
    } else {
      hasCacu = false;
    }
  }

  function calcuHeight() {
    // 动态调整站点概览高度
    if (!hasCacu && back2Top.classList.contains('pisces') || back2Top.classList.contains('gemini')) {
      let sideBar = document.querySelector('.sidebar');
      let navUl = document.querySelector('#site_nav');
      sideBar.style = 'margin-top:' + (navUl.offsetHeight + navUl.offsetTop + 15) + 'px;';
      hasCacu = true;
    }
  }
  calcuHeight();
  
  let open = false, MOTION_TIME = 300, RIGHT_MOVE_DIS = '320px';

  if (drawerBox) {
    let rightMotions = document.querySelectorAll('.right-motion');
    let right = drawerBox.classList.contains('right');

    let transitionDir = right ? "transition.slideRightIn" : "transition.slideLeftIn";

    let openProp, closeProp;
    if (right) {
      openProp = {
        paddingRight: RIGHT_MOVE_DIS 
      };
      closeProp = {
        paddingRight: '0px'
      };
    } else {
      openProp = {
        paddingLeft: RIGHT_MOVE_DIS 
      };
      closeProp = {
        paddingLeft: '0px'
      };
    }

    drawerBox.onclick = function() {
      open = !open;
      window.Velocity(rightSideBar, 'stop');
      window.Velocity(viewport, 'stop');
      window.Velocity(rightMotions, 'stop');
      if (open) {
        window.Velocity(rightSideBar, {
          width: RIGHT_MOVE_DIS
        }, {
          duration: MOTION_TIME,
          begin: function() {
            window.Velocity(rightMotions, transitionDir,{ });
          }
        })
        window.Velocity(viewport, openProp,{
          duration: MOTION_TIME
        });
      } else {
        window.Velocity(rightSideBar, {
          width: '0px'
        }, {
          duration: MOTION_TIME,
          begin: function() {
            window.Velocity(rightMotions, {
              opacity: 0
            });
          }
        })
        window.Velocity(viewport, closeProp ,{
          duration: MOTION_TIME
        });
      }
      for (let i = 0; i < drawerBox.children.length; i++) {
        drawerBox.children[i].classList.toggle('muse-line');
      }
      drawerBox.classList.toggle(sideBarOpen);
    }
  }

  // 链接跳转
  let newWindow = 'false'
  if (newWindow === 'true') {
    let links = document.querySelectorAll('.post-body a')
    links.forEach(item => {
      if (!item.classList.contains('btn')) {
        item.setAttribute("target","_blank");
      }
    })
  }
  // 代码高亮
  hljs.initHighlightingOnLoad();

</script>
    <div class="light-box" id="light_box"></div>
<script>
  let imgs = document.querySelectorAll('.post-body img');
  let lightBox = document.querySelector('#light_box');
  lightBox.addEventListener('mousedown', (e) => {
    e.preventDefault()
  })
  lightBox.addEventListener('mousewheel', (e) => {
    e.preventDefault()
  })
  let width = window.innerWidth * 0.8;
  lightBox.onclick = () => {
    let img = lightBox.querySelector('img');
    lightBox.style = '';
    img && img.remove();
  }
  imgs.forEach(item => {
    item.onclick = function (e) {
      let lightImg = document.createElement('img');
      lightImg.src = this.src;
      lightBox.style = `height: 100%; opacity: 1; background-color: rgba(0, 0, 0, 0.5);cursor: zoom-out;`;
      lightImg.style = `width: ${width}px; border: 1px solid #fff; border-radius: 2px;`;
      lightImg.onclick = function () {
        lightBox.style = '';
        this.remove();
      }
      lightBox.append(lightImg);
    }
  })
</script>
  </div>
</body>
<input hidden id="copy" />
<script>
  //拿来主义(真香)^_^，Clipboard 实现摘自掘金 https://juejin.im/post/5aefeb6e6fb9a07aa43c20af
  window.Clipboard = (function (window, document, navigator) {
    var textArea,
      copy;

    // 判断是不是ios端
    function isOS() {
      return navigator.userAgent.match(/ipad|iphone/i);
    }
    //创建文本元素
    function createTextArea(text) {
      textArea = document.createElement('textArea');
      textArea.value = text;
      textArea.style.width = 0;
      textArea.style.height = 0;
      textArea.clientHeight = 0;
      textArea.clientWidth = 0;
      document.body.appendChild(textArea);
    }
    //选择内容
    function selectText() {
      var range,
        selection;

      if (isOS()) {
        range = document.createRange();
        range.selectNodeContents(textArea);
        selection = window.getSelection();
        selection.removeAllRanges();
        selection.addRange(range);
        textArea.setSelectionRange(0, 999999);
      } else {
        textArea.select();
      }
    }

    //复制到剪贴板
    function copyToClipboard() {
      try {
        document.execCommand("Copy")
      } catch (err) {
        alert("复制错误！请手动复制！")
      }
      document.body.removeChild(textArea);
    }

    copy = function (text) {
      createTextArea(text);
      selectText();
      copyToClipboard();
    };

    return {
      copy: copy
    };
  })(window, document, navigator);

  function copyCode(e) {
    if (e.srcElement.tagName === 'SPAN' && e.srcElement.classList.contains('copy-code')) {
      let code = e.currentTarget.querySelector('code');
      var text = code.innerText;
      if (e.srcElement.textContent === '复制成功') {
        console.log('复制操作频率过高');
        return;
      }
      e.srcElement.textContent = '复制成功';
      (function (elem) {
        setTimeout(() => {
          if (elem.textContent === '复制成功') {
            elem.textContent = '复制代码'
          }
        }, 1000);
      })(e.srcElement)
      Clipboard.copy(text);
    }
  }

  let pres = document.querySelectorAll('pre');
  pres.forEach(pre => {
    let code = pre.querySelector('code');
    let copyElem = document.createElement('span');
    copyElem.classList.add('copy-code');
    copyElem.textContent = '复制代码';
    pre.appendChild(copyElem);
    pre.onclick = copyCode
  })
</script>
<script src="/media/js/motion.js"></script>

<script src="https://cdn.jsdelivr.net/gh/cferdinandi/smooth-scroll/dist/smooth-scroll.polyfills.min.js"></script>
<script>
  var scroll = new SmoothScroll('a[href*="#"]', {
    speed: 500
  });
</script>

<!-- <div class="search-mask" id="search_mask">
  <div class="search-box">
    <div class="search-title">
      <i class="fa fa-search"></i>
      <div class="input-box">
        <input type="text" placeholder="搜索">
      </div>
      <i class="fa fa-times-circle"></i>
    </div>
    <div class="result">
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/kafka-xiao-fei-ming-ling/"" data-c="
          &lt;p&gt;模拟生产消息：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bin/kafka-console-producer.sh --broker-list 192.168.101.100:9092 --topic flink-test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;模拟消费数据(从开始位置消费)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --bootstrap-server  192.168.101.100:9092  --topic flink-test --from-beginning  |head
&lt;/code&gt;&lt;/pre&gt;
">kafka消费命令</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/logstash-output-file-to-hdfs/"" data-c="
          &lt;p&gt;logstash 直接把文件内容写入 hdfs 中， 并支持 hdfs 压缩格式。&lt;br&gt;
logstash 需要安装第三方插件，webhdfs插件，通过hdfs的web接口写入。&lt;br&gt;
即 http://namenode00:50070/webhdfs/v1/ 接口&lt;/p&gt;
&lt;p&gt;新版本logstash已默认安装webhdfs插件&lt;br&gt;
官网地址及使用说明：&lt;br&gt;
https://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html&lt;br&gt;
检查hdfs的webhds接口&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -i  &amp;quot;http://namenode:50070/webhdfs/v1/?user.name=hadoop&amp;amp;op=LISTSTATUS&amp;quot;   
HTTP/1.1 200 OK
Cache-Control: no-cache
Expires: Thu, 13 Jul 2017 04:53:39 GMT
Date: Thu, 13 Jul 2017 04:53:39 GMT
Pragma: no-cache
Expires: Thu, 13 Jul 2017 04:53:39 GMT
Date: Thu, 13 Jul 2017 04:53:39 GMT
Pragma: no-cache
Content-Type: application/json
Set-Cookie: hadoop.auth=&amp;quot;u=hadoop&amp;amp;p=hadoop&amp;amp;t=simple&amp;amp;e=1499957619679&amp;amp;s=KSxdSAtjXAllhn73vh1MAurG9Bk=&amp;quot;; Path=/; Expires=Thu, 13-Jul-2017 14:53:39 GMT; HttpOnly
Transfer-Encoding: chunked
Server: Jetty(6.1.26)
注释： active namenode 返回是200 ，standby namenode 返回是403.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;测试hdfs是否正常通讯：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#通过webhdfs接口创建test.conf
curl -i -X PUT &amp;quot;http://hadoop-master:50070/webhdfs/v1/data/test.conf?user.name=hdfs&amp;amp;op=CREATE&amp;quot;
curl -i -T test.conf &amp;quot;http://hadoop-slave1:50075/webhdfs/v1/data/test.conf?op=CREATE&amp;amp;user.name=hdfs&amp;amp;namenoderpcaddress=hadoop-master:9000&amp;amp;overwrite=false&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;配置&lt;br&gt;
添加 logstash 一个配置文件&lt;/p&gt;
&lt;p&gt;vim /home/mtime/logstash-2.3.1/conf/hdfs.conf&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input {
    kafka {
        bootstrap_servers =&amp;gt; &amp;quot;192.168.101.22:9092,192.168.101.23:9092,192.168.101.93:9092&amp;quot;
        topics =&amp;gt; &amp;quot;test-logs&amp;quot;
        group_id =&amp;gt; &amp;quot;hdfs-test-logs&amp;quot;
        codec =&amp;gt; json
	    consumer_threads =&amp;gt; 15

    }
}

filter {
    date {
        match =&amp;gt; [&amp;quot;time&amp;quot;,&amp;quot;yyyy-MM-dd HH:mm:ss Z&amp;quot;]
        target =&amp;gt; &amp;quot;@timestamp&amp;quot;
        timezone =&amp;gt; &amp;quot;Asia/Shanghai&amp;quot;
    }
    ruby {
        code =&amp;gt; &amp;quot;event.set(&#39;index.date&#39;, event.get(&#39;@timestamp&#39;).time.localtime.strftime(&#39;%Y%m%d&#39;))&amp;quot;
    }
    ruby {
        code =&amp;gt; &amp;quot;event.set(&#39;index.hour&#39;, event.get(&#39;@timestamp&#39;).time.localtime.strftime(&#39;%H&#39;))&amp;quot;
    }
}
output {            
    webhdfs {
           host =&amp;gt; &amp;quot;hadoop-master&amp;quot;
           port =&amp;gt; 50070
           path =&amp;gt; &amp;quot;/data/pt-collect-log/test-logs/%{index.date}/application-%{index.hour}.log&amp;quot;
           user =&amp;gt; &amp;quot;hdfs&amp;quot;
	   codec =&amp;gt; line { format =&amp;gt; &amp;quot;%{message}&amp;quot;}
           flush_size =&amp;gt; 1000
           compression =&amp;gt; &amp;quot;gzip&amp;quot;            
           idle_flush_time =&amp;gt; 10
           retry_interval =&amp;gt; 3
	   retry_times =&amp;gt; 100
       }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;关于hdfs部分配置，可以在 plugins-outputs-webhdfs 官网找到&lt;br&gt;
启动 logstart&lt;br&gt;
cd /home/mtime/logstash-2.3.1/bin/&lt;br&gt;
./logstash -f ../conf/hdfs.conf    # 为前台启动&lt;br&gt;
报错处理&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[WARN ][logstash.outputs.webhdfs ] Failed to flush outgoing items {:outgoing_count=&amp;gt;160, :exception=&amp;gt;&amp;quot;WebHDFS::IOError&amp;quot;,
我将hdfs端口 由原来的50070 改为 14000 端口，就在不报错了。
官方提供的例子中用的就是50070端口，一直没有尝试14000端口。

还有：
because this file lease is currently owned by DFSClient
hadoop 租约问题，后期正常就没有了。
执行recoverLease来释放文件的锁

$ hdfs debug recoverLease -path /logstash/2017/02/10/go-03.log
还有：
:message=&amp;gt;&amp;quot;webhdfs write caused an exception: {\&amp;quot;RemoteException\&amp;quot;:{\&amp;quot;message\&amp;quot;:\&amp;quot;Failed to APPEND_FILE
当一个进程在读写这个文件的时候，另一个进程应该是不能同时写入的。
我们由原来3个logstash同时消费，改为了1个logstash消费，不在报错了。
这个应该也可以通过有话写入hdfs参数来解决。

还有：
Max write retries reached. Exception: initialize: name or service not known {:level=&amp;gt;:error}
losgstash 需要能解析所有 hadoop 集群所有节点的主机名。
&lt;/code&gt;&lt;/pre&gt;
">logstash output file to HDFS</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/ceph-bao-cuo-guan-li/"" data-c="
          &lt;p&gt;使用ceph -s查看集群状态，发现一直有如下报错，且数量一直在增加&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;daemons have recently crashed
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;经查当前系统运行状态正常，判断这里显示的应该是历史故障，处理方式如下：&lt;/p&gt;
&lt;p&gt;查看历史crash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ceph crash ls-new
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据ls出来的id查看详细信息&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ceph crash info &amp;lt;crash-id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将历史crash信息进行归档，即不再显示&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ceph crash archive &amp;lt;crash-id&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;归档所有信息&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ceph crash archive-all
&lt;/code&gt;&lt;/pre&gt;
">ceph报错管理</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/golang-bi-ji-stringintint64-hu-xiang-zhuan-huan/"" data-c="
          &lt;pre&gt;&lt;code&gt;#string到int  
int,err:=strconv.Atoi(string)  
#string到int64  
int64, err := strconv.ParseInt(string, 10, 64)  
#int到string  
string:=strconv.Itoa(int)  
#int64到string  
string:=strconv.FormatInt(int64,10)  ```

同类型之间转换，比如int64到int，直接int(int64)即可；
&lt;/code&gt;&lt;/pre&gt;
">golang笔记-string、int、int64互相转换</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/golang-bi-ji-go-restful/"" data-c="
          &lt;p&gt;用golang写一个restful api。如果您不知道什么是restful,可以看&lt;a href=&#34;http://www.ruanyifeng.com/blog/2014/05/restful_api.html&#34;&gt;阮一峰老师的教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;首先，我们需要解决的是路由的问题，也就是如何将不同的url映射到不同的处理函数。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    router.GET(&amp;quot;/api/todo/:todoid&amp;quot;, getTodoById)
    router.POST(&amp;quot;/api/todo/&amp;quot;, addTodo)
    router.DELETE(&amp;quot;/api/todo/:todoid&amp;quot;, deleteTodo)
    router.PUT(&amp;quot;/api/todo/:todoid&amp;quot;, modifyTodo)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;作为一个初学者，我马上打开github,找到了&lt;a href=&#34;https://github.com/avelino/awesome-go&#34;&gt;awesome-go&lt;/a&gt;,经过一番调研，我感觉有几个http router的库比较适合：bone, httprouter, mux&lt;/p&gt;
&lt;h2 id=&#34;基本框架&#34;&gt;基本框架&lt;/h2&gt;
&lt;p&gt;首先，我们设计了四个路由，分别为根据Id获得todo，增加todo，修改todo，删除todo。这里关于解析路由参数，我们使用了httprouter.Params的ByName函数。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;package main

import (
  &amp;quot;fmt&amp;quot;
  &amp;quot;github.com/julienschmidt/httprouter&amp;quot;
  &amp;quot;net/http&amp;quot;
  &amp;quot;log&amp;quot;
  &amp;quot;io&amp;quot;
  &amp;quot;io/ioutil&amp;quot;
)

func getTodoById(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&amp;quot;todoid&amp;quot;)
  fmt.Fprintf(w, &amp;quot;getTodo %s\n&amp;quot;, todoid)
}

func addTodo(w http.ResponseWriter, r *http.Request, _ httprouter.Params){
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  fmt.Fprintf(w, &amp;quot;addTodo! %s\n&amp;quot;,body)
}

func deleteTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&amp;quot;todoid&amp;quot;)
  fmt.Fprintf(w, &amp;quot;deleteTodo %s\n&amp;quot;, todoid)
}

func modifyTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&amp;quot;todoid&amp;quot;)
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  fmt.Fprintf(w, &amp;quot;modifyTodo %s to %s\n&amp;quot;, todoid, body)
}

func main() {
    router := httprouter.New()
    router.GET(&amp;quot;/api/todo/:todoid&amp;quot;, getTodoById)
    router.POST(&amp;quot;/api/todo/&amp;quot;, addTodo)
    router.DELETE(&amp;quot;/api/todo/:todoid&amp;quot;, deleteTodo)
    router.PUT(&amp;quot;/api/todo/:todoid&amp;quot;, modifyTodo)
    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, router))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们可以用curl来测试一下我们的api,以put为例&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --data &amp;quot;content=shopping&amp;amp;time=tomorrow&amp;quot; http://127.0.0.1:8080/api/todo/123 -X PUT

// modifyTodo 123 to content=shopping&amp;amp;time=tomorrow
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;json的解析&#34;&gt;json的解析&lt;/h2&gt;
&lt;p&gt;我们在使用restful api的时候，常常需要给后台传递数据。从上面可以看到，我们通过http.Request的Body属性可以获得数据&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从上面，我们读出的数据是[]byte，但是我们希望将其解析为对象，那么在这之前，我们需要先定义我们的struct。假设我们的todo只有一个字段，就是Name&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type Todo struct {
    Name      string
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;现在我们可以这样解析&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var todo Todo;
json.Unmarshal(body, &amp;amp;todo);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;model层设计&#34;&gt;model层设计&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;package main

import (
  &amp;quot;gopkg.in/mgo.v2&amp;quot;
  &amp;quot;fmt&amp;quot;
  &amp;quot;log&amp;quot;
  &amp;quot;gopkg.in/mgo.v2/bson&amp;quot;
)

var session *mgo.Session

func init(){
  session,_ = mgo.Dial(&amp;quot;mongodb://127.0.0.1&amp;quot;)
}

type Todo struct {
    Name      string
}

func createTodo(t Todo){
  c := session.DB(&amp;quot;test&amp;quot;).C(&amp;quot;todo&amp;quot;)
  c.Insert(&amp;amp;t)
}

func queryTodoById(id string){
  c := session.DB(&amp;quot;test&amp;quot;).C(&amp;quot;todo&amp;quot;)
  result := Todo{}

  err := c.Find(bson.M{&amp;quot;_id&amp;quot;: bson.ObjectIdHex(id)}).One(&amp;amp;result)
  if err != nil {
    log.Fatal(err)
  }

  fmt.Println(&amp;quot;Todo:&amp;quot;, result.Name)
}

func removeTodo(id string){
  c := session.DB(&amp;quot;test&amp;quot;).C(&amp;quot;todo&amp;quot;)
  err := c.Remove(bson.M{&amp;quot;_id&amp;quot;: bson.ObjectIdHex(id)})
  if err != nil{
    log.Fatal(err)
  }
}

func updateTodo(id string, update interface{}){
  //change := bson.M{&amp;quot;$set&amp;quot;: bson.M{&amp;quot;name&amp;quot;: &amp;quot;hahaha&amp;quot;}}
  c := session.DB(&amp;quot;test&amp;quot;).C(&amp;quot;todo&amp;quot;)
  err := c.Update(bson.M{&amp;quot;_id&amp;quot;: bson.ObjectIdHex(id)}, update)
  if err != nil{
    log.Fatal(err)
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们定义了Todo的struct,并添加了几种函数。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;package main

import (
  &amp;quot;fmt&amp;quot;
  &amp;quot;github.com/julienschmidt/httprouter&amp;quot;
  &amp;quot;net/http&amp;quot;
  &amp;quot;log&amp;quot;
  &amp;quot;io&amp;quot;
  &amp;quot;io/ioutil&amp;quot;
  &amp;quot;encoding/json&amp;quot;
  &amp;quot;gopkg.in/mgo.v2/bson&amp;quot;
)

func getTodoById(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&amp;quot;todoid&amp;quot;)
  queryTodoById(todoid)
  fmt.Fprintf(w, &amp;quot;getUser %s\n&amp;quot;, todoid)
}

func addTodo(w http.ResponseWriter, r *http.Request, _ httprouter.Params){
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  var todo Todo;
  json.Unmarshal(body, &amp;amp;todo);
  createTodo(todo)
  fmt.Fprintf(w, &amp;quot;addUser! %s\n&amp;quot;,body)
}

func deleteTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&amp;quot;todoid&amp;quot;)
  removeTodo(todoid)
  fmt.Fprintf(w, &amp;quot;deleteUser %s\n&amp;quot;, todoid)
}

func modifyTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&amp;quot;todoid&amp;quot;)
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  var todo Todo
  json.Unmarshal(body, &amp;amp;todo);
  change := bson.M{&amp;quot;$set&amp;quot;: bson.M{&amp;quot;name&amp;quot;: todo.Name}}
  updateTodo(todoid,change)
  fmt.Fprintf(w, &amp;quot;modifyUser %s to %s\n&amp;quot;, todoid, body)
}

func main() {
    router := httprouter.New()
    router.GET(&amp;quot;/api/todo/:todoid&amp;quot;, getTodoById)
    router.POST(&amp;quot;/api/todo/&amp;quot;, addTodo)
    router.DELETE(&amp;quot;/api/todo/:todoid&amp;quot;, deleteTodo)
    router.PUT(&amp;quot;/api/todo/:todoid&amp;quot;, modifyTodo)
    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, router))
}&lt;/code&gt;&lt;/pre&gt;
">golang笔记-go-restful</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/jenkins-x-on-kubernetes-shi-jian-zhi-chi-duo-zhu/"" data-c="
          &lt;h2 id=&#34;jenkins是什么&#34;&gt;jenkins是什么？&lt;/h2&gt;
&lt;p&gt;Jenkins是一个开源的持续集成工具，可用于自动化的执行与构建，测试和交付或部署软件有关的各种任务,有非常丰富的插件支持。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes是什么&#34;&gt;kubernetes是什么？&lt;/h2&gt;
&lt;p&gt;Kubernetes是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。这个视频生动地介绍了k8s&lt;/p&gt;
&lt;h2 id=&#34;jenkins-on-k8s-有什么好处&#34;&gt;jenkins on k8s 有什么好处？&lt;/h2&gt;
&lt;p&gt;jenkins通过单Master多个Slave的方式提供服务，Master保存了任务的配置信息，安装的插件等等，而slave主要负责执行任务，在使用中存在以下几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;当存在多个slave时，运行slave的机器难以统一管理，每次添加新节点时总要做大量的重复工作。&lt;/li&gt;
&lt;li&gt;由于不同业务的构建频率并不相同，在使用会发现有很多slave大多数时间都处于空闲状态，造成资源浪费&lt;/li&gt;
&lt;li&gt;jenkins默认采取保守的调度方式，造成某些slave的负载过高，任务不能平均分配&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;jenkins架构&#34;&gt;jenkins架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1592447693022.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
使用k8s管理jenkins具有以下优势：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用docker运行jenkins保证环境的一致性，可以根据不同业务选择合适的镜像&lt;/li&gt;
&lt;li&gt;k8s对抽象后的资源（pods）进行统一的管理调度，提供资源隔离和共享，使机器计算资源变得弹性可扩展,避免资源浪费。&lt;/li&gt;
&lt;li&gt;k8s提供容器的自愈功能，能够保证始终有一定数量的容器是可用的&lt;/li&gt;
&lt;li&gt;k8s默认的调度器提供了针对节点当前资源分配容器的调度策略，调度器支持插件化部署便于自定义。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;一搭建环境&#34;&gt;一，搭建环境&lt;/h2&gt;
&lt;h3 id=&#34;工具准备&#34;&gt;工具准备&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kubernetes v1.8.4
docker v1.12.6
jenkins master镜像 jenkins/jenkins:lts（v2.73.3）
slave镜像 jenkinsci/jnlp-slave
Kubernetes plugin (v1.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;安装kubernetes集群&#34;&gt;安装kubernetes集群&lt;/h3&gt;
&lt;p&gt;中文教程：https://www.kubernetes.org.cn/2906.html&lt;br&gt;
省略.....&lt;/p&gt;
&lt;h2 id=&#34;二创建statefulset&#34;&gt;二，创建StatefulSet&lt;/h2&gt;
&lt;p&gt;StatefulSet(有状态副本集)：Deployments适用于运行无状态应用，StatefulSet则为有状态的应用提供了支持，可以为应用提供有序的部署和扩展，稳定的持久化存储，我们使用SS来运行jenkins master。&lt;/p&gt;
&lt;p&gt;创建完整的Stateful Set需要依次创建一下对象：&lt;br&gt;
1、Persistent Volume&lt;br&gt;
2、Persistent Volume Claim&lt;br&gt;
3、StatefulSet&lt;br&gt;
4、Service&lt;/p&gt;
&lt;p&gt;创建PersistentVolume：&lt;br&gt;
为了保存应用运行时的数据需要先创建k8s的卷文件，K8s中存在Volume和PersistentVolume两种类型：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Volume：与docker中的volume不同在于Volume生命周期是和pod绑定的，与pod中的container无关。k8s为Volume提供了多种类型文件系统（cephfs,nfs…,简单起见我直接选择了hostPath，使用的node节点本地的存储系统）&lt;/li&gt;
&lt;li&gt;PersistentVolume:从名字可以看出来，PV的生命周期独立于使用它的pod，不会像volume随pod消失而消失，而是做为一个集群中的资源存在（像node节点一样），同时PV屏蔽了使用具体存储系统的细节。&lt;br&gt;
k8s中的对象都是通过yaml文件来定义的，首先创建名为jenkins-volume.yml的文件:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;❣️❣️注意：PV的创建有静态，动态两种方式，动态创建可以减少管理员的操作步骤，需要提供指定的StorageClass。为了测试方便，所以我们直接选择静态创建，manual是一个不存在的storage class&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: PersistentVolume
apiVersion: v1
metadata:
  name: jenkins-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &amp;quot;/tmp/data&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;master节点执行下面的命令，PV就手动创建完了&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f jenkins-volume1.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建PersistentVolumeClaim：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PersistentVolumeClaim(PVC):
持久化存储卷索取，如果说PV是集群中的资源，PVC就是资源的消费者，PVC可以指定需要的资源大小和访问方式,pod不会和PV直接接触，而是通过PVC来请求资源，PV的生成阶段叫做provision,生成PV后会绑定到PVC对象，然后才能被其他对象使用。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PV和PVC的生命周期如下图：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1592447954632.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;pv life&lt;br&gt;
创建文件jenkins-claim.yaml&lt;br&gt;
注意： name必须为jenkins-home-jenkins-0否则会绑定失败&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: jenkins-home-jenkins-0
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;执行命令kubectl create -f jenkins-claim.yaml&lt;br&gt;
然后查看PVC是否创建成功，status为bound说明PVC已经绑定&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl describe pvc jenkins-home-jenkins-0
Name:          jenkins-home-jenkins-0
Namespace:     kubernetes-plugin
StorageClass:  manual
Status:        Bound
Volume:        jenkins-volume
Labels:        &amp;lt;none&amp;gt;
Annotations:   pv.kubernetes.io/bind-completed=yes
               pv.kubernetes.io/bound-by-controller=yes
Capacity:      10Gi
Access Modes:  RWO
Events:        &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建StatefulSet和Service：&lt;br&gt;
从kubernetes-plugin github仓库下载jenkins.yml文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/jenkinsci/kubernetes-plugin/master/src/main/kubernetes/jenkins.yml
修改jenkins.yml：
去掉87行externalTrafficPolicy: Local（这是GKE使用的）
修改83行type: LoadBalancer改为type: NodePort
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意：&lt;br&gt;
service type=ClusterIP时只允许从集群内部访问， type设置为NodePort是为了从集群外的机器访问jenkins,请谨慎使用，开启NodePort会在所有节点（含master）的统一固定端口开放服务。&lt;/p&gt;
&lt;p&gt;执行命令&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create -f jenkins.yml 
statefulset &amp;quot;jenkins&amp;quot; created
service &amp;quot;jenkins&amp;quot; created
访问jenkins master,地址为masterip:32058
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#查看映射的端口&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get service jenkins
NAME      TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)                        AGE
jenkins   NodePort   10.96.82.68   &amp;lt;none&amp;gt;        80:32058/TCP,50000:30345/TCP   1m

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看pod : jenkins-0的容器日志，粘贴下面的密码进入jenkins,jenkins安装完成。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Jenkins initial setup is required. An admin user has been created and a password generated.
Please use the following password to proceed to installation:
70aa7b41ba894855abccd09306625b8a
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;问题分析&#34;&gt;问题分析&lt;/h3&gt;
&lt;p&gt;1.创建stateful set时失败，提示”PersistentVolumeClaim is not bound: “jenkins-home-jenkins-0”：”&lt;br&gt;
因为采用静态创建PV时，StatefulSet会按照固定名称查找PVC，PVC的名字要满足&lt;/p&gt;
&lt;p&gt;PVC_name == volumeClaimTemplates_name + “-“ + pod_name&lt;/p&gt;
&lt;p&gt;这里的名字就是jenkins-home-jenkins-0&lt;/p&gt;
&lt;p&gt;2.pod启动失败，jenkins用户没有目录权限&lt;br&gt;
错误提示”touch: cannot touch ‘/var/jenkins_home/copy_reference_file.log’: Permission denied&lt;br&gt;
Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions?”&lt;br&gt;
要确保节点目录开放权限,在node上执行命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo chown -R 1000:1000 /var/jenkins_home/
sudo chown -R 1000:1000 /tmp/data
##如果仍然失败，尝试在node上重启docker
systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意pv指定的hostPath权限也要修改，否则是无效的&lt;/p&gt;
&lt;h2 id=&#34;三-配置jenkins&#34;&gt;三 ，配置jenkins&lt;/h2&gt;
&lt;p&gt;创建jenkins服务账号&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/jenkinsci/kubernetes-plugin/master/src/main/kubernetes/service-account.yml
kubectl create -f service-account.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;配置插件&lt;br&gt;
访问http://masterip:32058/pluginManager/,搜索插件Kubernetes plugin安装；&lt;br&gt;
访问 http://masterip:32058/configure&lt;br&gt;
选择新建云–kubernetes,在URl填写api server地址，&lt;br&gt;
执行kubectl describe命令，复制output中的token，填入到 ‘Kubernetes server certificate key’&lt;/p&gt;
&lt;p&gt;[root@master ~]# kubectl get secret&lt;br&gt;
NAME                  TYPE                                  DATA      AGE&lt;br&gt;
default-token-4kb54   kubernetes.io/service-account-token   3         1d&lt;br&gt;
jenkins-token-wzbsx   kubernetes.io/service-account-token   3         1d&lt;br&gt;
[root@master ~]# kubectl describe secret/jenkins-token-wzbsx&lt;br&gt;
...&lt;br&gt;
jenkins url,tunnel填写service的CLUSTER-IP即可，结果如图：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1592448124617.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;peizhi1&lt;br&gt;
选择add pod template，填写下面的内容，retain slave可以设置运行jenkins slave 的container空闲后能存活多久。&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1592448148171.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;content&lt;br&gt;
插件配置完成。&lt;/p&gt;
&lt;h2 id=&#34;四-测试&#34;&gt;四 ，测试&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;扩容测试&lt;br&gt;
StatefulSet扩容：&lt;br&gt;
首先需要手动创建PV，PVC(见第二步),然后执行扩容命令&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;kubectl scale statefulset/jenkins --replicas=２
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看StatefulSet,此时已经拥有两个master节点，访问service时会随机将请求发送给后端的master。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get statefulset/jenkins 
NAME      DESIRED   CURRENT   AGE
jenkins   2         2         5d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;虽然通过k8s可以轻松实现jenkins master节点的拓展，但是由于jenkins存储数据的方式通过本地文件存储，master之间的数据同步还是一个麻烦的问题，参考jenkins存储模型。&lt;/p&gt;
&lt;p&gt;jenkins master上保存的文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ls /temp/data
jenkins.CLI.xml
jenkins.install.InstallUtil.lastExecVersion
jenkins.install.UpgradeWizard.state
jenkins.model.ArtifactManagerConfiguration.xml
jenkins.model.JenkinsLocationConfiguration.xml
jobs
logs
nodeMonitors.xml
nodes
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;高可用测试&lt;br&gt;
现在stateful set中已经有两个pod,在jenkins-1所在的节点执行docker stop停止运行jenkins-master的容器，同时在命令行查看pod的状态，可以看到jenkins-1异常（Error状态）之后慢慢恢复了运行状态（Running）：&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get pods -w
NAME        READY     STATUS    RESTARTS   AGE
jenkins-0   1/1       Running   0          1d
jenkins-1   0/1       Running   1         20h
jenkins-1   1/1       Running   1         20h
jenkins-1   0/1       Error     1         20h
jenkins-1   0/1       CrashLoopBackOff   1         20h
jenkins-1   0/1       Running   2         20h
jenkins-1   1/1       Running   2         20h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;kubectl describe pod jenkins-1查看pod的事件日志，k8s通过探针(probe)接口检测到服务停止之后自动执行了拉取镜像，重启container的操作。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Events:
  Type     Reason      Age                From                              Message
  ----     ------      ----               ----                              -------
  Warning  Unhealthy   27m (x2 over 27m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Readiness probe failed: HTTP probe failed with statuscode: 503
  Warning  Unhealthy   27m (x2 over 27m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Liveness probe failed: HTTP probe failed with statuscode: 503
  Warning  Unhealthy   24m                kubelet, iz8pscwd1fv6kprs1zw21kz  Readiness probe failed: Get http://192.168.24.4:8080/login: dial tcp 192.168.24.4:8080: getsockopt: connection refused
  Warning  BackOff     20m (x2 over 20m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Back-off restarting failed container
  Warning  FailedSync  20m (x2 over 20m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Error syncing pod
  Normal   Pulling     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  pulling image &amp;quot;jenkins/jenkins:lts-alpine&amp;quot;
  Normal   Started     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Started container
  Normal   Pulled      19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Successfully pulled image &amp;quot;jenkins/jenkins:lts-alpine&amp;quot;
  Normal   Created     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Created container
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;jenkins构建测试&lt;br&gt;
当前集群中使用的jenkins slave镜像只包含一个java运行环境来运行jenkins-slave.jar,在实际使用中需要自定义合适的镜像。选择自定义镜像之后需要修改插件的配置，同样name命名为jnlp替换默认镜像，arguments安装工具提示填写即可。&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1592448208433.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
创建job，同时开始构建,k8s会在不同节点上创建pod来运行任务&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;jenkins默认调度策略&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;尝试在上次构建的节点上构建，指定某台slave之后会一直使用。&lt;/li&gt;
&lt;li&gt;当队列有2个构建时，不会立刻创建两个executor,而是先创建一个executor然后尝试等待executor空闲，目的是保证每个executor被充分利用。&lt;br&gt;
k8s调度策略&lt;br&gt;
使用Pod.spec.nodeSelector根据label为pod选择node&lt;br&gt;
3 .调度器scheduler有Predicates，Priorities两个阶段，分别负责节点过滤和评分排序，各个阶段都有k8s提供的检查项，我们可以自由组合。&lt;br&gt;
（比如PodFitsResources检查cpu内存等资源，PodFitsHostPorts检查端口占用，SelectorSpreadPriority要求一个服务尽量分散分布）自定义schduler参考&lt;br&gt;
资源不足时会发生什么&lt;br&gt;
当前集群中有3个节点，我在node2运行一个CPU占用限制在80%的程序,然后设置jenkins插件ContainerTemplate的request和limit均为cpu 500m,内存500Mi,（500m代表单核CPU的50%）看一下pod会怎么调度&lt;br&gt;
k8s仍然尝试在node2分配节点（为什么其他节点不行），结果POD处于pending状态：&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;{
&amp;quot;phase&amp;quot;: &amp;quot;Pending&amp;quot;,
&amp;quot;conditions&amp;quot;: [
  {
    &amp;quot;type&amp;quot;: &amp;quot;PodScheduled&amp;quot;,
    &amp;quot;status&amp;quot;: &amp;quot;False&amp;quot;,
    &amp;quot;lastProbeTime&amp;quot;: null,
    &amp;quot;lastTransitionTime&amp;quot;: &amp;quot;2017-12-09T08:29:10Z&amp;quot;,
    &amp;quot;reason&amp;quot;: &amp;quot;Unschedulable&amp;quot;,
    &amp;quot;message&amp;quot;: &amp;quot;No nodes are available that match all of the predicates: Insufficient cpu (4), PodToleratesNodeTaints (1).&amp;quot;
  }
],
&amp;quot;qosClass&amp;quot;: &amp;quot;Guaranteed&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后pod被删除，而jenkins任务会阻塞一直到有其他空闲的slave出现。&lt;/p&gt;
&lt;h2 id=&#34;五总结&#34;&gt;五，总结&lt;/h2&gt;
&lt;p&gt;本文介绍了在k8s集群部署jenkins服务的方式和k8s带来的资源管理便捷，由于我也是刚开始接触k8s,所用的实例只是搭建了用于测试的实验环境，离在实际生产环境中使用还有问题需要验证。&lt;/p&gt;
">jenkins x on kubernetes实践(支持多主)</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/consul-shan-chu-wu-xiao-fu-wu-yu-jie-dian/"" data-c="
          &lt;p&gt;consul删除无效实例&lt;br&gt;
http://127.0.0.1:8500/v1/agent/service/deregister/test-9c14fa595ddfb8f4c34c673c65b072bb&lt;/p&gt;
&lt;p&gt;test-9c14fa595ddfb8f4c34c673c65b072bb : 实例id&lt;br&gt;
method : put&lt;/p&gt;
&lt;p&gt;删除无效节点&lt;br&gt;
http://127.0.0.1:8500/v1/v1/agent/force-leave/4b36b27317a0&lt;/p&gt;
&lt;p&gt;consul leave #关闭consul并离开集群。也可以使用Ctrl+C或kill -INT来gracefully停止agent，这种体面的离开方式让consule可以有机会通知集群其他成员自己的离开。如果你强制地结束了agent，其他member会检测到这个节点的failed。当成员离开时，它的services和checks都会从catalog中移除。当成员failed时，它的health只是简单的被标记为critical，并不会从catalog中移除。Consul会自动尝试重新连接failed节点，允许它从恶劣的网络环境中恢复，显然离开的nodes不会被重新连接。另外，如果这个节点是server，体面的离开对避免潜在的中断的可能很重要。&lt;br&gt;
为了防止dead nodes的积累，consul会自动把dead nodes移除出catalog。这个过程被称为reaping（收割）。默认是72小时的间隔（不建议更改）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash
clear 
echo &amp;quot;node_exporter注销工具&amp;quot;
read -p &amp;quot;请输入要踢掉的节点IP,如果有多个IP,请使用英文格式 &#39;,&#39; 隔开: &amp;quot; IP_LIST

for IP in `echo &amp;quot;${IP_LIST}&amp;quot;|awk -F, &#39;BEGIN{OFS=&amp;quot; &amp;quot;}{$1=$1;printf(&amp;quot;%s&amp;quot;,$0);}&#39;`
do 
   curl -XPUT http://10.100.x.x:8500/v1/agent/service/deregister/node-${IP}
   echo &amp;quot;${IP}节点已剔除!&amp;quot;
done
echo &amp;quot;${IP_LIST}完成剔除&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
">consul-删除无效服务与节点</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/docker-nei-cun-shi-yong-lu-chai-yi-cgroup-memoryusage_in_bytes-yu-docker-rong-qi-nei-de-rss/"" data-c="
          &lt;p&gt;&amp;quot;kubernetes&amp;quot;（v1.10.2）表示我的pod（包含一个容器）使用了大约5GB的内存。在容器内部，RSS说的更像681Mib。任何人都能用以下数据解释如何从681Mib到5GB吗（或者用我省略的另一个命令来描述如何弥补差异，无论是从容器还是从在Kubernetes中运行此容器的Docker主机）？&lt;br&gt;
Kubectl Top Pods表示5GB：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% kubectl top pods -l app=myapp
NAME                             CPU(cores)   MEMORY(bytes)
myapp-56b947bf6d-2lcr7           39m          5039Mi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cadvisor报告了类似的数字（可能来自稍有不同的时间，因此请忽略细微的差异）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;container_memory_usage_bytes{pod_name=~&amp;quot;.*myapp.*&amp;quot;}      5309456384

5309456384 / 1024.0 / 1024 ~= 5063 ~= 5039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在容器中，此文件似乎是cadvisor获取其数据的位置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl exec -it myapp-56b947bf6d-2lcr7 bash
meme@myapp-56b947bf6d-2lcr7:/app# cat /sys/fs/cgroup/memory/memory.usage_in_bytes
5309456384
容器中的常驻集大小（RSS）不匹配（小于1GB）：
meme@myapp-56b947bf6d-2lcr7:/app# kb=$(ps aux | grep -v grep | grep -v &#39;ps aux&#39; | grep -v bash | grep -v awk | grep -v RSS | awk &#39;{print $6}&#39; | awk &#39;{s+=$1} END {printf &amp;quot;%.0f&amp;quot;, s}&#39;); mb=$(expr $kb / 1024); printf &amp;quot;Kb: $kb\nMb: $mb\n&amp;quot;
Kb: 698076
Mb: 681
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;完整的PS AUX，以防有帮助：
meme@myapp-56b947bf6d-2lcr7:/app# ps aux | grep -v grep | grep -v &#39;ps aux&#39; | grep -v bash | grep -v awk
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
meme         1  0.0  0.0 151840 10984 ?        Ss   Jun04   0:29 /usr/sbin/apache2 -D FOREGROUND
www-data    10  0.0  0.0 147340  4652 ?        S    Jun04   0:00 /usr/sbin/apache2 -D FOREGROUND
www-data    11  0.0  0.0 148556  4392 ?        S    Jun04   0:16 /usr/sbin/apache2 -D FOREGROUND
www-data    12  0.2  0.0 2080632 11348 ?       Sl   Jun04  31:58 /usr/sbin/apache2 -D FOREGROUND
www-data    13  0.1  0.0 2080384 10980 ?       Sl   Jun04  18:12 /usr/sbin/apache2 -D FOREGROUND
www-data    68  0.3  0.0 349048 94272 ?        Sl   Jun04  47:09 hotapp
www-data   176  0.2  0.0 349624 92888 ?        Sl   Jun04  43:11 hotapp
www-data   179  0.2  0.0 349196 94456 ?        Sl   Jun04  42:20 hotapp
www-data   180  0.3  0.0 349828 95112 ?        Sl   Jun04  44:14 hotapp
www-data   185  0.3  0.0 346644 91948 ?        Sl   Jun04  43:49 hotapp
www-data   186  0.3  0.0 346208 91568 ?        Sl   Jun04  44:27 hotapp
www-data   189  0.2  0.0 350208 95476 ?        Sl   Jun04  41:47 hotapp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Docker容器统计API中的内存部分：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --unix-socket /var/run/docker.sock &#39;http:/v1.24/containers/a45fc651e7b12f527b677e6a46e2902786bee6620484922016a135e317a42b4e/stats?stream=false&#39; | jq . # yields:

&amp;quot;memory_stats&amp;quot;: {
  &amp;quot;usage&amp;quot;: 5327712256,
  &amp;quot;max_usage&amp;quot;: 5368344576,
  &amp;quot;stats&amp;quot;: {
    &amp;quot;active_anon&amp;quot;: 609095680,
    &amp;quot;active_file&amp;quot;: 74457088,
    &amp;quot;cache&amp;quot;: 109944832,
    &amp;quot;dirty&amp;quot;: 28672,
    &amp;quot;hierarchical_memory_limit&amp;quot;: 5368709120,
    &amp;quot;inactive_anon&amp;quot;: 1687552,
    &amp;quot;inactive_file&amp;quot;: 29974528,
    &amp;quot;mapped_file&amp;quot;: 1675264,
    &amp;quot;pgfault&amp;quot;: 295316278,
    &amp;quot;pgmajfault&amp;quot;: 77,
    &amp;quot;pgpgin&amp;quot;: 85138921,
    &amp;quot;pgpgout&amp;quot;: 84964308,
    &amp;quot;rss&amp;quot;: 605270016,
    &amp;quot;rss_huge&amp;quot;: 0,
    &amp;quot;shmem&amp;quot;: 5513216,
    &amp;quot;total_active_anon&amp;quot;: 609095680,
    &amp;quot;total_active_file&amp;quot;: 74457088,
    &amp;quot;total_cache&amp;quot;: 109944832,
    &amp;quot;total_dirty&amp;quot;: 28672,
    &amp;quot;total_inactive_anon&amp;quot;: 1687552,
    &amp;quot;total_inactive_file&amp;quot;: 29974528,
    &amp;quot;total_mapped_file&amp;quot;: 1675264,
    &amp;quot;total_pgfault&amp;quot;: 295316278,
    &amp;quot;total_pgmajfault&amp;quot;: 77,
    &amp;quot;total_pgpgin&amp;quot;: 85138921,
    &amp;quot;total_pgpgout&amp;quot;: 84964308,
    &amp;quot;total_rss&amp;quot;: 605270016,
    &amp;quot;total_rss_huge&amp;quot;: 0,
    &amp;quot;total_shmem&amp;quot;: 5513216,
    &amp;quot;total_unevictable&amp;quot;: 0,
    &amp;quot;total_writeback&amp;quot;: 0,
    &amp;quot;unevictable&amp;quot;: 0,
    &amp;quot;writeback&amp;quot;: 0
  },
  &amp;quot;limit&amp;quot;: 5368709120
},
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对断言的注释：&lt;br&gt;
总计（memory.usage_in_bytes）=rss+缓存&lt;br&gt;
说：&lt;br&gt;
用法\u字节：为了提高效率，与其他内核组件一样，内存组使用一些优化&lt;br&gt;
避免不必要的缓存线错误共享。使用率受&lt;br&gt;
方法，不显示内存（和交换）使用的“精确”值，这是一个模糊的&lt;br&gt;
有效访问的值。（当然，必要时，它是同步的。）&lt;br&gt;
如果您想知道更精确的内存使用情况，应该使用rss+cache（+swap）&lt;br&gt;
内存中的值。stat（见5.2）。&lt;br&gt;
说：&lt;br&gt;
注意：在Linux上，Docker CLI通过从总内存使用量中减去页面缓存使用量来报告内存使用情况。API不执行这样的计算，而是提供总内存使用量和页面缓存的数量，以便客户机可以根据需要使用数据。&lt;br&gt;
实际上，容器中/sys/fs/cgroup/memory/memory.stat中的大多数内容都出现在上面的docker stats api响应中（与在不同时间采集样本略有不同，抱歉）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;meme@myapp-56b947bf6d-2lcr7:/app# cat /sys/fs/cgroup/memory/memory.stat
cache 119492608
rss 607436800
rss_huge 0
shmem 5525504
mapped_file 1675264
dirty 69632
writeback 0
pgpgin 85573974
pgpgout 85396501
pgfault 296366011
pgmajfault 80
inactive_anon 1687552
active_anon 611213312
inactive_file 32800768
active_file 81166336
unevictable 0
hierarchical_memory_limit 5368709120
total_cache 119492608
total_rss 607436800
total_rss_huge 0
total_shmem 5525504
total_mapped_file 1675264
total_dirty 69632
total_writeback 0
total_pgpgin 85573974
total_pgpgout 85396501
total_pgfault 296366011
total_pgmajfault 80
total_inactive_anon 1687552
total_active_anon 611213312
total_inactive_file 32800768
total_active_file 81166336
total_unevictable 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;内存信息来自：&lt;br&gt;
Limits:&lt;br&gt;
memory:  5Gi&lt;br&gt;
Requests:&lt;br&gt;
memory:   4Gi&lt;/p&gt;
&lt;p&gt;下面是容器内的提示。在这一行程序中，我获取所有进程ID，在它们上运行pmap-x，并从pmap结果中提取kbytes列。总的结果是256兆字节（远小于PS的RSS，我认为部分原因是许多进程没有从PMAP-X返回输出）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ps aux | awk &#39;{print $2}&#39; | grep -v PID | xargs sudo pmap -x | grep total | grep -v grep | awk &#39;{print $3}&#39; | awk &#39;{s+=$1} END {printf &amp;quot;%.0f&amp;quot;, s}&#39;; echo
256820
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;https://github.com/google/cadvisor/issues/638中提到了。它检查kubectl describe pod &lt;pod&gt;和pmap。这里没有照明（同样，它似乎忽略了一些过程）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# python ps_mem.py
Private  +   Shared  =  RAM used    Program

  1.7 MiB +   1.0 MiB =   2.7 MiB   apache2
  2.0 MiB +   1.0 MiB =   3.0 MiB   bash (3)
---------------------------------
                          5.7 MiB
=================================
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最佳答案&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;有一件事我没看到您检查这里是内核内存。这在memory.usage_in_bytes图中也有说明，但在memory.stat中没有出现。您可以通过查看/sys/fs/cgroup/memory/memory.kmem.usage_in_bytes找到它。
有一次，我看到我们的一个.NET核心应用程序也发生了类似的事情，但我不知道到底发生了什么（可能是.NET核心内存泄漏，因为它是我们的应用程序无法控制的非托管内存）。这将取决于您的应用程序使用是否正常，但就cgroups而言，我相信内核内存使用在默认情况下是不受约束的。
&lt;/code&gt;&lt;/pre&gt;
">docker - 内存使用率差异:cgroup memory.usage_in_bytes与docker容器内的RSS</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/golang-bi-ji-go-mod-yu-go-vendor/"" data-c="
          &lt;h1 id=&#34;go-mod-使用&#34;&gt;go mod 使用&lt;/h1&gt;
&lt;p&gt;解决的问题是golang不再依赖gopath的设置，下载下来的包可以直接使用。&lt;br&gt;
go mod init ./&lt;br&gt;
go build main.go 或 go build -mod=vendor main.go&lt;br&gt;
go mod vendor #将包打到vendor文件夹下&lt;/p&gt;
&lt;h1 id=&#34;go-vendor&#34;&gt;go vendor&lt;/h1&gt;
&lt;p&gt;管理Golang项目依赖，应该是一个第三方的，但是比较好用。&lt;/p&gt;
&lt;p&gt;安装&lt;br&gt;
go get -u github.com/kardianos/govendor&lt;/p&gt;
&lt;p&gt;使用一套连招：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;govendor init # 创建vendor目录，创建vendor.json文件  
govendor add +external #生成依赖包  
govendor update +vendor # 更新vendor的包命令  
状态	缩写状态	含义
+local	l	本地包，即项目自身的包组织
+external	e	外部包，即被 $GOPATH 管理，但不在 vendor 目录下
+vendor	v	已被 govendor 管理，即在 vendor 目录下
+std	s	标准库中的包
+unused	u	未使用的包，即包在 vendor 目录下，但项目并没有用到
+missing	m	代码引用了依赖包，但该包并没有找到
+program	p	主程序包，意味着可以编译为执行文件
+outside	 	外部包和缺失的包
+all	 	所有的包
命令	功能
init	初始化 vendor 目录
list	列出所有的依赖包
add	添加包到 vendor 目录，如 govendor add +external 添加所有外部包
add PKG_PATH	添加指定的依赖包到 vendor 目录
update	从 $GOPATH 更新依赖包到 vendor 目录
remove	从 vendor 管理中删除依赖
status	列出所有缺失、过期和修改过的包
fetch	添加或更新包到本地 vendor 目录
sync	本地存在 vendor.json 时候拉去依赖包，匹配所记录的版本
get	类似 go get 目录，拉取依赖包到 vendor 目录
&lt;/code&gt;&lt;/pre&gt;
">golang笔记-go mod与go vendor</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/golang-bi-ji-zip-ri-zhi-shi-yong/"" data-c="
          &lt;p&gt;在beego中使用zap管理日志，很方便👍&lt;br&gt;
需要在conf/app.conf定义几个参数&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;appname = web-terminal
httpport = &amp;quot;9600&amp;quot;

runmode = &amp;quot;prod&amp;quot;
LogLevel = &amp;quot;info&amp;quot;
autorender = true
recoverpanic = false
copyrequestbody = true
viewspath = &amp;quot;static&amp;quot;
LogPath = &amp;quot;logs/k8s-websocket.log&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;logger.go&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;package controllers

import (
	&amp;quot;os&amp;quot;

	&amp;quot;github.com/astaxie/beego&amp;quot;
	&amp;quot;github.com/natefinch/lumberjack&amp;quot;
	&amp;quot;go.uber.org/zap&amp;quot;
	&amp;quot;go.uber.org/zap/zapcore&amp;quot;
)

//MyLogger初始化zaplogger日志库
var MyLogger *zap.Logger

func initLogger(logpath string, loglevel string) *zap.Logger {

	// 设置日志级别
	var level zapcore.Level
	switch loglevel {
	case &amp;quot;debug&amp;quot;:
		level = zap.DebugLevel
	case &amp;quot;info&amp;quot;:
		level = zap.InfoLevel
	case &amp;quot;error&amp;quot;:
		level = zap.ErrorLevel
	default:
		level = zap.InfoLevel
	}

	hook := lumberjack.Logger{
		Filename:   logpath, // 日志文件路径
		MaxSize:    20,      // 每个日志文件保存的最大尺寸 单位：M
		MaxBackups: 10,      // 日志文件最多保存多少个备份
		MaxAge:     7,       // 文件最多保存多少天
		Compress:   true,    // 是否压缩
	}

	encoderConfig := zapcore.EncoderConfig{
		TimeKey:        &amp;quot;time&amp;quot;,
		LevelKey:       &amp;quot;level&amp;quot;,
		NameKey:        &amp;quot;logger&amp;quot;,
		CallerKey:      &amp;quot;linenum&amp;quot;,
		MessageKey:     &amp;quot;msg&amp;quot;,
		StacktraceKey:  &amp;quot;stacktrace&amp;quot;,
		LineEnding:     zapcore.DefaultLineEnding,
		EncodeLevel:    zapcore.LowercaseLevelEncoder,  // 小写编码器
		EncodeTime:     zapcore.ISO8601TimeEncoder,     // ISO8601 UTC 时间格式
		EncodeDuration: zapcore.SecondsDurationEncoder, //
		EncodeCaller:   zapcore.ShortCallerEncoder,     // 短路径编码器
		EncodeName:     zapcore.FullNameEncoder,
	}

	core := zapcore.NewCore(
		zapcore.NewJSONEncoder(encoderConfig),                                           // 编码器配置
		zapcore.NewMultiWriteSyncer(zapcore.AddSync(os.Stdout), zapcore.AddSync(&amp;amp;hook)), // 打印到控制台和文件
		level, // 日志级别
	)

	// 开启开发模式，堆栈跟踪
	caller := zap.AddCaller()
	// 开启文件及行号
	development := zap.Development()

	// 设置初始化字段
	filed := zap.Fields(zap.String(&amp;quot;serviceName&amp;quot;, &amp;quot;k8s-websocket-dashboard&amp;quot;))
	// 构造日志
	logger := zap.New(core, caller, development, filed)

	logger.Info(&amp;quot;initlog 初始化成功&amp;quot;)
	return logger
}

// 初始化日志
func init() {
	logPath := beego.AppConfig.String(&amp;quot;LogPath&amp;quot;)
	loglevel := beego.AppConfig.String(&amp;quot;LogLevel&amp;quot;)
	MyLogger = initLogger(logPath, loglevel)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;日志输出：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{&amp;quot;level&amp;quot;:&amp;quot;info&amp;quot;,&amp;quot;time&amp;quot;:&amp;quot;2020-06-02T19:41:37.799+0800&amp;quot;,&amp;quot;linenum&amp;quot;:&amp;quot;controllers/logger.go:59&amp;quot;,&amp;quot;msg&amp;quot;:&amp;quot;log 初始化成功&amp;quot;,&amp;quot;serviceName&amp;quot;:&amp;quot;k8s-websocket-dashboard&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
">golang笔记-zip日志使用</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/golang-bi-ji-beego-huo-qu-url-qing-qiu-de-can-shu/"" data-c="
          &lt;h1 id=&#34;获取参数&#34;&gt;获取参数&lt;/h1&gt;
&lt;p&gt;我们经常需要获取用户传递的数据，包括Get、POST等方式的请求，beego里面会自动解析这些数据，你可以通过如下方式获取数据：&lt;/p&gt;
&lt;p&gt;GetString(key string) string&lt;br&gt;
GetStrings(key string) []string&lt;br&gt;
GetInt(key string) (int64, error)&lt;br&gt;
GetBool(key string) (bool, error)&lt;br&gt;
GetFloat(key string) (float64, error)&lt;br&gt;
示例1：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func (this *MainController) Post() {
    jsoninfo := &amp;lt;strong&amp;gt;this.GetString&amp;lt;/strong&amp;gt;(&amp;quot;jsoninfo&amp;quot;)
    if jsoninfo == &amp;quot;&amp;quot; {
        this.Ctx.WriteString(&amp;quot;jsoninfo is empty&amp;quot;)
        return
    }
}```
如果你需要的数据可能是其它类型，例如是int类型而不是int64，那么你需要这样处理：

示例2：
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;func (this *MainController) Post() {&lt;br&gt;
id := &lt;strong&gt;this.Input().Get&lt;/strong&gt;(&amp;quot;id&amp;quot;)&lt;br&gt;
intid, err := &lt;strong&gt;strconv.Atoi&lt;/strong&gt;(id)&lt;br&gt;
}```&lt;br&gt;
更多其他的request的信息，用户可以通过this.Ctx.Request获取信息。&lt;/p&gt;
&lt;p&gt;关于该对象的属性和方法可参考request官方手册https://gowalker.org/net/http#Request&lt;/p&gt;
&lt;h1 id=&#34;直接解析到struct&#34;&gt;直接解析到struct&lt;/h1&gt;
&lt;p&gt;如果要把表单里的内容赋值到一个struct里，除了用上面的方法一个一个获取再赋值之外，&lt;br&gt;
beego提供了通过另外一个更便捷的方式，就是通过struct的字段名或tag与表单字段对应直接解析到struct。&lt;/p&gt;
&lt;p&gt;示例3：&lt;/p&gt;
&lt;p&gt;定义struct&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type user struct {
    Id    int         `form:&amp;quot;-&amp;quot;`
    Name  interface{} `form:&amp;quot;&amp;lt;strong&amp;gt;username&amp;lt;/strong&amp;gt;&amp;quot;`
    Age   int         `form:&amp;quot;&amp;lt;strong&amp;gt;age&amp;lt;/strong&amp;gt;&amp;quot;`
    Email string
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;表单&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;form id=&amp;quot;&amp;lt;strong&amp;gt;user&amp;lt;/strong&amp;gt;&amp;quot;&amp;gt;
    名字：&amp;lt;input name=&amp;quot;&amp;lt;strong&amp;gt;username&amp;lt;/strong&amp;gt;&amp;quot; type=&amp;quot;text&amp;quot; /&amp;gt;
    年龄：&amp;lt;input name=&amp;quot;&amp;lt;strong&amp;gt;age&amp;lt;/strong&amp;gt;&amp;quot; type=&amp;quot;text&amp;quot; /&amp;gt;
    邮箱：&amp;lt;input name=&amp;quot;Email&amp;quot; type=&amp;quot;text&amp;quot; /&amp;gt;
    &amp;lt;input type=&amp;quot;submit&amp;quot; value=&amp;quot;提交&amp;quot; /&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;controller里解析&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func (this *MainController) Post() {
    u := &amp;lt;strong&amp;gt;user{}&amp;lt;/strong&amp;gt;
    if err := this.ParseForm(&amp;lt;strong&amp;gt;&amp;amp;u&amp;lt;/strong&amp;gt;); err != nil {
        //handle error
    }
}　　
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;需要说明的是：&lt;/p&gt;
&lt;p&gt;（1）structTag form的定义和renderform方法共用一个标签。&lt;/p&gt;
&lt;p&gt;（2）定义struct时，字段名后如果有form这个tag，则会把form表单里的name和tag的名字一样的字段赋值给这个字段，&lt;/p&gt;
&lt;p&gt;否则就会把form表单里与字段名一样的表单内容赋值给这个字段。&lt;/p&gt;
&lt;p&gt;例如上面的例子中，会把表单中的username和age分别赋值给user里的Name和Age字段，而Email里的内容则会赋值给Email这个字段。&lt;/p&gt;
&lt;p&gt;（3）调用Controller PraseForm这个方法的时候，传入的参数必须为一个struct的指针，否则对struct的赋值不会成功并返回xx must be a struct pointer的错误。&lt;/p&gt;
&lt;p&gt;（4）如果要忽略一个字段，有两种方法，一是：字段名小写开头，二是：form标签设置为_&lt;/p&gt;
&lt;h1 id=&#34;获取request-body里的内容&#34;&gt;获取Request Body里的内容&lt;/h1&gt;
&lt;p&gt;在API的开发中，我们经常会用到JSON或XML来作为数据交互的格式，如何在beego中获取Request Body里的JSON或XML的数据呢？&lt;/p&gt;
&lt;p&gt;首先，在配置文件里设置copyrequestbody = true&lt;/p&gt;
&lt;p&gt;其次，在Controller中：&lt;/p&gt;
&lt;p&gt;示例4：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func (this *ObjectController) Post() {
    var ob models.Object
    var err error
    if err = json.Unmarshal(this.Ctx.Input.RequestBody, &amp;amp;ob); err == nil {
        objectid := models.AddOne(ob)
        this.Data[&amp;quot;json&amp;quot;] = &amp;quot;{\&amp;quot;ObjectId\&amp;quot;:\&amp;quot;&amp;quot; + objectid + &amp;quot;\&amp;quot;}&amp;quot;
    } else {
        this.Data[&amp;quot;json&amp;quot;] = err.Error()
    }
    this.ServeJSON()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h1 id=&#34;文件上传&#34;&gt;文件上传&lt;/h1&gt;
&lt;p&gt;在beego中你可以很容易的处理文件上传，就是别忘记在你的form表单中增加“enctype=&amp;quot;multipart/form-data”，否则你的浏览器不会传输你的上传文件。&lt;/p&gt;
&lt;p&gt;文件上传之后一般是放在系统的内存里面，如果文件的size大于设置的缓存大小，那么就放在临时文件中，&lt;/p&gt;
&lt;p&gt;默认的缓存内存是64M，你可以通过如下方式来调整这个缓存内存的大小。&lt;/p&gt;
&lt;p&gt;beego.MaxMemory = 1&amp;lt;&amp;lt;22&lt;br&gt;
或者在配置文件中通过如下设置：&lt;/p&gt;
&lt;p&gt;maxmemory = 1&amp;lt;&amp;lt;22&lt;br&gt;
Beego提供了两个很方便的方法来处理文件上传：&lt;/p&gt;
&lt;p&gt;（1）GetFile(key string) (multipart.File, *multipart.FileHeader, error)&lt;/p&gt;
&lt;p&gt;该方法主要用于用户读取表单中的文件名the_file，然后返回相应的信息，用户根据这些变量来处理文件上传：过滤、保存文件等。&lt;/p&gt;
&lt;p&gt;（2）SaveToFile(fromfile, tofile string) error&lt;/p&gt;
&lt;p&gt;该方法是在GetFile的基础上实现了快速保存的功能，fromfile是提交的时候html表单中的name。&lt;/p&gt;
&lt;p&gt;示例5&lt;/p&gt;
&lt;p&gt;表单：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;form enctype=&amp;quot;multipart/form-data&amp;quot; method=&amp;quot;post&amp;quot;&amp;gt;
    &amp;lt;input type=&amp;quot;file&amp;quot; name=&amp;quot;uploadname&amp;quot; /&amp;gt;
    &amp;lt;input type=&amp;quot;submit&amp;quot;&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Controller中代码：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func (c *FormController) Post() {
    f, h, err := c.GetFile(&amp;quot;uploadname&amp;quot;)
    if err != nil {
        log.Fatal(&amp;quot;getfile err &amp;quot;, err)
    }
    defer f.Close()
    c.SaveToFile(&amp;quot;uploadname&amp;quot;, &amp;quot;static/upload/&amp;quot; + h.Filename) // 保存位置在 static/upload, 没有文件夹要先创建
     
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;数据绑定&#34;&gt;数据绑定&lt;/h1&gt;
&lt;p&gt;支持从用户请求中直接数据bind到指定的对象。&lt;/p&gt;
&lt;p&gt;示例6：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var id int
this.Ctx.Input.Bind(&amp;amp;id, &amp;quot;id&amp;quot;)  //id ==123
 
var isok bool
this.Ctx.Input.Bind(&amp;amp;isok, &amp;quot;isok&amp;quot;)  //isok ==true
 
var ft float64
this.Ctx.Input.Bind(&amp;amp;ft, &amp;quot;ft&amp;quot;)  //ft ==1.2
 
ol := make([]int, 0, 2)
this.Ctx.Input.Bind(&amp;amp;ol, &amp;quot;ol&amp;quot;)  //ol ==[1 2]
 
ul := make([]string, 0, 2)
this.Ctx.Input.Bind(&amp;amp;ul, &amp;quot;ul&amp;quot;)  //ul ==[str array]
 
user struct{Name}
this.Ctx.Input.Bind(&amp;amp;user, &amp;quot;user&amp;quot;)  //user =={Name:&amp;quot;astaxie&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
">golang笔记-beego获取url请求的参数</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/golang-bi-ji-json-shu-ju-jie-xi-marshal-yu-unmarshal/"" data-c="
          &lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;
&lt;p&gt;Json(Javascript Object Nanotation)是一种数据交换格式，常用于前后端数据传输。任意一端将数据转换成json 字符串，另一端再将该字符串解析成相应的数据结构，如string类型，strcut对象等。&lt;/p&gt;
&lt;p&gt;go语言本身为我们提供了json的工具包”encoding/json”。&lt;br&gt;
更多的使用方式，可以参考：https://studygolang.com/articles/6742&lt;/p&gt;
&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;
&lt;p&gt;Json Marshal：将数据编码成json字符串&lt;br&gt;
看一个简单的例子&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type Stu struct {
    Name  string `json:&amp;quot;name&amp;quot;`
    Age   int
    HIgh  bool
    sex   string
    Class *Class `json:&amp;quot;class&amp;quot;`
}

type Class struct {
    Name  string
    Grade int
}

func main() {
    //实例化一个数据结构，用于生成json字符串
    stu := Stu{
        Name: &amp;quot;张三&amp;quot;,
        Age:  18,
        HIgh: true,
        sex:  &amp;quot;男&amp;quot;,
    }

    //指针变量
    cla := new(Class)
    cla.Name = &amp;quot;1班&amp;quot;
    cla.Grade = 3
    stu.Class=cla

    //Marshal失败时err!=nil
    jsonStu, err := json.Marshal(stu)
    if err != nil {
        fmt.Println(&amp;quot;生成json字符串错误&amp;quot;)
    }

    //jsonStu是[]byte类型，转化成string类型便于查看
    fmt.Println(string(jsonStu))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{&amp;quot;name&amp;quot;:&amp;quot;张三&amp;quot;,&amp;quot;Age&amp;quot;:18,&amp;quot;HIgh&amp;quot;:true,&amp;quot;class&amp;quot;:{&amp;quot;Name&amp;quot;:&amp;quot;1班&amp;quot;,&amp;quot;Grade&amp;quot;:3}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从结果中可以看出&lt;/p&gt;
&lt;p&gt;只要是可导出成员（变量首字母大写），都可以转成json。因成员变量sex是不可导出的，故无法转成json。&lt;/p&gt;
&lt;p&gt;如果变量打上了json标签，如Name旁边的 &lt;code&gt;json:&amp;quot;name&amp;quot;&lt;/code&gt; ，那么转化成的json key就用该标签“name”，否则取变量名作为key，如“Age”，“HIgh”。&lt;/p&gt;
&lt;p&gt;bool类型也是可以直接转换为json的value值。Channel， complex 以及函数不能被编码json字符串。当然，循环的数据结构也不行，它会导致marshal陷入死循环。&lt;/p&gt;
&lt;p&gt;指针变量，编码时自动转换为它所指向的值，如cla变量。&lt;br&gt;
（当然，不传指针，Stu struct的成员Class如果换成Class struct类型，效果也是一模一样的。只不过指针更快，且能节省内存空间。）&lt;/p&gt;
&lt;p&gt;最后，强调一句：json编码成字符串后就是纯粹的字符串了。&lt;/p&gt;
&lt;p&gt;上面的成员变量都是已知的类型，只能接收指定的类型，比如string类型的Name只能赋值string类型的数据。&lt;br&gt;
但有时为了通用性，或使代码简洁，我们希望有一种类型可以接受各种类型的数据，并进行json编码。这就用到了interface{}类型。&lt;/p&gt;
&lt;p&gt;前言：&lt;br&gt;
interface{}类型其实是个空接口，即没有方法的接口。go的每一种类型都实现了该接口。因此，任何其他类型的数据都可以赋值给interface{}类型。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type Stu struct {
    Name  interface{} `json:&amp;quot;name&amp;quot;`
    Age   interface{}
    HIgh  interface{}
    sex   interface{}
    Class interface{} `json:&amp;quot;class&amp;quot;`
}

type Class struct {
    Name  string
    Grade int
}

func main() {
    //与前面的例子一样
    ......
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{&amp;quot;name&amp;quot;:&amp;quot;张三&amp;quot;,&amp;quot;Age&amp;quot;:18,&amp;quot;HIgh&amp;quot;:true,&amp;quot;class&amp;quot;:{&amp;quot;Name&amp;quot;:&amp;quot;1班&amp;quot;,&amp;quot;Grade&amp;quot;:3}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从结果中可以看出，无论是string，int，bool，还是指针类型等，都可赋值给interface{}类型，且正常编码，效果与前面的例子一样。&lt;/p&gt;
&lt;p&gt;补充：&lt;br&gt;
在实际项目中，编码成json串的数据结构，往往是切片类型。如下定义了一个[]StuRead类型的切片&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//正确示范

//方式1：只声明，不分配内存
var stus1 []*StuRead

//方式2：分配初始值为0的内存
stus2 := make([]*StuRead,0)

//错误示范
//new()只能实例化一个struct对象，而[]StuRead是切片，不是对象
stus := new([]StuRead)

stu1 := StuRead{成员赋值...}
stu2 := StuRead{成员赋值...}

//由方式1和2创建的切片，都能成功追加数据
//方式2最好分配0长度，append时会自动增长。反之指定初始长度，长度不够时不会自动增长，导致数据丢失
stus1 := appen(stus1,stu1,stu2)
stus2 := appen(stus2,stu1,stu2)

//成功编码
json1,_ := json.Marshal(stus1)
json2,_ := json.Marshal(stus2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解码时定义对应的切片接受即可&lt;/p&gt;
&lt;p&gt;Json Unmarshal：将json字符串解码到相应的数据结构&lt;br&gt;
我们将上面的例子进行解码&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type StuRead struct {
    Name  interface{} `json:&amp;quot;name&amp;quot;`
    Age   interface{}
    HIgh  interface{}
    sex   interface{}
    Class interface{} `json:&amp;quot;class&amp;quot;`
    Test  interface{}
}

type Class struct {
    Name  string
    Grade int
}

func main() {
    //json字符中的&amp;quot;引号，需用\进行转义，否则编译出错
    //json字符串沿用上面的结果，但对key进行了大小的修改，并添加了sex数据
    data:=&amp;quot;{\&amp;quot;name\&amp;quot;:\&amp;quot;张三\&amp;quot;,\&amp;quot;Age\&amp;quot;:18,\&amp;quot;high\&amp;quot;:true,\&amp;quot;sex\&amp;quot;:\&amp;quot;男\&amp;quot;,\&amp;quot;CLASS\&amp;quot;:{\&amp;quot;naME\&amp;quot;:\&amp;quot;1班\&amp;quot;,\&amp;quot;GradE\&amp;quot;:3}}&amp;quot;
    str:=[]byte(data)

    //1.Unmarshal的第一个参数是json字符串，第二个参数是接受json解析的数据结构。
    //第二个参数必须是指针，否则无法接收解析的数据，如stu仍为空对象StuRead{}
    //2.可以直接stu:=new(StuRead),此时的stu自身就是指针
    stu:=StuRead{}
    err:=json.Unmarshal(str,&amp;amp;stu)

    //解析失败会报错，如json字符串格式不对，缺&amp;quot;号，缺}等。
    if err!=nil{
        fmt.Println(err)
    }

    fmt.Println(stu)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{张三 18 true &amp;lt;nil&amp;gt; map[naME:1班 GradE:3] &amp;lt;nil&amp;gt;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;总结：&lt;/p&gt;
&lt;p&gt;json字符串解析时，需要一个“接收体”接受解析后的数据，且Unmarshal时接收体必须传递指针。否则解析虽不报错，但数据无法赋值到接受体中。如这里用的是StuRead{}接收。&lt;/p&gt;
&lt;p&gt;解析时，接收体可自行定义。json串中的key自动在接收体中寻找匹配的项进行赋值。匹配规则是：&lt;/p&gt;
&lt;p&gt;先查找与key一样的json标签，找到则赋值给该标签对应的变量(如Name)。&lt;br&gt;
没有json标签的，就从上往下依次查找变量名与key一样的变量，如Age。或者变量名忽略大小写后与key一样的变量。如HIgh，Class。第一个匹配的就赋值，后面就算有匹配的也忽略。&lt;br&gt;
(前提是该变量必需是可导出的，即首字母大写)。&lt;br&gt;
不可导出的变量无法被解析（如sex变量，虽然json串中有key为sex的k-v，解析后其值仍为nil,即空值）&lt;/p&gt;
&lt;p&gt;当接收体中存在json串中匹配不了的项时，解析会自动忽略该项，该项仍保留原值。如变量Test，保留空值nil。&lt;/p&gt;
&lt;p&gt;你一定会发现，变量Class貌似没有解析为我们期待样子。因为此时的Class是个interface{}类型的变量，而json串中key为CLASS的value是个复合结构，不是可以直接解析的简单类型数据（如“张三”，18，true等）。所以解析时，由于没有指定变量Class的具体类型，json自动将value为复合结构的数据解析为map[string]interface{}类型的项。也就是说，此时的struct Class对象与StuRead中的Class变量没有半毛钱关系，故与这次的json解析没有半毛钱关系。&lt;/p&gt;
&lt;p&gt;让我们看一下这几个interface{}变量解析后的类型&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func main() {
    //与前边json解析的代码一致
    ...
    fmt.Println(stu) //打印json解析前变量类型
    err:=json.Unmarshal(str,&amp;amp;stu)
    fmt.Println(&amp;quot;--------------json 解析后-----------&amp;quot;)
    ... 
    fmt.Println(stu) //打印json解析后变量类型    
}

//利用反射，打印变量类型
func printType(stu *StuRead){
    nameType:=reflect.TypeOf(stu.Name)
    ageType:=reflect.TypeOf(stu.Age)
    highType:=reflect.TypeOf(stu.HIgh)
    sexType:=reflect.TypeOf(stu.sex)
    classType:=reflect.TypeOf(stu.Class)
    testType:=reflect.TypeOf(stu.Test)

    fmt.Println(&amp;quot;nameType:&amp;quot;,nameType)
    fmt.Println(&amp;quot;ageType:&amp;quot;,ageType)
    fmt.Println(&amp;quot;highType:&amp;quot;,highType)
    fmt.Println(&amp;quot;sexType:&amp;quot;,sexType)
    fmt.Println(&amp;quot;classType:&amp;quot;,classType)
    fmt.Println(&amp;quot;testType:&amp;quot;,testType)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nameType: &amp;lt;nil&amp;gt;
ageType: &amp;lt;nil&amp;gt;
highType: &amp;lt;nil&amp;gt;
sexType: &amp;lt;nil&amp;gt;
classType: &amp;lt;nil&amp;gt;
testType: &amp;lt;nil&amp;gt;
--------------json 解析后-----------
nameType: string
ageType: float64
highType: bool
sexType: &amp;lt;nil&amp;gt;
classType: map[string]interface {}
testType: &amp;lt;nil&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从结果中可见&lt;/p&gt;
&lt;p&gt;interface{}类型变量在json解析前，打印出的类型都为nil，就是没有具体类型，这是空接口（interface{}类型）的特点。&lt;/p&gt;
&lt;p&gt;json解析后，json串中value，只要是”简单数据”，都会按照默认的类型赋值，如”张三”被赋值成string类型到Name变量中，数字18对应float64，true对应bool类型。&lt;/p&gt;
&lt;p&gt;“简单数据”：是指不能再进行二次json解析的数据，如”name”:”张三”只能进行一次json解析。&lt;br&gt;
“复合数据”：类似”CLASS\”:{\”naME\”:\”1班\”,\”GradE\”:3}这样的数据，是可进行二次甚至多次json解析的，因为它的value也是个可被解析的独立json。即第一次解析key为CLASS的value，第二次解析value中的key为naME和GradE的value&lt;/p&gt;
&lt;p&gt;对于”复合数据”，如果接收体中配的项被声明为interface{}类型，go都会默认解析成map[string]interface{}类型。如果我们想直接解析到struct Class对象中，可以将接受体对应的项定义为该struct类型。如下所示：&lt;/p&gt;
&lt;p&gt;type StuRead struct {&lt;br&gt;
...&lt;br&gt;
//普通struct类型&lt;br&gt;
Class Class &lt;code&gt;json:&amp;quot;class&amp;quot;&lt;/code&gt;&lt;br&gt;
//指针类型&lt;br&gt;
Class *Class &lt;code&gt;json:&amp;quot;class&amp;quot;&lt;/code&gt;&lt;br&gt;
}&lt;/p&gt;
&lt;p&gt;stu打印结果&lt;/p&gt;
&lt;p&gt;Class类型：{张三 18 true &lt;nil&gt; {1班 3} &lt;nil&gt;}&lt;br&gt;
*Class类型：{张三 18 true &lt;nil&gt; 0xc42008a0c0 &lt;nil&gt;}&lt;/p&gt;
&lt;p&gt;可以看出，传递Class类型的指针时，stu中的Class变量存的是指针，我们可通过该指针直接访问所属的数据，如stu.Class.Name/stu.Class.Grade&lt;/p&gt;
&lt;p&gt;Class变量解析后类型&lt;/p&gt;
&lt;p&gt;classType: main.Class&lt;br&gt;
classType: *main.Class&lt;br&gt;
解析时，如果接受体中同时存在2个匹配的项，会发生什么呢？&lt;br&gt;
测试1&lt;/p&gt;
&lt;p&gt;type StuRead struct {&lt;br&gt;
NAme interface{}&lt;br&gt;
Name  interface{}&lt;br&gt;
NAMe interface{}    &lt;code&gt;json:&amp;quot;name&amp;quot;&lt;/code&gt;&lt;br&gt;
}&lt;br&gt;
结果1:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//当存在匹配的json标签时，其对应的项被赋值。
//切记：匹配的标签可以没有，但有时最好只有一个哦
{&amp;lt;nil&amp;gt; &amp;lt;nil&amp;gt; 张三}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;测试2&lt;/p&gt;
&lt;p&gt;type StuRead struct {&lt;br&gt;
NAme interface{}&lt;br&gt;
Name  interface{}&lt;br&gt;
NAMe interface{}    &lt;code&gt;json:&amp;quot;name&amp;quot;&lt;/code&gt;&lt;br&gt;
NamE interface{}    &lt;code&gt;json:&amp;quot;name&amp;quot;&lt;/code&gt;&lt;br&gt;
}&lt;br&gt;
结果2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//当匹配的json标签有多个时，标签对应的项都不会被赋值。
//忽略标签项，从上往下寻找第一个没有标签且匹配的项赋值
{张三 &amp;lt;nil&amp;gt; &amp;lt;nil&amp;gt; &amp;lt;nil&amp;gt;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;测试3&lt;/p&gt;
&lt;p&gt;type StuRead struct {&lt;br&gt;
NAme interface{}&lt;br&gt;
Name  interface{}&lt;br&gt;
}&lt;br&gt;
结果3&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//没有json标签时，从上往下，第一个匹配的项会被赋值哦
{张三 &amp;lt;nil&amp;gt;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;测试4&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type StuRead struct {
    NAMe interface{}    `json:&amp;quot;name&amp;quot;`
    NamE interface{}    `json:&amp;quot;name&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果4&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//当相同的json标签有多个，且没有不带标签的匹配项时，报错了哦
# command-line-arguments
src/test/b.go:48: stu.Name undefined (type *StuRead has no field or method Name, but does have NAMe)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可见，与前边说过的匹配规则是一致的。&lt;/p&gt;
&lt;p&gt;如果不想指定Class变量为具体的类型，仍想保留interface{}类型，但又希望该变量可以解析到struct Class对象中，这时候该怎么办呢？&lt;/p&gt;
&lt;p&gt;这种需求是很可能存在的，例如笔者我就碰到了&lt;/p&gt;
&lt;p&gt;办法还是有的，我们可以将该变量定义为json.RawMessage类型&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type StuRead struct {
    Name  interface{}
    Age   interface{}
    HIgh  interface{}
    Class json.RawMessage `json:&amp;quot;class&amp;quot;` //注意这里
}

type Class struct {
    Name  string
    Grade int
}

func main() {
    data:=&amp;quot;{\&amp;quot;name\&amp;quot;:\&amp;quot;张三\&amp;quot;,\&amp;quot;Age\&amp;quot;:18,\&amp;quot;high\&amp;quot;:true,\&amp;quot;sex\&amp;quot;:\&amp;quot;男\&amp;quot;,\&amp;quot;CLASS\&amp;quot;:{\&amp;quot;naME\&amp;quot;:\&amp;quot;1班\&amp;quot;,\&amp;quot;GradE\&amp;quot;:3}}&amp;quot;
    str:=[]byte(data)
    stu:=StuRead{}
    _:=json.Unmarshal(str,&amp;amp;stu)

    //注意这里：二次解析！
    cla:=new(Class)
    json.Unmarshal(stu.Class,cla)

    fmt.Println(&amp;quot;stu:&amp;quot;,stu)
    fmt.Println(&amp;quot;string(stu.Class):&amp;quot;,string(stu.Class))
    fmt.Println(&amp;quot;class:&amp;quot;,cla)
    printType(&amp;amp;stu) //函数实现前面例子有
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;stu: {张三 18 true [123 34 110 97 77 69 34 58 34 49 231 143 173 34 44 34 71 114 97 100 69 34 58 51 125]}
string(stu.Class): {&amp;quot;naME&amp;quot;:&amp;quot;1班&amp;quot;,&amp;quot;GradE&amp;quot;:3}
class: &amp;amp;{1班 3}
nameType: string
ageType: float64
highType: bool
classType: json.RawMessage
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从结果中可见&lt;/p&gt;
&lt;p&gt;接收体中，被声明为json.RawMessage类型的变量在json解析时，变量值仍保留json的原值，即未被自动解析为map[string]interface{}类型。如变量Class解析后的值为：{“naME”:”1班”,”GradE”:3}&lt;/p&gt;
&lt;p&gt;从打印的类型也可以看出，在第一次json解析时，变量Class的类型是json.RawMessage。此时，我们可以对该变量进行二次json解析，因为其值仍是个独立且可解析的完整json串。我们只需再定义一个新的接受体即可，如json.Unmarshal(stu.Class,cla)&lt;/p&gt;
">golang笔记-json数据解析：Marshal与Unmarshal</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/golang-bi-ji-shu-zu-qie-pian-map/"" data-c="
          &lt;h1 id=&#34;数组&#34;&gt;数组&lt;/h1&gt;
&lt;p&gt;数组的长度一旦定义了就不能动态增长，并且存储的数据类型必须相同。&lt;/p&gt;
&lt;h2 id=&#34;创建方法&#34;&gt;创建方法：&lt;/h2&gt;
&lt;p&gt;var 数组名 [长度]数据类型&lt;br&gt;
例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;package main
import &amp;quot;fmt&amp;quot;
 
func main(){
    var test [5]int //定义数组名字test，长度为5，数据类型为int的数组
    test[0] = 1    //赋值
    test[1] = 2   
    test[2] = 3
    test[3] = 4
    fmt.Println(test) 
    fmt.Println(test[2])
    fmt.Println(test[1:3]) //输出1到3的数组
    fmt.Println(test[0:]) //0到结尾
    fmt.Println(test[:3])  //0到3
 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##结果##
[1 2 3 4 0]
3
[2 3]
[1 2 3 4 0]
[1 2 3]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;数组的四种初始化方式&#34;&gt;数组的四种初始化方式&lt;/h2&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var s1 [3]int = [3]int{1,2,3}
fmt.Println(&amp;quot;s1&amp;quot;,s1)
var s2 [4]int = [...]int{5,6,7,8} //[...]是固定写法
fmt.Println(&amp;quot;s2&amp;quot;,s2)
var s3 = [2]int{9,10} //第一种的简化
fmt.Println(&amp;quot;s3&amp;quot;,s3)
var s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对
fmt.Println(&amp;quot;s4&amp;quot;,s4)
var s5 = new([5]int)
s5[4] =12
fmt.Println(&amp;quot;s5&amp;quot;,s5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##结果##
s1 [1 2 3]
s2 [5 6 7 8]
s3 [9 10]
s4 [40 41 42 43]
s5 [0 0 0 0 5]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;数组的遍历&#34;&gt;数组的遍历&lt;/h2&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对
fmt.Println(&amp;quot;s4&amp;quot;,s4)
     
for index,value := range s4{
fmt.Println(index,value)
}
 
#结果##
0 40
1 41
2 42
3 43
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;var s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对
for i := 0;i &amp;lt;len(s4);i++{
fmt.Println(i,s4[i])
}
 
#结果##
0 40
1 41
2 42
3 43
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;slice切片&#34;&gt;slice切片&lt;/h1&gt;
&lt;p&gt;1、切片是数组的引用(切片是数组的一部分)&lt;br&gt;
2、切片的使用类似数组，如遍历&lt;br&gt;
3、切片的长度是可变的&lt;/p&gt;
&lt;h2 id=&#34;创建语法&#34;&gt;创建语法&lt;/h2&gt;
&lt;p&gt;var 切片名 []类型&lt;br&gt;
如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var qiepian []int
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;切片示例&#34;&gt;切片示例:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;###例子一&amp;lt;br&amp;gt;var suzhu [4]int = [...]int{5,6,7,8}
slice := suzhu[1:4] //1到4的值，不包含4
fmt.Println(suzhu)
fmt.Println(slice)
fmt.Println(&amp;quot;切片的容量&amp;quot;,cap(slice))
 
##结果
[5 6 7 8]
[6 7 8]
切片的容量 3&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
###例子二、使用make创建切片
var slice []int = make([]int,4,10) //类型，大小(长度),容量（可选），容量必须大于长度
slice[0] = 10
slice[1] = 11
fmt.Println(slice)
 
##结果##
[10 11 0 0]
 
 
###例子三
var slice []int = []int {2,4,6}
fmt.Println(slice)
 
##结果##
2 4 6
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;切片的append追加&#34;&gt;切片的append追加&lt;/h2&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var slice []int = []int {2,4,6}
fmt.Println(slice)
//使用append直接追加切片内容（类似python list的append）
slice = append(slice,8,10)
fmt.Println(slice)
slice = append(slice,slice...) //追加切片，...是固定写法
fmt.Println(slice)
 
###结果###
[2 4 6]
[2 4 6 8 10]
[2 4 6 8 10 2 4 6 8 10]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;切片的copy操作&#34;&gt;切片的copy操作&lt;/h2&gt;
&lt;p&gt;使用copy内置函数&lt;br&gt;
例如:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var slice []int = []int {2,4,6}
fmt.Println(slice)
var slice2 []int = make([]int,5)
fmt.Println(slice2)
copy(slice2,slice) //将slice复制给slice2
fmt.Println(slice)
fmt.Println(slice2)
 
##结果##
[2 4 6]
[0 0 0 0 0]
[2 4 6]
[2 4 6 0 0]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;使用切片改变字符串的内容&#34;&gt;使用切片改变字符串的内容&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;var str string = &amp;quot;hello&amp;quot;
fmt.Println(str)
arr := []byte(str)
arr[1] = &#39;a&#39; //转成字符串
arr1 := []rune(str) //中文转换
arr1[0] = &#39;狗&#39;
fmt.Println(arr)
str = string(arr)
fmt.Println(str)
str = string(arr1)
fmt.Println(str)
 
##结果##
hello
[104 97 108 108 111]
hello
狗hello
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;map&#34;&gt;map&lt;/h1&gt;
&lt;p&gt;map是key-value数据结构(类似python的dict)&lt;br&gt;
map是无序存储的&lt;/p&gt;
&lt;p&gt;创建map语法&lt;br&gt;
var map 变量名 map[keytype]valuetype&lt;/p&gt;
&lt;p&gt;如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var m1 map[string]string
var m2 map[string]int
var m3 map[int]string
var m4 map[string]map[string]string
```　　

使用例子：
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;package main&lt;br&gt;
import &amp;quot;fmt&amp;quot;&lt;/p&gt;
&lt;p&gt;func main(){&lt;br&gt;
var m1 map[string]string&lt;br&gt;
//在使用map前,需要先make，make的作用技术给map分配数据空间&lt;br&gt;
m1 = make(map[string]string)&lt;br&gt;
m2 := map[string]string{  //使用方式二&lt;br&gt;
&amp;quot;a1&amp;quot; : &amp;quot;q1&amp;quot;,&lt;br&gt;
&amp;quot;a2&amp;quot; : &amp;quot;a2&amp;quot;,&lt;br&gt;
}&lt;br&gt;
m1[&amp;quot;s1&amp;quot;] = &amp;quot;亚索&amp;quot;&lt;br&gt;
m1[&amp;quot;s2&amp;quot;] = &amp;quot;盖伦&amp;quot;&lt;br&gt;
fmt.Println(m1)&lt;br&gt;
fmt.Println(m1[&amp;quot;s1&amp;quot;])&lt;br&gt;
fmt.Println(m2)&lt;br&gt;
}&lt;/p&gt;
&lt;p&gt;###结果###&lt;br&gt;
map[s1:亚索 s2:盖伦]&lt;br&gt;
亚索&lt;br&gt;
map[a1:q1 a2:a2]&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
 

map的增删改查
增、改
map[key] = value //没有就增加，存在就修改

删
delete(map,key)

查
map[key]   //对应的value，和python的dict一样&lt;/code&gt;&lt;/pre&gt;
">golang笔记-数组、切片、map</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/k8s-hpa-dan-xing-kuo-rong-pei-zhi/"" data-c="
          &lt;pre&gt;&lt;code&gt;HPA全称Horizontal Pod Autoscaling，是K8s实现pod自动水平扩容缩容的特性，这个特性使整个kubernetes集群马上高大上起来了。
要使用HPA也不是这么简单的，HPA api分v1、v2beta1、v2bate2三种，v1只支持通过CPU衡量扩缩容，v2bate1加入针对内存作为度量，v2bate2可以用customer metrics例如网络等，所以v2bate1开始才比较实用。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;要使用HPA必须要开启以下两个特性：&lt;/p&gt;
&lt;p&gt;Aggregation Layer 聚合层，通过与核心的apiserver分离，实现自定义的扩展功能&lt;br&gt;
metrics-server 数据收集，能够收集pod、node等实时运行指标（cpu、内存），给k8s集群使用，例如kubectl top命令、HPA&lt;br&gt;
比较老的版本使用heapster&lt;/p&gt;
&lt;h1 id=&#34;aggregation-layer&#34;&gt;Aggregation Layer&lt;/h1&gt;
&lt;p&gt;要打开Aggregation Layer，需要配置一下apiserver，增加相关认证证书。认证流程是client发起请求到apiserver，apiserver与aggergated apiserver建立tls安全链接，把请求proxy到aggergated apiserver，继续进行–requestheader-*参数的相关认证。&lt;/p&gt;
&lt;p&gt;认证流程&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1589865988979.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;需要生成aggregate使用的证书，参考cfssl生成证书方法，proxy-client-cert-file的CN需要与requestheader-allowed-names匹配。&lt;br&gt;
在apiserver增加如下启动参数&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--requestheader-client-ca-file=/etc/kubernetes/pki/agg-ca.pem
--proxy-client-cert-file=/etc/kubernetes/pki/aggregate.pem
--proxy-client-key-file=/etc/kubernetes/pki/aggregate-key.pem
--requestheader-allowed-names=aggregator
--requestheader-extra-headers-prefix=X-Remote-Extra-
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
#如果kube-proxy没有在Master上面运行，还需要配置
--enable-aggregator-routing=true
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;metrics-server&#34;&gt;metrics server&lt;/h1&gt;
&lt;p&gt;从k8s 1.8开始，集群的资源使用情况都通过metrics api收集，例如容器CPU、内存。这些指标可用于kuberctl top或者k8s的HPA等特性。&lt;br&gt;
metrice server可以在github找到并部署&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/kubernetes-incubator/metrics-server
cd metrics-server
kubectl create -f deploy/1.8+/
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;注1：metrics-server默认使用node的主机名，但是coredns里面没有物理机主机名的解析，一种是部署的时候添加一个参数： –kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP,第二种是使用dnsmasq构建一个上游的dns服务
注2：kubelet 的10250端口使用的是https协议，连接需要验证tls证书。可以在metrics server启动命令添加参数–kubelet-insecure-tls不验证客户端证书
注3：yaml文件中的image地址k8s.gcr.io/metrics-server-amd64:v0.3.3 需要梯子，需要改成中国可以访问的image地址，可以使用aliyun的。这里使用hub.docker.com里的google镜像地址 image: mirrorgooglecontainers/metrics-server-amd64:v0.3.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;成功运行kubectl top命令&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ubuntu@k8s-dev-m1:~/k8sssl/agglayer$ kubectl top nodes
NAME                   CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-dev-node2          103m         5%      2696Mi                 72%       
k8s-dev-node3.bxr.cn   115m      2%     5312Mi                  67%  
k8s-dev-node4          57m          2%       2634Mi                  70%     
k8s-dev-node5          148m         7%       2443Mi                  65%
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;hpa&#34;&gt;HPA&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;有了metrics就可以开始使用HPA特性了。hpa有几个特点
deploy或者rs等需要设置resources才能使用hpa
如果我们创建一个HPA controller，它会每隔15s（可以通过–horizontal-pod-autoscaler-sync-period修改）检测一次hpa定义的资源与实际资源使用情况，如果达到阀值就会调整pod数量。
HPA设置的阀值不是绝对的，允许设置一个浮动范围，–horizontal-pod-autoscaler-tolerance默认是0.1
pod调整算法 desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]
scale有一个窗口期，期间每次变化会记录下来，选择最优的调整建议再进行scale，这样可以保证资源平滑变动，通过–horizontal-pod-autoscaler-downscale-stabilization设定，默认5分钟。
通过hpa调整新增的pod不会马上ready，这时候收集的metrics就不准，为了减少影响，hpa一开始不会收集新pod的metrics。通过–horizontal-pod-autoscaler-initial-readiness-delay（默认30s）和 –horizontal-pod-autoscaler-cpu-initialization-period（默认为 5 分钟）调整
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1589866316410.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
示例hpa.yml:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-test
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization 
        averageUtilization: 160
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面的示例包括cpu和memory指标，averageUtilization这个百分比是根据deployment的resources.requests计算的。例如有deployment限制requests是512Mi，replicas是2，实际pod1用了612Mi，pod2用了598Mi，计算公式是 (612+598)/2/512 = 118%&lt;/p&gt;
&lt;p&gt;查看hpa的情况，targets第一个是memory，第二个是cpu指标，REPLICAS是根据计算后的当前pod数&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ubuntu@k8s-m1:~/k8s/hpa$ kubectl get hpa
NAME       REFERENCE            TARGETS             MINPODS   MAXPODS   REPLICAS   AGE
hpa-test   Deployment/podinfo   120%/160%, 6%/75%   2         4         3          97m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;官方示例还包括packets-per-second、requests-per-second这些指标，需要进一步验证&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: AverageUtilization
        averageUtilization: 50
  - type: Pods
    pods:
      metric:
        name: packets-per-second
      targetAverageValue: 1k
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        name: main-route
      target:
        kind: Value
        value: 10k
status:
  observedGeneration: 1
  lastScaleTime: &amp;lt;some-time&amp;gt;
  currentReplicas: 1
  desiredReplicas: 1
  currentMetrics:
  - type: Resource
    resource:
      name: cpu
    current:
      averageUtilization: 0
      averageValue: 0
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        name: main-route
      current:
        value: 10k

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#create-horizontal-pod-autoscaler&#34;&gt;官方hpa参数&lt;/a&gt;&lt;/p&gt;
">k8s hpa弹性扩容配置</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/beego-jing-tai-wen-jian-jia-zai-lu-jing-xiu-gai/"" data-c="
          &lt;p&gt;Go 语言内部其实已经提供了 http.ServeFile，通过这个函数可以实现静态文件的服务。&lt;/p&gt;
&lt;p&gt;beego 针对这个功能进行了一层封装，通过下面的方式进行静态文件注册：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;beego.SetStaticPath(&amp;quot;/static&amp;quot;,&amp;quot;public&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第一个参数是路径，url 路径信息&lt;br&gt;
第二个参数是静态文件目录（相对应用所在的目录）&lt;br&gt;
beego 支持多个目录的静态文件注册，用户可以注册如下的静态文件目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;beego.SetStaticPath(&amp;quot;/images&amp;quot;,&amp;quot;images&amp;quot;)
beego.SetStaticPath(&amp;quot;/css&amp;quot;,&amp;quot;css&amp;quot;)
beego.SetStaticPath(&amp;quot;/js&amp;quot;,&amp;quot;js&amp;quot;)
```　　

设置了如上的静态目录之后，用户访问 /images/login/login.png，那么就会访问应用对应的目录下面的 images/login/login.png 文件。

如果是访问 /static/img/logo.png，那么就访问 public/img/logo.png文件。

默认情况下 beego 会判断目录下文件是否存在，不存在直接返回 404 页面，如果请求的是 index.html，那么由于 http.ServeFile 默认是会跳转的，不提供该页面的显示。

因此 beego 可以设置 beego.BConfig.WebConfig.DirectoryIndex=true 这样来使得显示 index.html 页面。而且开启该功能之后，用户访问目录就会显示该目录下所有的文件列表。&lt;/code&gt;&lt;/pre&gt;
">beego静态文件加载路径修改</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/yong-hu-kong-jian-gua-zai-ceph-wen-jian-xi-tong/"" data-c="
          &lt;p&gt;Ceph v0.55 及后续版本默认开启了 cephx 认证。从用户空间（ FUSE ）挂载一 Ceph 文件系统前，确保客户端主机有一份 Ceph 配置副本、和具备 Ceph 元数据服务器能力的密钥环。&lt;/p&gt;
&lt;p&gt;1、 在客户端主机上，把监视器主机上的 Ceph 配置文件拷贝到 /etc/ceph/ 目录下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo mkdir -p /etc/ceph
sudo scp {user}@{server-machine}:/etc/ceph/ceph.conf /etc/ceph/ceph.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2、 在客户端主机上，把监视器主机上的 Ceph 密钥环拷贝到 /etc/ceph 目录下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo scp {user}@{server-machine}:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3、 确保客户端机器上的 Ceph 配置文件和密钥环都有合适的权限位，如 chmod 644 。&lt;br&gt;
cephx 如何配置请参考 CEPHX 配置参考。&lt;br&gt;
要把 Ceph 文件系统挂载为用户空间文件系统，可以用 ceph-fuse 命令，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo mkdir /home/usernname/cephfs
sudo ceph-fuse -m 192.168.0.1:6789 /home/username/cephfs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;详情见 &lt;a href=&#34;http://docs.ceph.org.cn/man/8/ceph-fuse/&#34;&gt;ceph-fuse&lt;/a&gt;&lt;/p&gt;
">用户空间挂载 CEPH 文件系统</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/linux-ru-he-dao-ru-zi-ding-yi-zheng-shu/"" data-c="
          &lt;h1 id=&#34;问题&#34;&gt;问题&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;curl https://192.168.0.200:8443
提示curl: (60) Peer&#39;s Certificate issuer is not recognized.
curl: (60) Peer&#39;s Certificate issuer is not recognized.
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a &amp;quot;bundle&amp;quot;
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn&#39;t adequate, you can specify an alternate file
 using the --cacert option.
If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
If you&#39;d like to turn off curl&#39;s verification of the certificate, use
 the -k (or --insecure) option.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;mac-os-x&#34;&gt;Mac OS X&lt;/h1&gt;
&lt;h2 id=&#34;添加证书&#34;&gt;添加证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ~/new-root-certificate.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;移除证书&#34;&gt;移除证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sudo security delete-certificate -c &amp;quot;&amp;lt;name of existing certificate&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;windows&#34;&gt;Windows&lt;/h1&gt;
&lt;h2 id=&#34;添加证书-2&#34;&gt;添加证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;certutil -addstore -f &amp;quot;ROOT&amp;quot; new-root-certificate.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;移除证书-2&#34;&gt;移除证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;certutil -delstore &amp;quot;ROOT&amp;quot; serial-number-hex
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;linux-ubuntu-debian&#34;&gt;Linux (Ubuntu, Debian)&lt;/h1&gt;
&lt;h2 id=&#34;添加证书-3&#34;&gt;添加证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;复制 CA 文件到目录： /usr/local/share/ca-certificates/
执行:
sudo cp foo.crt /usr/local/share/ca-certificates/foo.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;更新-ca-证书库&#34;&gt;更新 CA 证书库:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sudo update-ca-certificates
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;移除证书-3&#34;&gt;移除证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Remove your CA.

Update the CA store:

sudo update-ca-certificates --fresh

Restart Kerio Connect to reload the certificates in the 32-bit versions or Debian 7.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;linux-centos-6&#34;&gt;Linux (CentOs 6)&lt;/h1&gt;
&lt;h2 id=&#34;添加证书-4&#34;&gt;添加证书：&lt;/h2&gt;
&lt;p&gt;// root-ca.crt 为ca证书&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;安装 ca-certificates package:

yum install ca-certificates

启用dynamic CA configuration feature:

update-ca-trust force-enable

Add it as a new file to /etc/pki/ca-trust/source/anchors/:
cp root-ca.crt /etc/pki/ca-trust/source/anchors/

执行:

update-ca-trust extract

Restart Kerio Connect to reload the certificates in the 32-bit version.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;linux-centos-5&#34;&gt;Linux (CentOs 5)&lt;/h1&gt;
&lt;h2 id=&#34;添加证书-5&#34;&gt;添加证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Append your trusted certificate to file /etc/pki/tls/certs/ca-bundle.crt

cat foo.crt &amp;gt;&amp;gt; /etc/pki/tls/certs/ca-bundle.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;测试访问&#34;&gt;测试访问&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt; curl -v &amp;quot;https:/gitlab.test.com/micro-lib/server?go-get=1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
">linux如何导入自定义证书</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/istio-ke-hu-duan-yuan-di-zhi-ru-he-xian-shi/"" data-c="
          &lt;h3 id=&#34;前提&#34;&gt;前提&lt;/h3&gt;
&lt;p&gt;由于istio部署到IDC、非云环境无法获取客户端真实IP地址，web应用无法使用该组件，早期版本的istio做了很多测试，最终无法实现获取remote client address；&lt;/p&gt;
&lt;h3 id=&#34;环境&#34;&gt;环境&lt;/h3&gt;
&lt;p&gt;kubernetes版本：k8s-1.16.9&lt;br&gt;
istio版本：1.5&lt;/p&gt;
&lt;h3 id=&#34;方法&#34;&gt;方法&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kubectl -n istio-system edit  deployments. istio-pilot
添加如下：
       env:
       - name: PILOT_SIDECAR_USE_REMOTE_ADDRESS
          value: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下是github相应的issue&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588232244321.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;其他测试&#34;&gt;其他测试&lt;/h3&gt;
&lt;p&gt;istio-1.5版本回归单体，各个组件优化了很多，后期测试http链接与tcp链接应用&lt;/p&gt;
">istio-客户端源地址如何显示</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/k8s-tong-guo-helm-bu-shu-elk-73/"" data-c="
          &lt;h1 id=&#34;前提&#34;&gt;前提：&lt;/h1&gt;
&lt;p&gt;在kubernetes集群中部署elk组件，es集群部署3个节点，kibana部署一个，容器数据持久化需要storageclass&lt;/p&gt;
&lt;p&gt;依赖：&lt;br&gt;
Helm&lt;br&gt;
Persistent Volumes&lt;/p&gt;
&lt;h1 id=&#34;准备配置&#34;&gt;准备配置&lt;/h1&gt;
&lt;p&gt;由于repo在线安装太慢，建议下载char本地修改参数后安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/elastic/helm-charts.git
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;部署-elk&#34;&gt;部署 ELK&lt;/h1&gt;
&lt;h2 id=&#34;创建elk命名空间&#34;&gt;创建elk命名空间&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#cat elk-ns.yml
apiVersion: v1
kind: Namespace
metadata:
  name: elk
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;部署elasticsearch&#34;&gt;部署elasticsearch&lt;/h2&gt;
&lt;p&gt;cd helm-charts/elasticsearch&lt;/p&gt;
&lt;p&gt;helm install --namespace=elk  --name=elasticsearch .&lt;/p&gt;
&lt;h2 id=&#34;部署-kibana&#34;&gt;部署 Kibana&lt;/h2&gt;
&lt;p&gt;cd helm-charts/kibana&lt;/p&gt;
&lt;p&gt;helm install --namespace=elk --name=kibana .&lt;br&gt;
通过 kubectl get deploy 和 pod 了解部署状态；&lt;/p&gt;
&lt;h1 id=&#34;小知识&#34;&gt;小知识&lt;/h1&gt;
&lt;p&gt;Kibana 直接通过 K8S 内部 DNS 域名 访问 ES。&lt;/p&gt;
&lt;p&gt;查看容器内的配置&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl  exec kibana-kibana-7cbc5db55c-6qct7 -c kibana -- cat /usr/share/kibana/config/kibana.yml

# Default Kibana configuration for docker target
server.name: kibana
server.host: &amp;quot;0&amp;quot;
elasticsearch.hosts: [ &amp;quot;http://elasticsearch:9200&amp;quot; ]
xpack.monitoring.ui.container.elasticsearch.enabled: true
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;kibana-添加-ingress&#34;&gt;Kibana 添加 Ingress&lt;/h1&gt;
&lt;p&gt;通过 Ingress 添加访问入口&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kibana
  namespace: default
spec:
  rules:
  - host: &amp;lt;YourDomain&amp;gt;  ## 访问 Kibana 的域名 
    http:
      paths:
      - backend:
          serviceName: kibana-kibana
          servicePort: 5601
        path: /
 status:
  loadBalancer:
    ingress:
    - ip: &amp;lt;YourLoadBalancerIP&amp;gt;  ## LB 的 IP
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;访问测试&#34;&gt;访问测试&lt;/h1&gt;
&lt;p&gt;访问域名，即可打开 Kibana 7.3 版本；&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588231324831.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;查看集群的运行状态&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588231313286.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;也可以通过命令行查看&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~$ curl  -s &amp;lt;YourESHost&amp;gt;/_cluster/health | jq .
{
  &amp;quot;cluster_name&amp;quot;: &amp;quot;elasticsearch&amp;quot;,
  &amp;quot;status&amp;quot;: &amp;quot;yellow&amp;quot;,
  &amp;quot;timed_out&amp;quot;: false,
  &amp;quot;number_of_nodes&amp;quot;: 3,
  &amp;quot;number_of_data_nodes&amp;quot;: 3,
  &amp;quot;active_primary_shards&amp;quot;: 19,
  &amp;quot;active_shards&amp;quot;: 35,
  &amp;quot;relocating_shards&amp;quot;: 0,
  &amp;quot;initializing_shards&amp;quot;: 0,
  &amp;quot;unassigned_shards&amp;quot;: 3,
  &amp;quot;delayed_unassigned_shards&amp;quot;: 0,
  &amp;quot;number_of_pending_tasks&amp;quot;: 0,
  &amp;quot;number_of_in_flight_fetch&amp;quot;: 0,
  &amp;quot;task_max_waiting_in_queue_millis&amp;quot;: 0,
  &amp;quot;active_shards_percent_as_number&amp;quot;: 92.10526315789474
}
&lt;/code&gt;&lt;/pre&gt;
">K8S通过helm 部署 ELK 7.3</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/kubesphere-an-zhuang-shi-yong-ti-yan/"" data-c="
          &lt;p&gt;最近又出来个kubesphere的工具用来管理k8s，今天特意来安装体验下；&lt;br&gt;
github地址：https://github.com/pixiake/ks-installer&lt;/p&gt;
&lt;p&gt;官方使用文档：https://kubesphere.io/docs/advanced-v2.0/zh-CN/installation/all-in-one/&lt;/p&gt;
&lt;p&gt;先放上安装效果图，UI界面还是很清爽的：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588152934567.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;当前环境&#34;&gt;当前环境：&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;k8s集群已经安装完成，用kubesphere管理现有的k8s集群；

k8s版本为1.14  

系统为centos7.6

kubesphere使用要求：

kubernetes version &amp;gt; 1.13.0

helm version &amp;gt; 2.10.0

a default storage class must be in kubernetes cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装完成后默认用户名密码：&lt;/p&gt;
&lt;p&gt;用户名：admin&lt;/p&gt;
&lt;p&gt;密码：P@88w0rd&lt;/p&gt;
&lt;h3 id=&#34;开始安装&#34;&gt;开始安装&lt;/h3&gt;
&lt;p&gt;安装步骤大概记录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create ns kubesphere-system
kubectl create ns kubesphere-monitoring-system

#访问etcd用到的secret
kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  --from-file=etcd-client-ca.crt=ca.pem  --from-file=etcd-client.crt=etcd-key.pem  --from-file=etcd-client.key=etcd.pem

#管理k8s用到的secret

kubectl -n kubesphere-system create secret generic kubesphere-ca  --from-file=ca.crt=ca.pem --from-file=ca.key=ca-key.pem

#clone好github项目，执行下面的这条命令

cd deploy
kubectl apply -f kubesphere-installer.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;执行完上面的命令，可以通过下面的命令，查看安装过程日志&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l job-name=kubesphere-installer -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -f
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看安装结果，STATUS跟下面保持一致才说明安装成功&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@ks-allinone deploy]# kubectl get pods -n kubesphere-system
NAME                                     READY   STATUS      RESTARTS   AGE
ks-account-6db466d8dc-srrwj              1/1     Running     0          149m
ks-apigateway-7d77cb9495-jzmg6           1/1     Running     0          170m
ks-apiserver-f8469fd47-b58rm             1/1     Running     0          166m
ks-console-54c849bdc9-dfkbf              1/1     Running     0          168m
ks-console-54c849bdc9-z2d5q              1/1     Running     0          168m
ks-controller-manager-569456b4cd-gngm5   1/1     Running     0          170m
ks-docs-6bbdcc9bfb-6jldz                 1/1     Running     0          3h7m
kubesphere-installer-7ph6l               0/1     Completed   1          3h11m
openldap-5c986c5bff-rzqwv                1/1     Running     0          3h25m
redis-76dc4db5dd-lv6kg                   1/1     Running     0          149m
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;安装过程出现的错误&#34;&gt;安装过程出现的错误&lt;/h3&gt;
&lt;p&gt;1.在安装的时提示metrics-server已经安装，导致安装中断；&lt;/p&gt;
&lt;p&gt;解析办法：在kubesphere-installer.yaml的configMap增加配置：metrics_server_enable: False（默认是没有的）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
data:
  ks-config.yaml: |
    kube_apiserver_host: 10.10.5.208:6443
    etcd_tls_enable: True
    etcd_endpoint_ips: 10.10.5.169,10.10.5.183,10.10.5.184
    disableMultiLogin: True
    elk_prefix: logstash
    metrics_server_enable: False
  #  local_registry: 192.168.1.2:5000
kind: ConfigMap
metadata:
  name: kubesphere-config
  namespace: kubesphere-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;增加Ingress配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubesphere
  namespace: kubesphere-system
  annotations:
    #kubernetes.io/ingress.class: traefik
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: ks.staplescn.com
    http:
      paths:
      - path:
        backend:
          serviceName: ks-console
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;访问界面：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588153130667.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
">kubesphere安装使用体验</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian/"" data-c="
          &lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;
&lt;p&gt;我们通常的开发流程是，在本地开发完成应用之后，使用git作为版本管理工具，将本地代码提交到类似Github这样的仓库中做持久化存储，当我们可能来自多个仓库、可能涉及到多个中间件作为底层依赖一起部署到生产环境中时，相信不少在大中型企业工作的小伙伴都知道公司内部通常会有发布系统，那么云原生技术栈中有没有为我们提供类似的发布系统呢？档案是肯定的，而且不乏竞争者，业内知名的有knavtie Build/jekinsX/spinnaker/orgo/tekton，其中，tekon凭借其众多优良特性在一众竞争者中胜出，成为领域内的事实标准，今天我们就来揭开Tekton的神秘面纱。&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588236885419.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;正文&#34;&gt;正文&lt;/h1&gt;
&lt;h3 id=&#34;什么是tekton&#34;&gt;什么是Tekton&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588236902661.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
那Tekton都提供了哪些CRD呢？&lt;br&gt;
•	Task：顾名思义，task表示一个构建任务，task里可以定义一系列的steps，例如编译代码、构建镜像、推送镜像等，每个step实际由一个Pod执行。&lt;br&gt;
•	TaskRun：task只是定义了一个模版，taskRun才真正代表了一次实际的运行，当然你也可以自己手动创建一个taskRun，taskRun创建出来之后，就会自动触发task描述的构建任务。&lt;br&gt;
•	Pipeline：一个或多个task、PipelineResource以及各种定义参数的集合。&lt;br&gt;
•	PipelineRun：类似task和taskRun的关系，pipelineRun也表示某一次实际运行的pipeline，下发一个pipelineRun CRD实例到kubernetes后，同样也会触发一次pipeline的构建。&lt;br&gt;
•	PipelineResource：表示pipeline input资源，比如github上的源码，或者pipeline output资源，例如一个容器镜像或者构建生成的jar包等。&lt;br&gt;
他们大概有如下图所示的关系：&lt;br&gt;
官方介绍：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tekton 是一个功能强大且灵活的 Kubernetes 原生开源框架，用于创建持续集成和交付（CI/CD）系统。通过抽象底层实现细节，用户可以跨多云平台和本地系统进行构建、测试和部署。
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;个人理解：
•	以yaml文件编排应用构建及部署流程
•	knavtive build模块升级版，社区最终采用 Tekton 替代 knavtive Build作为云原生领域的CI/CD 解决方案
•	标准化CI/CD流水线构建、测试及部署流程的工具
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tekton在一众竞争对手的比拼中PK胜出：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588236947460.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;下面就让我们一起来深入详细了解下Tekton到底怎么玩。&lt;br&gt;
Tekton Pipeline中有5类对象，核心理念是通过定义yaml定义构建过程。&lt;br&gt;
•	Task：一个任务的执行模板，用于描述单个任务的构建过程&lt;br&gt;
•	TaskRun：需要通过定义TaskRun任务去运行Task。&lt;br&gt;
•	Pipeline：包含多个Task,并在此基础上定义input和output,input和output以PipelineResource作为交付。&lt;br&gt;
•	PipelineRun：需要定义PipelineRun才会运行Pipeline。&lt;br&gt;
•	PipelineResource：可用于input和output的对象集合。&lt;/p&gt;
&lt;h3 id=&#34;task&#34;&gt;Task&lt;/h3&gt;
&lt;p&gt;Task 就是一个任务执行模板，之所以说 Task 是一个模板是因为 Task 定义中可以包含变量，Task 在真正执行的时候需要给定变量的具体值。如果把 Tekton 的 Task 有点儿类似于定义一个函数，Task 通过 inputs.params 定义需要哪些入参，并且每一个入参还可以指定默认值。Task 的 steps 字段表示当前 Task 是有哪些步骤组成的，每一个步骤具体就是基于镜像启动一个 container 执行一些操作，container 的启动参数可以通过 Task 的入参使用模板语法进行配置。下面是一个Demo：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588236983683.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;taskrun&#34;&gt;TaskRun&lt;/h3&gt;
&lt;p&gt;Task 定义好以后是不能执行的，就像一个函数定义好以后需要调用才能执行一样。所以需要再定义一个 TaskRun 去执行 Task。&lt;br&gt;
TaskRun 主要是负责设置 Task 需要的参数，并通过 taskRef 字段引用要执行的 Task。下面是一个Demo：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237005091.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;但是在实际使用过程中，我们一般很少使用TaskRun，因为它只能给不一个Task 传参，Tekton提供了给多个Task同时传参的解决方案Pipeline和PipelineRun，且看下文详解，这里只是多嘴一下，这个TaskRun很少使用，稍微了解下就可以了。&lt;/p&gt;
&lt;h3 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h3&gt;
&lt;p&gt;一个 TaskRun 只能执行一个 Task，当需要编排多个 Task 的时候就需要 Pipeline 出马了。Pipeline 是一个编排 Task 的模板。Pipeline 的 params 声明了执行时需要的入参。 Pipeline 的 spec.tasks 定义了需要编排的 Task。Tasks 是一个数组，数组中的 task 并不是通过数组声明的顺序去执行的，而是通过 runAfter 来声明 task 执行的顺序。Tekton controller 在解析 CRD 的时候会解析 Task 的顺序，然后根据 runAfter 设置生成的依次树依次去执行。Pipeline 在编排 Task 的时候需要给每一个 Task 传入必须的参数，这些参数的值可以来自 Pipeline 自身的 params 设置。下面是一个Demo：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237033509.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237113343.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pipelinerun&#34;&gt;PipelineRun&lt;/h3&gt;
&lt;p&gt;和 Task 一样 Pipeline 定义完成以后也是不能直接执行的，需要 PipelineRun 才能执行 Pipeline。PipelineRun 的主要作用是给 Pipeline 传入必要的入参，并执行 Pipeline。下面是一个Demo：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237145814.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237151158.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pipelineresource&#34;&gt;PipelineResource&lt;/h3&gt;
&lt;p&gt;可能你还想在 Task 之间共享资源，这就是 PipelineResource 的作用。比如我们可以把 git 仓库信息放在 PipelineResource 中。这样所有 Task 就可以共享这些信息了。&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237189249.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237194640.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237221271.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;实战&#34;&gt;实战&lt;/h1&gt;
&lt;p&gt;关于Tekton的实战，可以参考Github里面的这个完整的Demo，里面是一个go语言吧编写的web服务，接口可以打印&amp;quot;Hello world&amp;quot;。时间有限，就不做演示了，感兴趣的可以在自己的k8s集群上面跑一下感受一下，相关的yaml文件也可以拷贝下来，作为后面改写的模板。&lt;br&gt;
准备 PIpeline 的资源&lt;br&gt;
kubectl apply -f tasks/source-to-image.yaml -f tasks/deploy-using-kubectl.yaml  -f resources/picalc-git.yaml -f image-secret.yaml -f pipeline-account.yaml -f pipeline/build-and-deploy-pipeline.yaml&lt;br&gt;
执行 create 把 pipelieRun 提交到 Kubernetes 集群。之所以这里使用 create 而不是使用 apply 是因为 PIpelineRun 每次都会创建一个新的，kubectl 的 create 指令会基于 generateName 创建新的 PIpelineRun 资源。&lt;br&gt;
kubectl create -f run/picalc-pipeline-run.yaml&lt;/p&gt;
&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;
&lt;p&gt;Tekton以K8S为依托，成为云原生领域CI/CD的事实性标准，帮助我们提高云原生环境下的应用构建和部署效率。&lt;br&gt;
来一张图对全文做一个简单的总结：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588236724398.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/tekton/&#34;&gt;Tekton官网&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/knative-sample/tekton-knative/tree/b1.0?spm=ata.13261165.0.0.21213a182xyMm5&amp;amp;file=b1.0&#34;&gt;Tekton Demo&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
">kubernetes Tekton-CI/CD 持续集成流水线</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/kubernetes搭建rook-ceph/"" data-c="
          &lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;
&lt;p&gt;Rook官网：https://rook.io&lt;br&gt;
Rook是云原生计算基金会(CNCF)的孵化级项目.&lt;br&gt;
Rook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。&lt;br&gt;
至于CEPH，官网在这：https://ceph.com/&lt;br&gt;
ceph官方提供的helm部署，至今我没成功过，所以转向使用rook提供的方案&lt;br&gt;
有道笔记原文：http://note.youdao.com/noteshare?id=281719f1f0374f787effc90067e0d5ad&amp;amp;sub=0B59EA339D4A4769B55F008D72C1A4C0&lt;/p&gt;
&lt;h1 id=&#34;环境&#34;&gt;环境&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;centos 7.5
kernel 4.18.7-1.el7.elrepo.x86_64
docker 18.06
kubernetes v1.12.2
    kubeadm部署：
        网络: canal
        DNS: coredns
    集群成员：    
    192.168.1.1 kube-master
    192.168.1.2 kube-node1
    192.168.1.3 kube-node2
    192.168.1.4 kube-node3
    192.168.1.5 kube-node4

所有node节点准备一块200G的磁盘：/dev/sdb
kubernetes搭建rook-ceph
&lt;/code&gt;&lt;/pre&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233297037.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h1 id=&#34;准备工作&#34;&gt;准备工作&lt;/h1&gt;
&lt;p&gt;所有节点开启ip_forward&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;sysctl -p&lt;/p&gt;
&lt;h1 id=&#34;开始部署operator&#34;&gt;开始部署Operator&lt;/h1&gt;
&lt;h2 id=&#34;部署rook-operator&#34;&gt;部署Rook Operator&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;cd $HOME
git clone https://github.com/rook/rook.git

cd rook
cd cluster/examples/kubernetes/ceph
kubectl apply -f operator.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233371696.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h2 id=&#34;查看operator的状态&#34;&gt;查看Operator的状态&lt;/h2&gt;
&lt;p&gt;执行apply之后稍等一会&lt;br&gt;
operator会在集群内的每个主机创建两个pod:rook-discover,rook-ceph-agent&lt;/p&gt;
&lt;p&gt;kubectl -n rook-ceph-system get pod -o wide&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233523024.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;给节点打标签&#34;&gt;给节点打标签&lt;/h2&gt;
&lt;p&gt;运行ceph-mon的节点打上：ceph-mon=enabled&lt;br&gt;
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-mon=enabled&lt;br&gt;
运行ceph-osd的节点，也就是存储节点，打上：ceph-osd=enabled&lt;br&gt;
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-osd=enabled&lt;br&gt;
运行ceph-mgr的节点，打上：ceph-mgr=enabled&lt;br&gt;
mgr只能支持一个节点运行，这是ceph跑k8s里的局限&lt;br&gt;
kubectl label nodes kube-node1 ceph-mgr=enabled&lt;/p&gt;
&lt;h2 id=&#34;配置clusteryaml文件&#34;&gt;配置cluster.yaml文件&lt;/h2&gt;
&lt;p&gt;官方配置文件详解：https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html&lt;br&gt;
文件中有几个地方要注意：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dataDirHostPath: 这个路径是会在宿主机上生成的，保存的是ceph的相关的配置文件，再重新生成*集群的时候要确保这个目录为空，否则mon会无法启动&lt;/li&gt;
&lt;li&gt;useAllDevices: 使用所有的设备，建议为false，否则会把宿主机所有可用的磁盘都干掉&lt;/li&gt;
&lt;li&gt;useAllNodes：使用所有的node节点，建议为false，肯定不会用k8s集群内的所有node来搭建ceph的&lt;/li&gt;
&lt;li&gt;databaseSizeMB和journalSizeMB：当磁盘大于100G的时候，就注释这俩项就行了&lt;br&gt;
本次实验用到的 cluster.yaml 文件内容如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;configmaps&amp;quot;]
  verbs: [ &amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;delete&amp;quot; ]
---
# Allow the operator to create resources in this cluster&#39;s namespace
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster-mgmt
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-cluster-mgmt
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph-system
---
# Allow the pods in this namespace to work with configmaps
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-cluster
subjects:
- kind: ServiceAccount
  name: rook-ceph-cluster
  namespace: rook-ceph
---
apiVersion: ceph.rook.io/v1beta1
kind: Cluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
    # v12 is luminous, v13 is mimic, and v14 is nautilus.
    # RECOMMENDATION: In production, use a specific version tag instead of the general v13 flag, which pulls the latest release and could result in different
    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
    image: ceph/ceph:v13
    # Whether to allow unsupported versions of Ceph. Currently only luminous and mimic are supported.
    # After nautilus is released, Rook will be updated to support nautilus.
    # Do not set to true in production.
    allowUnsupported: false
  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).
  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
  # In Minikube, the &#39;/data&#39; directory is configured to persist across reboots. Use &amp;quot;/data/rook&amp;quot; in Minikube environment.
  dataDirHostPath: /var/lib/rook
  # The service account under which to run the daemon pods in this cluster if the default account is not sufficient (OSDs)
  serviceAccount: rook-ceph-cluster
  # set the amount of mons to be started
  # count可以定义ceph-mon运行的数量，这里默认三个就行了
  mon:
    count: 3
    allowMultiplePerNode: true
  # enable the ceph dashboard for viewing cluster status
  # 开启ceph资源面板
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
  network:
    # toggle to use hostNetwork
    # 使用宿主机的网络进行通讯
    # 使用宿主机的网络貌似可以让集群外的主机挂载ceph
    # 但是我没试过，有兴趣的兄弟可以试试改成true
    # 反正这里只是集群内用，我就不改了
    hostNetwork: false
  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
  # The example under &#39;all&#39; would have all services scheduled on kubernetes nodes labeled with &#39;role=storage-node&#39; and
  # tolerate taints with a key of &#39;storage-node&#39;.
  placement:
#    all:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: role
#              operator: In
#              values:
#              - storage-node
#      podAffinity:
#      podAntiAffinity:
#      tolerations:
#      - key: storage-node
#        operator: Exists
# The above placement information can also be specified for mon, osd, and mgr components
#    mon:
#    osd:
#    mgr:
# nodeAffinity：通过选择标签的方式，可以限制pod被调度到特定的节点上
# 建议限制一下，为了让这几个pod不乱跑
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mon
              operator: In
              values:
              - enabled
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-osd
              operator: In
              values:
              - enabled
    mgr:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mgr
              operator: In
              values:
              - enabled
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
#    mgr:
#      limits:
#        cpu: &amp;quot;500m&amp;quot;
#        memory: &amp;quot;1024Mi&amp;quot;
#      requests:
#        cpu: &amp;quot;500m&amp;quot;
#        memory: &amp;quot;1024Mi&amp;quot;
# The above example requests/limits can also be added to the mon and osd components
#    mon:
#    osd:
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    deviceFilter:
    location:
    config:
      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.
      # Set the storeType explicitly only if it is required not to use the default.
      # storeType: bluestore
      # databaseSizeMB: &amp;quot;1024&amp;quot; # this value can be removed for environments with normal sized disks (100 GB or larger)
      # journalSizeMB: &amp;quot;1024&amp;quot;  # this value can be removed for environments with normal sized disks (20 GB or larger)
# Cluster level list of directories to use for storage. These values will be set for all nodes that have no `directories` set.
#    directories:
#    - path: /rook/storage-dir
# Individual nodes and their config can be specified as well, but &#39;useAllNodes&#39; above must be set to false. Then, only the named
# nodes below will be used as storage resources.  Each node&#39;s &#39;name&#39; field should match their &#39;kubernetes.io/hostname&#39; label.
#建议磁盘配置方式如下：
#name: 选择一个节点，节点名字为kubernetes.io/hostname的标签，也就是kubectl get nodes看到的名字
#devices: 选择磁盘设置为OSD
# - name: &amp;quot;sdb&amp;quot;:将/dev/sdb设置为osd
    nodes:
    - name: &amp;quot;kube-node1&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
    - name: &amp;quot;kube-node2&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
    - name: &amp;quot;kube-node3&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;

#      directories: # specific directories to use for storage can be specified for each node
#      - path: &amp;quot;/rook/storage-dir&amp;quot;
#      resources:
#        limits:
#          cpu: &amp;quot;500m&amp;quot;
#          memory: &amp;quot;1024Mi&amp;quot;
#        requests:
#          cpu: &amp;quot;500m&amp;quot;
#          memory: &amp;quot;1024Mi&amp;quot;
#    - name: &amp;quot;172.17.4.201&amp;quot;
#      devices: # specific devices to use for storage can be specified for each node
#      - name: &amp;quot;sdb&amp;quot;
#      - name: &amp;quot;sdc&amp;quot;
#      config: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: filestore
#    - name: &amp;quot;172.17.4.301&amp;quot;
#      deviceFilter: &amp;quot;^sd.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;开始部署ceph&#34;&gt;开始部署ceph&lt;/h2&gt;
&lt;p&gt;部署ceph&lt;br&gt;
kubectl apply -f cluster.yaml&lt;br&gt;
cluster会在rook-ceph这个namesapce创建资源&lt;br&gt;
看到所有的pod都Running就行了&lt;br&gt;
注意看一下pod分布的宿主机，跟我们打标签的主机是一致的&lt;br&gt;
kubectl -n rook-ceph get pod -o wide&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233756856.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;切换到其他主机看一下磁盘&lt;/p&gt;
&lt;p&gt;切换到kube-node1&lt;br&gt;
lsblk&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233803458.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;配置ceph-dashboard&#34;&gt;配置ceph dashboard&lt;/h2&gt;
&lt;p&gt;看一眼dashboard在哪个service上&lt;br&gt;
kubectl -n rook-ceph get service&lt;br&gt;
可以看到dashboard监听了8443端口&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233853544.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;创建个nodeport类型的service以便集群外部访问&lt;br&gt;
kubectl apply -f dashboard-external-https.yaml&lt;/p&gt;
&lt;p&gt;查看一下nodeport在哪个端口&lt;br&gt;
kubectl -n rook-ceph get service&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233909644.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;找出Dashboard的登陆账号和密码&lt;br&gt;
MGR_POD=&lt;code&gt;kubectl get pod -n rook-ceph | grep mgr | awk &#39;{print $1}&#39;&lt;/code&gt;&lt;br&gt;
kubectl -n rook-ceph logs $MGR_POD | grep password&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233955731.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;打开浏览器输入任意一个Node的IP+nodeport端口&lt;br&gt;
这里我的就是：https://192.168.1.2:30290&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234024005.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234065656.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;配置ceph为storageclass&#34;&gt;配置ceph为storageclass&lt;/h2&gt;
&lt;p&gt;官方给了一个样本文件：storageclass.yaml&lt;br&gt;
这个文件使用的是 RBD 块存储&lt;br&gt;
pool创建详解：https://rook.io/docs/rook/v0.8/ceph-pool-crd.html&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  #这个name就是创建成ceph pool之后的pool名字
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 1
  # size 池中数据的副本数,1就是不保存任何副本
  failureDomain: osd
  #  failureDomain：数据块的故障域，
  #  值为host时，每个数据块将放置在不同的主机上
  #  值为osd时，每个数据块将放置在不同的osd上
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: ceph
   # StorageClass的名字，pvc调用时填的名字
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use `rook` as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: xfs
# 设置回收策略默认为：Retain
reclaimPolicy: Retain
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;创建storageclass&#34;&gt;创建StorageClass&lt;/h2&gt;
&lt;p&gt;kubectl apply -f storageclass.yaml&lt;br&gt;
kubectl get storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;
kubectl describe storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;创建个nginx pod尝试挂载&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; nginx.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: ceph

---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports: 
  - port: 80
    name: nginx-port
    targetPort: 80
    protocol: TCP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /html
          name: http-file
      volumes:
      - name: http-file
        persistentVolumeClaim:
          claimName: nginx-pvc
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;kubectl apply -f nginx.yaml&lt;br&gt;
查看pv,pvc是否创建了&lt;br&gt;
kubectl get pv,pvc&lt;/p&gt;
&lt;p&gt;看一下nginx这个pod也运行了&lt;br&gt;
kubectl get pod&lt;/p&gt;
&lt;p&gt;删除这个pod,看pv是否还存在&lt;br&gt;
kubectl delete -f nginx.yaml&lt;/p&gt;
&lt;p&gt;kubectl get pv,pvc&lt;br&gt;
可以看到，pod和pvc都已经被删除了，但是pv还在！！！&lt;/p&gt;
&lt;h2 id=&#34;添加新节点进入集群&#34;&gt;添加新节点进入集群&lt;/h2&gt;
&lt;p&gt;这次我们要把node4添加进集群，先打标签&lt;br&gt;
kubectl label nodes kube-node4 ceph-osd=enabled&lt;br&gt;
重新编辑cluster.yaml文件&lt;br&gt;
原来的基础上添加node4的信息&lt;/p&gt;
&lt;p&gt;cd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;
vi cluster.yam&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234207475.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;
kubectl apply -f cluster.yaml&lt;/p&gt;
&lt;p&gt;盯着rook-ceph名称空间,集群会自动添加node4进来&lt;br&gt;
kubectl -n rook-ceph get pod -o wide -w&lt;br&gt;
kubectl -n rook-ceph get pod -o wide&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234283568.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234307875.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
去node4节点看一下磁盘&lt;br&gt;
lsblk&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234334204.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;删除一个节点&#34;&gt;删除一个节点&lt;/h2&gt;
&lt;p&gt;去掉node3的标签&lt;br&gt;
kubectl label nodes kube-node3 ceph-osd-&lt;br&gt;
重新编辑cluster.yaml文件&lt;br&gt;
删除node3的信息&lt;br&gt;
cd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;
vi cluster.yam&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234380826.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;
kubectl apply -f cluster.yaml&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234493936.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;kubectl -n rook-ceph get pod -o wide&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234548709.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234579591.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
最后记得删除宿主机的/var/lib/rook文件夹&lt;/p&gt;
">kubernetes搭建rook-ceph</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/about/"" data-c="
          &lt;blockquote&gt;
&lt;p&gt;欢迎来到我的小站呀，很高兴遇见你！🤝&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;关于本站&#34;&gt;🏠 关于本站&lt;/h2&gt;
&lt;p&gt;记录平时运维相关的技术:&lt;br&gt;
1.网络&lt;br&gt;
2.系统&lt;br&gt;
3.虚拟化&lt;br&gt;
4.数据库&lt;br&gt;
5.中间件&lt;br&gt;
...&lt;/p&gt;
&lt;h2 id=&#34;博主是谁&#34;&gt;👨‍💻 博主是谁&lt;/h2&gt;
&lt;p&gt;目前在职于巨人网络&lt;/p&gt;
&lt;h2 id=&#34;兴趣爱好&#34;&gt;⛹ 兴趣爱好&lt;/h2&gt;
&lt;p&gt;番剧 FPS射击游戏 dota&lt;/p&gt;
&lt;h2 id=&#34;联系我呀&#34;&gt;📬 联系我呀&lt;/h2&gt;
">关于</a>
      </div>
      
    </div>
  </div>
</div>
<script>
  // var escape = "[{&#34;content&#34;:&#34;&lt;p&gt;模拟生产消息：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;bin/kafka-console-producer.sh --broker-list 192.168.101.100:9092 --topic flink-test\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;模拟消费数据(从开始位置消费)&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --bootstrap-server  192.168.101.100:9092  --topic flink-test --from-beginning  |head\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;kafka-xiao-fei-ming-ling&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;kafka消费命令&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;kafka&#34;,&#34;slug&#34;:&#34;_ZocZPKKq&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/_ZocZPKKq/&#34;}],&#34;date&#34;:&#34;2020-09-28 16:12:09&#34;,&#34;dateFormat&#34;:&#34;2020-09-28&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/kafka-xiao-fei-ming-ling/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;1 min read&#34;,&#34;time&#34;:16000,&#34;words&#34;:52,&#34;minutes&#34;:1},&#34;description&#34;:&#34;模拟生产消息：\nbin/kafka-console-producer.sh --broker-list 192.168.101.100:9092 --topic flink-test\n\n模拟消费数据(从开始位置消费)\nbin/kafka-c...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;p&gt;logstash 直接把文件内容写入 hdfs 中， 并支持 hdfs 压缩格式。&lt;br&gt;\nlogstash 需要安装第三方插件，webhdfs插件，通过hdfs的web接口写入。&lt;br&gt;\n即 http://namenode00:50070/webhdfs/v1/ 接口&lt;/p&gt;\n&lt;p&gt;新版本logstash已默认安装webhdfs插件&lt;br&gt;\n官网地址及使用说明：&lt;br&gt;\nhttps://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html&lt;br&gt;\n检查hdfs的webhds接口&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;curl -i  &amp;quot;http://namenode:50070/webhdfs/v1/?user.name=hadoop&amp;amp;op=LISTSTATUS&amp;quot;   \nHTTP/1.1 200 OK\nCache-Control: no-cache\nExpires: Thu, 13 Jul 2017 04:53:39 GMT\nDate: Thu, 13 Jul 2017 04:53:39 GMT\nPragma: no-cache\nExpires: Thu, 13 Jul 2017 04:53:39 GMT\nDate: Thu, 13 Jul 2017 04:53:39 GMT\nPragma: no-cache\nContent-Type: application/json\nSet-Cookie: hadoop.auth=&amp;quot;u=hadoop&amp;amp;p=hadoop&amp;amp;t=simple&amp;amp;e=1499957619679&amp;amp;s=KSxdSAtjXAllhn73vh1MAurG9Bk=&amp;quot;; Path=/; Expires=Thu, 13-Jul-2017 14:53:39 GMT; HttpOnly\nTransfer-Encoding: chunked\nServer: Jetty(6.1.26)\n注释： active namenode 返回是200 ，standby namenode 返回是403.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;测试hdfs是否正常通讯：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;#通过webhdfs接口创建test.conf\ncurl -i -X PUT &amp;quot;http://hadoop-master:50070/webhdfs/v1/data/test.conf?user.name=hdfs&amp;amp;op=CREATE&amp;quot;\ncurl -i -T test.conf &amp;quot;http://hadoop-slave1:50075/webhdfs/v1/data/test.conf?op=CREATE&amp;amp;user.name=hdfs&amp;amp;namenoderpcaddress=hadoop-master:9000&amp;amp;overwrite=false&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;配置&lt;br&gt;\n添加 logstash 一个配置文件&lt;/p&gt;\n&lt;p&gt;vim /home/mtime/logstash-2.3.1/conf/hdfs.conf&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;input {\n    kafka {\n        bootstrap_servers =&amp;gt; &amp;quot;192.168.101.22:9092,192.168.101.23:9092,192.168.101.93:9092&amp;quot;\n        topics =&amp;gt; &amp;quot;test-logs&amp;quot;\n        group_id =&amp;gt; &amp;quot;hdfs-test-logs&amp;quot;\n        codec =&amp;gt; json\n\t    consumer_threads =&amp;gt; 15\n\n    }\n}\n\nfilter {\n    date {\n        match =&amp;gt; [&amp;quot;time&amp;quot;,&amp;quot;yyyy-MM-dd HH:mm:ss Z&amp;quot;]\n        target =&amp;gt; &amp;quot;@timestamp&amp;quot;\n        timezone =&amp;gt; &amp;quot;Asia/Shanghai&amp;quot;\n    }\n    ruby {\n        code =&amp;gt; &amp;quot;event.set(&#39;index.date&#39;, event.get(&#39;@timestamp&#39;).time.localtime.strftime(&#39;%Y%m%d&#39;))&amp;quot;\n    }\n    ruby {\n        code =&amp;gt; &amp;quot;event.set(&#39;index.hour&#39;, event.get(&#39;@timestamp&#39;).time.localtime.strftime(&#39;%H&#39;))&amp;quot;\n    }\n}\noutput {            \n    webhdfs {\n           host =&amp;gt; &amp;quot;hadoop-master&amp;quot;\n           port =&amp;gt; 50070\n           path =&amp;gt; &amp;quot;/data/pt-collect-log/test-logs/%{index.date}/application-%{index.hour}.log&amp;quot;\n           user =&amp;gt; &amp;quot;hdfs&amp;quot;\n\t   codec =&amp;gt; line { format =&amp;gt; &amp;quot;%{message}&amp;quot;}\n           flush_size =&amp;gt; 1000\n           compression =&amp;gt; &amp;quot;gzip&amp;quot;            \n           idle_flush_time =&amp;gt; 10\n           retry_interval =&amp;gt; 3\n\t   retry_times =&amp;gt; 100\n       }\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;关于hdfs部分配置，可以在 plugins-outputs-webhdfs 官网找到&lt;br&gt;\n启动 logstart&lt;br&gt;\ncd /home/mtime/logstash-2.3.1/bin/&lt;br&gt;\n./logstash -f ../conf/hdfs.conf    # 为前台启动&lt;br&gt;\n报错处理&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;[WARN ][logstash.outputs.webhdfs ] Failed to flush outgoing items {:outgoing_count=&amp;gt;160, :exception=&amp;gt;&amp;quot;WebHDFS::IOError&amp;quot;,\n我将hdfs端口 由原来的50070 改为 14000 端口，就在不报错了。\n官方提供的例子中用的就是50070端口，一直没有尝试14000端口。\n\n还有：\nbecause this file lease is currently owned by DFSClient\nhadoop 租约问题，后期正常就没有了。\n执行recoverLease来释放文件的锁\n\n$ hdfs debug recoverLease -path /logstash/2017/02/10/go-03.log\n还有：\n:message=&amp;gt;&amp;quot;webhdfs write caused an exception: {\\&amp;quot;RemoteException\\&amp;quot;:{\\&amp;quot;message\\&amp;quot;:\\&amp;quot;Failed to APPEND_FILE\n当一个进程在读写这个文件的时候，另一个进程应该是不能同时写入的。\n我们由原来3个logstash同时消费，改为了1个logstash消费，不在报错了。\n这个应该也可以通过有话写入hdfs参数来解决。\n\n还有：\nMax write retries reached. Exception: initialize: name or service not known {:level=&amp;gt;:error}\nlosgstash 需要能解析所有 hadoop 集群所有节点的主机名。\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;logstash-output-file-to-hdfs&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;logstash output file to HDFS&#34;,&#34;tags&#34;:[{&#34;name&#34;:&#34;hdfs&#34;,&#34;slug&#34;:&#34;geKI8DkKE&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/geKI8DkKE/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;logstash&#34;,&#34;slug&#34;:&#34;07WDPUsCB&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/07WDPUsCB/&#34;}],&#34;date&#34;:&#34;2020-09-04 10:45:33&#34;,&#34;dateFormat&#34;:&#34;2020-09-04&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/logstash-output-file-to-hdfs/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;4 min read&#34;,&#34;time&#34;:227000,&#34;words&#34;:734,&#34;minutes&#34;:4},&#34;description&#34;:&#34;logstash 直接把文件内容写入 hdfs 中， 并支持 hdfs 压缩格式。\nlogstash 需要安装第三方插件，webhdfs插件，通过hdfs的web接口写入。\n即 http://namenode00:50070/webhdfs...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;p&gt;使用ceph -s查看集群状态，发现一直有如下报错，且数量一直在增加&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;daemons have recently crashed\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;经查当前系统运行状态正常，判断这里显示的应该是历史故障，处理方式如下：&lt;/p&gt;\n&lt;p&gt;查看历史crash&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;ceph crash ls-new\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;根据ls出来的id查看详细信息&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;ceph crash info &amp;lt;crash-id&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;将历史crash信息进行归档，即不再显示&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;ceph crash archive &amp;lt;crash-id&amp;gt;\n\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;归档所有信息&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;ceph crash archive-all\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;ceph-bao-cuo-guan-li&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;ceph报错管理&#34;,&#34;tags&#34;:[{&#34;name&#34;:&#34;rook-ceph&#34;,&#34;slug&#34;:&#34;hu0mNDfuy&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/hu0mNDfuy/&#34;}],&#34;date&#34;:&#34;2020-08-25 18:12:33&#34;,&#34;dateFormat&#34;:&#34;2020-08-25&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/ceph-bao-cuo-guan-li/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;1 min read&#34;,&#34;time&#34;:30000,&#34;words&#34;:123,&#34;minutes&#34;:1},&#34;description&#34;:&#34;使用ceph -s查看集群状态，发现一直有如下报错，且数量一直在增加\ndaemons have recently crashed\n\n经查当前系统运行状态正常，判断这里显示的应该是历史故障，处理方式如下：\n查看历史crash\nceph cra...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;pre&gt;&lt;code&gt;#string到int  \nint,err:=strconv.Atoi(string)  \n#string到int64  \nint64, err := strconv.ParseInt(string, 10, 64)  \n#int到string  \nstring:=strconv.Itoa(int)  \n#int64到string  \nstring:=strconv.FormatInt(int64,10)  ```\n\n同类型之间转换，比如int64到int，直接int(int64)即可；\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;golang-bi-ji-stringintint64-hu-xiang-zhuan-huan&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;golang笔记-string、int、int64互相转换&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;golang&#34;,&#34;slug&#34;:&#34;oHQV9cP9p&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/oHQV9cP9p/&#34;}],&#34;date&#34;:&#34;2020-08-10 16:35:04&#34;,&#34;dateFormat&#34;:&#34;2020-08-10&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/golang-bi-ji-stringintint64-hu-xiang-zhuan-huan/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;1 min read&#34;,&#34;time&#34;:14000,&#34;words&#34;:46,&#34;minutes&#34;:1},&#34;description&#34;:&#34;#string到int  \nint,err:=strconv.Atoi(string)  \n#string到int64  \nint64, err := strconv.ParseInt(string, 10, 64)  \n#int到stri...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;p&gt;用golang写一个restful api。如果您不知道什么是restful,可以看&lt;a href=\&#34;http://www.ruanyifeng.com/blog/2014/05/restful_api.html\&#34;&gt;阮一峰老师的教程&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;首先，我们需要解决的是路由的问题，也就是如何将不同的url映射到不同的处理函数。&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;    router.GET(&amp;quot;/api/todo/:todoid&amp;quot;, getTodoById)\n    router.POST(&amp;quot;/api/todo/&amp;quot;, addTodo)\n    router.DELETE(&amp;quot;/api/todo/:todoid&amp;quot;, deleteTodo)\n    router.PUT(&amp;quot;/api/todo/:todoid&amp;quot;, modifyTodo)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;作为一个初学者，我马上打开github,找到了&lt;a href=\&#34;https://github.com/avelino/awesome-go\&#34;&gt;awesome-go&lt;/a&gt;,经过一番调研，我感觉有几个http router的库比较适合：bone, httprouter, mux&lt;/p&gt;\n&lt;h2 id=\&#34;基本框架\&#34;&gt;基本框架&lt;/h2&gt;\n&lt;p&gt;首先，我们设计了四个路由，分别为根据Id获得todo，增加todo，修改todo，删除todo。这里关于解析路由参数，我们使用了httprouter.Params的ByName函数。&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;package main\n\nimport (\n  &amp;quot;fmt&amp;quot;\n  &amp;quot;github.com/julienschmidt/httprouter&amp;quot;\n  &amp;quot;net/http&amp;quot;\n  &amp;quot;log&amp;quot;\n  &amp;quot;io&amp;quot;\n  &amp;quot;io/ioutil&amp;quot;\n)\n\nfunc getTodoById(w http.ResponseWriter, r *http.Request, params httprouter.Params){\n  todoid := params.ByName(&amp;quot;todoid&amp;quot;)\n  fmt.Fprintf(w, &amp;quot;getTodo %s\\n&amp;quot;, todoid)\n}\n\nfunc addTodo(w http.ResponseWriter, r *http.Request, _ httprouter.Params){\n  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))\n  fmt.Fprintf(w, &amp;quot;addTodo! %s\\n&amp;quot;,body)\n}\n\nfunc deleteTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){\n  todoid := params.ByName(&amp;quot;todoid&amp;quot;)\n  fmt.Fprintf(w, &amp;quot;deleteTodo %s\\n&amp;quot;, todoid)\n}\n\nfunc modifyTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){\n  todoid := params.ByName(&amp;quot;todoid&amp;quot;)\n  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))\n  fmt.Fprintf(w, &amp;quot;modifyTodo %s to %s\\n&amp;quot;, todoid, body)\n}\n\nfunc main() {\n    router := httprouter.New()\n    router.GET(&amp;quot;/api/todo/:todoid&amp;quot;, getTodoById)\n    router.POST(&amp;quot;/api/todo/&amp;quot;, addTodo)\n    router.DELETE(&amp;quot;/api/todo/:todoid&amp;quot;, deleteTodo)\n    router.PUT(&amp;quot;/api/todo/:todoid&amp;quot;, modifyTodo)\n    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, router))\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;我们可以用curl来测试一下我们的api,以put为例&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;curl --data &amp;quot;content=shopping&amp;amp;time=tomorrow&amp;quot; http://127.0.0.1:8080/api/todo/123 -X PUT\n\n// modifyTodo 123 to content=shopping&amp;amp;time=tomorrow\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;json的解析\&#34;&gt;json的解析&lt;/h2&gt;\n&lt;p&gt;我们在使用restful api的时候，常常需要给后台传递数据。从上面可以看到，我们通过http.Request的Body属性可以获得数据&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;从上面，我们读出的数据是[]byte，但是我们希望将其解析为对象，那么在这之前，我们需要先定义我们的struct。假设我们的todo只有一个字段，就是Name&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;type Todo struct {\n    Name      string\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;现在我们可以这样解析&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var todo Todo;\njson.Unmarshal(body, &amp;amp;todo);\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;model层设计\&#34;&gt;model层设计&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;package main\n\nimport (\n  &amp;quot;gopkg.in/mgo.v2&amp;quot;\n  &amp;quot;fmt&amp;quot;\n  &amp;quot;log&amp;quot;\n  &amp;quot;gopkg.in/mgo.v2/bson&amp;quot;\n)\n\nvar session *mgo.Session\n\nfunc init(){\n  session,_ = mgo.Dial(&amp;quot;mongodb://127.0.0.1&amp;quot;)\n}\n\ntype Todo struct {\n    Name      string\n}\n\nfunc createTodo(t Todo){\n  c := session.DB(&amp;quot;test&amp;quot;).C(&amp;quot;todo&amp;quot;)\n  c.Insert(&amp;amp;t)\n}\n\nfunc queryTodoById(id string){\n  c := session.DB(&amp;quot;test&amp;quot;).C(&amp;quot;todo&amp;quot;)\n  result := Todo{}\n\n  err := c.Find(bson.M{&amp;quot;_id&amp;quot;: bson.ObjectIdHex(id)}).One(&amp;amp;result)\n  if err != nil {\n    log.Fatal(err)\n  }\n\n  fmt.Println(&amp;quot;Todo:&amp;quot;, result.Name)\n}\n\nfunc removeTodo(id string){\n  c := session.DB(&amp;quot;test&amp;quot;).C(&amp;quot;todo&amp;quot;)\n  err := c.Remove(bson.M{&amp;quot;_id&amp;quot;: bson.ObjectIdHex(id)})\n  if err != nil{\n    log.Fatal(err)\n  }\n}\n\nfunc updateTodo(id string, update interface{}){\n  //change := bson.M{&amp;quot;$set&amp;quot;: bson.M{&amp;quot;name&amp;quot;: &amp;quot;hahaha&amp;quot;}}\n  c := session.DB(&amp;quot;test&amp;quot;).C(&amp;quot;todo&amp;quot;)\n  err := c.Update(bson.M{&amp;quot;_id&amp;quot;: bson.ObjectIdHex(id)}, update)\n  if err != nil{\n    log.Fatal(err)\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;我们定义了Todo的struct,并添加了几种函数。&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;package main\n\nimport (\n  &amp;quot;fmt&amp;quot;\n  &amp;quot;github.com/julienschmidt/httprouter&amp;quot;\n  &amp;quot;net/http&amp;quot;\n  &amp;quot;log&amp;quot;\n  &amp;quot;io&amp;quot;\n  &amp;quot;io/ioutil&amp;quot;\n  &amp;quot;encoding/json&amp;quot;\n  &amp;quot;gopkg.in/mgo.v2/bson&amp;quot;\n)\n\nfunc getTodoById(w http.ResponseWriter, r *http.Request, params httprouter.Params){\n  todoid := params.ByName(&amp;quot;todoid&amp;quot;)\n  queryTodoById(todoid)\n  fmt.Fprintf(w, &amp;quot;getUser %s\\n&amp;quot;, todoid)\n}\n\nfunc addTodo(w http.ResponseWriter, r *http.Request, _ httprouter.Params){\n  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))\n  var todo Todo;\n  json.Unmarshal(body, &amp;amp;todo);\n  createTodo(todo)\n  fmt.Fprintf(w, &amp;quot;addUser! %s\\n&amp;quot;,body)\n}\n\nfunc deleteTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){\n  todoid := params.ByName(&amp;quot;todoid&amp;quot;)\n  removeTodo(todoid)\n  fmt.Fprintf(w, &amp;quot;deleteUser %s\\n&amp;quot;, todoid)\n}\n\nfunc modifyTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){\n  todoid := params.ByName(&amp;quot;todoid&amp;quot;)\n  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))\n  var todo Todo\n  json.Unmarshal(body, &amp;amp;todo);\n  change := bson.M{&amp;quot;$set&amp;quot;: bson.M{&amp;quot;name&amp;quot;: todo.Name}}\n  updateTodo(todoid,change)\n  fmt.Fprintf(w, &amp;quot;modifyUser %s to %s\\n&amp;quot;, todoid, body)\n}\n\nfunc main() {\n    router := httprouter.New()\n    router.GET(&amp;quot;/api/todo/:todoid&amp;quot;, getTodoById)\n    router.POST(&amp;quot;/api/todo/&amp;quot;, addTodo)\n    router.DELETE(&amp;quot;/api/todo/:todoid&amp;quot;, deleteTodo)\n    router.PUT(&amp;quot;/api/todo/:todoid&amp;quot;, modifyTodo)\n    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, router))\n}&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;golang-bi-ji-go-restful&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;golang笔记-go-restful&#34;,&#34;tags&#34;:[],&#34;date&#34;:&#34;2020-06-22 14:45:14&#34;,&#34;dateFormat&#34;:&#34;2020-06-22&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/golang-bi-ji-go-restful/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;6 min read&#34;,&#34;time&#34;:326000,&#34;words&#34;:998,&#34;minutes&#34;:6},&#34;description&#34;:&#34;用golang写一个restful api。如果您不知道什么是restful,可以看阮一峰老师的教程\n首先，我们需要解决的是路由的问题，也就是如何将不同的url映射到不同的处理函数。\n    router.GET(&amp;quot;/api/to...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6\&#34;&gt;基本框架&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#json%E7%9A%84%E8%A7%A3%E6%9E%90\&#34;&gt;json的解析&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#model%E5%B1%82%E8%AE%BE%E8%AE%A1\&#34;&gt;model层设计&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h2 id=\&#34;jenkins是什么\&#34;&gt;jenkins是什么？&lt;/h2&gt;\n&lt;p&gt;Jenkins是一个开源的持续集成工具，可用于自动化的执行与构建，测试和交付或部署软件有关的各种任务,有非常丰富的插件支持。&lt;/p&gt;\n&lt;h2 id=\&#34;kubernetes是什么\&#34;&gt;kubernetes是什么？&lt;/h2&gt;\n&lt;p&gt;Kubernetes是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。这个视频生动地介绍了k8s&lt;/p&gt;\n&lt;h2 id=\&#34;jenkins-on-k8s-有什么好处\&#34;&gt;jenkins on k8s 有什么好处？&lt;/h2&gt;\n&lt;p&gt;jenkins通过单Master多个Slave的方式提供服务，Master保存了任务的配置信息，安装的插件等等，而slave主要负责执行任务，在使用中存在以下几个问题：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;当存在多个slave时，运行slave的机器难以统一管理，每次添加新节点时总要做大量的重复工作。&lt;/li&gt;\n&lt;li&gt;由于不同业务的构建频率并不相同，在使用会发现有很多slave大多数时间都处于空闲状态，造成资源浪费&lt;/li&gt;\n&lt;li&gt;jenkins默认采取保守的调度方式，造成某些slave的负载过高，任务不能平均分配&lt;/li&gt;\n&lt;/ol&gt;\n&lt;h2 id=\&#34;jenkins架构\&#34;&gt;jenkins架构&lt;/h2&gt;\n&lt;p&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1592447693022.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n使用k8s管理jenkins具有以下优势：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;使用docker运行jenkins保证环境的一致性，可以根据不同业务选择合适的镜像&lt;/li&gt;\n&lt;li&gt;k8s对抽象后的资源（pods）进行统一的管理调度，提供资源隔离和共享，使机器计算资源变得弹性可扩展,避免资源浪费。&lt;/li&gt;\n&lt;li&gt;k8s提供容器的自愈功能，能够保证始终有一定数量的容器是可用的&lt;/li&gt;\n&lt;li&gt;k8s默认的调度器提供了针对节点当前资源分配容器的调度策略，调度器支持插件化部署便于自定义。&lt;/li&gt;\n&lt;/ol&gt;\n&lt;h2 id=\&#34;一搭建环境\&#34;&gt;一，搭建环境&lt;/h2&gt;\n&lt;h3 id=\&#34;工具准备\&#34;&gt;工具准备&lt;/h3&gt;\n&lt;pre&gt;&lt;code&gt;kubernetes v1.8.4\ndocker v1.12.6\njenkins master镜像 jenkins/jenkins:lts（v2.73.3）\nslave镜像 jenkinsci/jnlp-slave\nKubernetes plugin (v1.1)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h3 id=\&#34;安装kubernetes集群\&#34;&gt;安装kubernetes集群&lt;/h3&gt;\n&lt;p&gt;中文教程：https://www.kubernetes.org.cn/2906.html&lt;br&gt;\n省略.....&lt;/p&gt;\n&lt;h2 id=\&#34;二创建statefulset\&#34;&gt;二，创建StatefulSet&lt;/h2&gt;\n&lt;p&gt;StatefulSet(有状态副本集)：Deployments适用于运行无状态应用，StatefulSet则为有状态的应用提供了支持，可以为应用提供有序的部署和扩展，稳定的持久化存储，我们使用SS来运行jenkins master。&lt;/p&gt;\n&lt;p&gt;创建完整的Stateful Set需要依次创建一下对象：&lt;br&gt;\n1、Persistent Volume&lt;br&gt;\n2、Persistent Volume Claim&lt;br&gt;\n3、StatefulSet&lt;br&gt;\n4、Service&lt;/p&gt;\n&lt;p&gt;创建PersistentVolume：&lt;br&gt;\n为了保存应用运行时的数据需要先创建k8s的卷文件，K8s中存在Volume和PersistentVolume两种类型：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;Volume：与docker中的volume不同在于Volume生命周期是和pod绑定的，与pod中的container无关。k8s为Volume提供了多种类型文件系统（cephfs,nfs…,简单起见我直接选择了hostPath，使用的node节点本地的存储系统）&lt;/li&gt;\n&lt;li&gt;PersistentVolume:从名字可以看出来，PV的生命周期独立于使用它的pod，不会像volume随pod消失而消失，而是做为一个集群中的资源存在（像node节点一样），同时PV屏蔽了使用具体存储系统的细节。&lt;br&gt;\nk8s中的对象都是通过yaml文件来定义的，首先创建名为jenkins-volume.yml的文件:&lt;/li&gt;\n&lt;/ol&gt;\n&lt;p&gt;❣️❣️注意：PV的创建有静态，动态两种方式，动态创建可以减少管理员的操作步骤，需要提供指定的StorageClass。为了测试方便，所以我们直接选择静态创建，manual是一个不存在的storage class&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: jenkins-volume\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: &amp;quot;/tmp/data&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;master节点执行下面的命令，PV就手动创建完了&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kubectl create -f jenkins-volume1.yaml\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;创建PersistentVolumeClaim：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;PersistentVolumeClaim(PVC):\n持久化存储卷索取，如果说PV是集群中的资源，PVC就是资源的消费者，PVC可以指定需要的资源大小和访问方式,pod不会和PV直接接触，而是通过PVC来请求资源，PV的生成阶段叫做provision,生成PV后会绑定到PVC对象，然后才能被其他对象使用。\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;PV和PVC的生命周期如下图：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1592447954632.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;pv life&lt;br&gt;\n创建文件jenkins-claim.yaml&lt;br&gt;\n注意： name必须为jenkins-home-jenkins-0否则会绑定失败&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: jenkins-home-jenkins-0\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;执行命令kubectl create -f jenkins-claim.yaml&lt;br&gt;\n然后查看PVC是否创建成功，status为bound说明PVC已经绑定&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl describe pvc jenkins-home-jenkins-0\nName:          jenkins-home-jenkins-0\nNamespace:     kubernetes-plugin\nStorageClass:  manual\nStatus:        Bound\nVolume:        jenkins-volume\nLabels:        &amp;lt;none&amp;gt;\nAnnotations:   pv.kubernetes.io/bind-completed=yes\n               pv.kubernetes.io/bound-by-controller=yes\nCapacity:      10Gi\nAccess Modes:  RWO\nEvents:        &amp;lt;none&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;创建StatefulSet和Service：&lt;br&gt;\n从kubernetes-plugin github仓库下载jenkins.yml文件&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/jenkinsci/kubernetes-plugin/master/src/main/kubernetes/jenkins.yml\n修改jenkins.yml：\n去掉87行externalTrafficPolicy: Local（这是GKE使用的）\n修改83行type: LoadBalancer改为type: NodePort\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;注意：&lt;br&gt;\nservice type=ClusterIP时只允许从集群内部访问， type设置为NodePort是为了从集群外的机器访问jenkins,请谨慎使用，开启NodePort会在所有节点（含master）的统一固定端口开放服务。&lt;/p&gt;\n&lt;p&gt;执行命令&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create -f jenkins.yml \nstatefulset &amp;quot;jenkins&amp;quot; created\nservice &amp;quot;jenkins&amp;quot; created\n访问jenkins master,地址为masterip:32058\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;#查看映射的端口&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get service jenkins\nNAME      TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)                        AGE\njenkins   NodePort   10.96.82.68   &amp;lt;none&amp;gt;        80:32058/TCP,50000:30345/TCP   1m\n\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;查看pod : jenkins-0的容器日志，粘贴下面的密码进入jenkins,jenkins安装完成。&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;Jenkins initial setup is required. An admin user has been created and a password generated.\nPlease use the following password to proceed to installation:\n70aa7b41ba894855abccd09306625b8a\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h3 id=\&#34;问题分析\&#34;&gt;问题分析&lt;/h3&gt;\n&lt;p&gt;1.创建stateful set时失败，提示”PersistentVolumeClaim is not bound: “jenkins-home-jenkins-0”：”&lt;br&gt;\n因为采用静态创建PV时，StatefulSet会按照固定名称查找PVC，PVC的名字要满足&lt;/p&gt;\n&lt;p&gt;PVC_name == volumeClaimTemplates_name + “-“ + pod_name&lt;/p&gt;\n&lt;p&gt;这里的名字就是jenkins-home-jenkins-0&lt;/p&gt;\n&lt;p&gt;2.pod启动失败，jenkins用户没有目录权限&lt;br&gt;\n错误提示”touch: cannot touch ‘/var/jenkins_home/copy_reference_file.log’: Permission denied&lt;br&gt;\nCan not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions?”&lt;br&gt;\n要确保节点目录开放权限,在node上执行命令：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;sudo chown -R 1000:1000 /var/jenkins_home/\nsudo chown -R 1000:1000 /tmp/data\n##如果仍然失败，尝试在node上重启docker\nsystemctl restart docker\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;注意pv指定的hostPath权限也要修改，否则是无效的&lt;/p&gt;\n&lt;h2 id=\&#34;三-配置jenkins\&#34;&gt;三 ，配置jenkins&lt;/h2&gt;\n&lt;p&gt;创建jenkins服务账号&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/jenkinsci/kubernetes-plugin/master/src/main/kubernetes/service-account.yml\nkubectl create -f service-account.yml\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;配置插件&lt;br&gt;\n访问http://masterip:32058/pluginManager/,搜索插件Kubernetes plugin安装；&lt;br&gt;\n访问 http://masterip:32058/configure&lt;br&gt;\n选择新建云–kubernetes,在URl填写api server地址，&lt;br&gt;\n执行kubectl describe命令，复制output中的token，填入到 ‘Kubernetes server certificate key’&lt;/p&gt;\n&lt;p&gt;[root@master ~]# kubectl get secret&lt;br&gt;\nNAME                  TYPE                                  DATA      AGE&lt;br&gt;\ndefault-token-4kb54   kubernetes.io/service-account-token   3         1d&lt;br&gt;\njenkins-token-wzbsx   kubernetes.io/service-account-token   3         1d&lt;br&gt;\n[root@master ~]# kubectl describe secret/jenkins-token-wzbsx&lt;br&gt;\n...&lt;br&gt;\njenkins url,tunnel填写service的CLUSTER-IP即可，结果如图：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1592448124617.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;peizhi1&lt;br&gt;\n选择add pod template，填写下面的内容，retain slave可以设置运行jenkins slave 的container空闲后能存活多久。&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1592448148171.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;content&lt;br&gt;\n插件配置完成。&lt;/p&gt;\n&lt;h2 id=\&#34;四-测试\&#34;&gt;四 ，测试&lt;/h2&gt;\n&lt;ol&gt;\n&lt;li&gt;扩容测试&lt;br&gt;\nStatefulSet扩容：&lt;br&gt;\n首先需要手动创建PV，PVC(见第二步),然后执行扩容命令&lt;/li&gt;\n&lt;/ol&gt;\n&lt;pre&gt;&lt;code&gt;kubectl scale statefulset/jenkins --replicas=２\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;查看StatefulSet,此时已经拥有两个master节点，访问service时会随机将请求发送给后端的master。&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get statefulset/jenkins \nNAME      DESIRED   CURRENT   AGE\njenkins   2         2         5d\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;虽然通过k8s可以轻松实现jenkins master节点的拓展，但是由于jenkins存储数据的方式通过本地文件存储，master之间的数据同步还是一个麻烦的问题，参考jenkins存储模型。&lt;/p&gt;\n&lt;p&gt;jenkins master上保存的文件：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;ls /temp/data\njenkins.CLI.xml\njenkins.install.InstallUtil.lastExecVersion\njenkins.install.UpgradeWizard.state\njenkins.model.ArtifactManagerConfiguration.xml\njenkins.model.JenkinsLocationConfiguration.xml\njobs\nlogs\nnodeMonitors.xml\nnodes\n&lt;/code&gt;&lt;/pre&gt;\n&lt;ol start=\&#34;2\&#34;&gt;\n&lt;li&gt;高可用测试&lt;br&gt;\n现在stateful set中已经有两个pod,在jenkins-1所在的节点执行docker stop停止运行jenkins-master的容器，同时在命令行查看pod的状态，可以看到jenkins-1异常（Error状态）之后慢慢恢复了运行状态（Running）：&lt;/li&gt;\n&lt;/ol&gt;\n&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get pods -w\nNAME        READY     STATUS    RESTARTS   AGE\njenkins-0   1/1       Running   0          1d\njenkins-1   0/1       Running   1         20h\njenkins-1   1/1       Running   1         20h\njenkins-1   0/1       Error     1         20h\njenkins-1   0/1       CrashLoopBackOff   1         20h\njenkins-1   0/1       Running   2         20h\njenkins-1   1/1       Running   2         20h\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;kubectl describe pod jenkins-1查看pod的事件日志，k8s通过探针(probe)接口检测到服务停止之后自动执行了拉取镜像，重启container的操作。&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;Events:\n  Type     Reason      Age                From                              Message\n  ----     ------      ----               ----                              -------\n  Warning  Unhealthy   27m (x2 over 27m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Readiness probe failed: HTTP probe failed with statuscode: 503\n  Warning  Unhealthy   27m (x2 over 27m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Liveness probe failed: HTTP probe failed with statuscode: 503\n  Warning  Unhealthy   24m                kubelet, iz8pscwd1fv6kprs1zw21kz  Readiness probe failed: Get http://192.168.24.4:8080/login: dial tcp 192.168.24.4:8080: getsockopt: connection refused\n  Warning  BackOff     20m (x2 over 20m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Back-off restarting failed container\n  Warning  FailedSync  20m (x2 over 20m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Error syncing pod\n  Normal   Pulling     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  pulling image &amp;quot;jenkins/jenkins:lts-alpine&amp;quot;\n  Normal   Started     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Started container\n  Normal   Pulled      19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Successfully pulled image &amp;quot;jenkins/jenkins:lts-alpine&amp;quot;\n  Normal   Created     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Created container\n&lt;/code&gt;&lt;/pre&gt;\n&lt;ol start=\&#34;3\&#34;&gt;\n&lt;li&gt;jenkins构建测试&lt;br&gt;\n当前集群中使用的jenkins slave镜像只包含一个java运行环境来运行jenkins-slave.jar,在实际使用中需要自定义合适的镜像。选择自定义镜像之后需要修改插件的配置，同样name命名为jnlp替换默认镜像，arguments安装工具提示填写即可。&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1592448208433.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n创建job，同时开始构建,k8s会在不同节点上创建pod来运行任务&lt;/li&gt;\n&lt;/ol&gt;\n&lt;p&gt;jenkins默认调度策略&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;尝试在上次构建的节点上构建，指定某台slave之后会一直使用。&lt;/li&gt;\n&lt;li&gt;当队列有2个构建时，不会立刻创建两个executor,而是先创建一个executor然后尝试等待executor空闲，目的是保证每个executor被充分利用。&lt;br&gt;\nk8s调度策略&lt;br&gt;\n使用Pod.spec.nodeSelector根据label为pod选择node&lt;br&gt;\n3 .调度器scheduler有Predicates，Priorities两个阶段，分别负责节点过滤和评分排序，各个阶段都有k8s提供的检查项，我们可以自由组合。&lt;br&gt;\n（比如PodFitsResources检查cpu内存等资源，PodFitsHostPorts检查端口占用，SelectorSpreadPriority要求一个服务尽量分散分布）自定义schduler参考&lt;br&gt;\n资源不足时会发生什么&lt;br&gt;\n当前集群中有3个节点，我在node2运行一个CPU占用限制在80%的程序,然后设置jenkins插件ContainerTemplate的request和limit均为cpu 500m,内存500Mi,（500m代表单核CPU的50%）看一下pod会怎么调度&lt;br&gt;\nk8s仍然尝试在node2分配节点（为什么其他节点不行），结果POD处于pending状态：&lt;/li&gt;\n&lt;/ol&gt;\n&lt;pre&gt;&lt;code&gt;{\n&amp;quot;phase&amp;quot;: &amp;quot;Pending&amp;quot;,\n&amp;quot;conditions&amp;quot;: [\n  {\n    &amp;quot;type&amp;quot;: &amp;quot;PodScheduled&amp;quot;,\n    &amp;quot;status&amp;quot;: &amp;quot;False&amp;quot;,\n    &amp;quot;lastProbeTime&amp;quot;: null,\n    &amp;quot;lastTransitionTime&amp;quot;: &amp;quot;2017-12-09T08:29:10Z&amp;quot;,\n    &amp;quot;reason&amp;quot;: &amp;quot;Unschedulable&amp;quot;,\n    &amp;quot;message&amp;quot;: &amp;quot;No nodes are available that match all of the predicates: Insufficient cpu (4), PodToleratesNodeTaints (1).&amp;quot;\n  }\n],\n&amp;quot;qosClass&amp;quot;: &amp;quot;Guaranteed&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;最后pod被删除，而jenkins任务会阻塞一直到有其他空闲的slave出现。&lt;/p&gt;\n&lt;h2 id=\&#34;五总结\&#34;&gt;五，总结&lt;/h2&gt;\n&lt;p&gt;本文介绍了在k8s集群部署jenkins服务的方式和k8s带来的资源管理便捷，由于我也是刚开始接触k8s,所用的实例只是搭建了用于测试的实验环境，离在实际生产环境中使用还有问题需要验证。&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;jenkins-x-on-kubernetes-shi-jian-zhi-chi-duo-zhu&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;jenkins x on kubernetes实践(支持多主)&#34;,&#34;tags&#34;:[{&#34;name&#34;:&#34;jenkinsx&#34;,&#34;slug&#34;:&#34;bGPINTeO5&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/bGPINTeO5/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;kubernetes&#34;,&#34;slug&#34;:&#34;KWlALzyzR&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/KWlALzyzR/&#34;}],&#34;date&#34;:&#34;2020-06-18 10:26:17&#34;,&#34;dateFormat&#34;:&#34;2020-06-18&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/jenkins-x-on-kubernetes-shi-jian-zhi-chi-duo-zhu/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;12 min read&#34;,&#34;time&#34;:704000,&#34;words&#34;:2746,&#34;minutes&#34;:12},&#34;description&#34;:&#34;jenkins是什么？\nJenkins是一个开源的持续集成工具，可用于自动化的执行与构建，测试和交付或部署软件有关的各种任务,有非常丰富的插件支持。\nkubernetes是什么？\nKubernetes是容器集群管理系统，是一个开源的平台，可...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#jenkins%E6%98%AF%E4%BB%80%E4%B9%88\&#34;&gt;jenkins是什么？&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#kubernetes%E6%98%AF%E4%BB%80%E4%B9%88\&#34;&gt;kubernetes是什么？&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#jenkins-on-k8s-%E6%9C%89%E4%BB%80%E4%B9%88%E5%A5%BD%E5%A4%84\&#34;&gt;jenkins on k8s 有什么好处？&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#jenkins%E6%9E%B6%E6%9E%84\&#34;&gt;jenkins架构&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E4%B8%80%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83\&#34;&gt;一，搭建环境&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%B7%A5%E5%85%B7%E5%87%86%E5%A4%87\&#34;&gt;工具准备&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4\&#34;&gt;安装kubernetes集群&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E4%BA%8C%E5%88%9B%E5%BB%BAstatefulset\&#34;&gt;二，创建StatefulSet&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90\&#34;&gt;问题分析&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E4%B8%89-%E9%85%8D%E7%BD%AEjenkins\&#34;&gt;三 ，配置jenkins&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%9B%9B-%E6%B5%8B%E8%AF%95\&#34;&gt;四 ，测试&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E4%BA%94%E6%80%BB%E7%BB%93\&#34;&gt;五，总结&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;p&gt;consul删除无效实例&lt;br&gt;\nhttp://127.0.0.1:8500/v1/agent/service/deregister/test-9c14fa595ddfb8f4c34c673c65b072bb&lt;/p&gt;\n&lt;p&gt;test-9c14fa595ddfb8f4c34c673c65b072bb : 实例id&lt;br&gt;\nmethod : put&lt;/p&gt;\n&lt;p&gt;删除无效节点&lt;br&gt;\nhttp://127.0.0.1:8500/v1/v1/agent/force-leave/4b36b27317a0&lt;/p&gt;\n&lt;p&gt;consul leave #关闭consul并离开集群。也可以使用Ctrl+C或kill -INT来gracefully停止agent，这种体面的离开方式让consule可以有机会通知集群其他成员自己的离开。如果你强制地结束了agent，其他member会检测到这个节点的failed。当成员离开时，它的services和checks都会从catalog中移除。当成员failed时，它的health只是简单的被标记为critical，并不会从catalog中移除。Consul会自动尝试重新连接failed节点，允许它从恶劣的网络环境中恢复，显然离开的nodes不会被重新连接。另外，如果这个节点是server，体面的离开对避免潜在的中断的可能很重要。&lt;br&gt;\n为了防止dead nodes的积累，consul会自动把dead nodes移除出catalog。这个过程被称为reaping（收割）。默认是72小时的间隔（不建议更改）&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;#!/bin/bash\nclear \necho &amp;quot;node_exporter注销工具&amp;quot;\nread -p &amp;quot;请输入要踢掉的节点IP,如果有多个IP,请使用英文格式 &#39;,&#39; 隔开: &amp;quot; IP_LIST\n\nfor IP in `echo &amp;quot;${IP_LIST}&amp;quot;|awk -F, &#39;BEGIN{OFS=&amp;quot; &amp;quot;}{$1=$1;printf(&amp;quot;%s&amp;quot;,$0);}&#39;`\ndo \n   curl -XPUT http://10.100.x.x:8500/v1/agent/service/deregister/node-${IP}\n   echo &amp;quot;${IP}节点已剔除!&amp;quot;\ndone\necho &amp;quot;${IP_LIST}完成剔除&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;consul-shan-chu-wu-xiao-fu-wu-yu-jie-dian&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;consul-删除无效服务与节点&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;consul&#34;,&#34;slug&#34;:&#34;Ud1dybROf&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Ud1dybROf/&#34;}],&#34;date&#34;:&#34;2020-06-17 17:43:15&#34;,&#34;dateFormat&#34;:&#34;2020-06-17&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/consul-shan-chu-wu-xiao-fu-wu-yu-jie-dian/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;2 min read&#34;,&#34;time&#34;:91000,&#34;words&#34;:362,&#34;minutes&#34;:2},&#34;description&#34;:&#34;consul删除无效实例\nhttp://127.0.0.1:8500/v1/agent/service/deregister/test-9c14fa595ddfb8f4c34c673c65b072bb\ntest-9c14fa595ddfb8...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;p&gt;&amp;quot;kubernetes&amp;quot;（v1.10.2）表示我的pod（包含一个容器）使用了大约5GB的内存。在容器内部，RSS说的更像681Mib。任何人都能用以下数据解释如何从681Mib到5GB吗（或者用我省略的另一个命令来描述如何弥补差异，无论是从容器还是从在Kubernetes中运行此容器的Docker主机）？&lt;br&gt;\nKubectl Top Pods表示5GB：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;% kubectl top pods -l app=myapp\nNAME                             CPU(cores)   MEMORY(bytes)\nmyapp-56b947bf6d-2lcr7           39m          5039Mi\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;Cadvisor报告了类似的数字（可能来自稍有不同的时间，因此请忽略细微的差异）：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;container_memory_usage_bytes{pod_name=~&amp;quot;.*myapp.*&amp;quot;}      5309456384\n\n5309456384 / 1024.0 / 1024 ~= 5063 ~= 5039\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;在容器中，此文件似乎是cadvisor获取其数据的位置：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;# kubectl exec -it myapp-56b947bf6d-2lcr7 bash\nmeme@myapp-56b947bf6d-2lcr7:/app# cat /sys/fs/cgroup/memory/memory.usage_in_bytes\n5309456384\n容器中的常驻集大小（RSS）不匹配（小于1GB）：\nmeme@myapp-56b947bf6d-2lcr7:/app# kb=$(ps aux | grep -v grep | grep -v &#39;ps aux&#39; | grep -v bash | grep -v awk | grep -v RSS | awk &#39;{print $6}&#39; | awk &#39;{s+=$1} END {printf &amp;quot;%.0f&amp;quot;, s}&#39;); mb=$(expr $kb / 1024); printf &amp;quot;Kb: $kb\\nMb: $mb\\n&amp;quot;\nKb: 698076\nMb: 681\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;完整的PS AUX，以防有帮助：\nmeme@myapp-56b947bf6d-2lcr7:/app# ps aux | grep -v grep | grep -v &#39;ps aux&#39; | grep -v bash | grep -v awk\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nmeme         1  0.0  0.0 151840 10984 ?        Ss   Jun04   0:29 /usr/sbin/apache2 -D FOREGROUND\nwww-data    10  0.0  0.0 147340  4652 ?        S    Jun04   0:00 /usr/sbin/apache2 -D FOREGROUND\nwww-data    11  0.0  0.0 148556  4392 ?        S    Jun04   0:16 /usr/sbin/apache2 -D FOREGROUND\nwww-data    12  0.2  0.0 2080632 11348 ?       Sl   Jun04  31:58 /usr/sbin/apache2 -D FOREGROUND\nwww-data    13  0.1  0.0 2080384 10980 ?       Sl   Jun04  18:12 /usr/sbin/apache2 -D FOREGROUND\nwww-data    68  0.3  0.0 349048 94272 ?        Sl   Jun04  47:09 hotapp\nwww-data   176  0.2  0.0 349624 92888 ?        Sl   Jun04  43:11 hotapp\nwww-data   179  0.2  0.0 349196 94456 ?        Sl   Jun04  42:20 hotapp\nwww-data   180  0.3  0.0 349828 95112 ?        Sl   Jun04  44:14 hotapp\nwww-data   185  0.3  0.0 346644 91948 ?        Sl   Jun04  43:49 hotapp\nwww-data   186  0.3  0.0 346208 91568 ?        Sl   Jun04  44:27 hotapp\nwww-data   189  0.2  0.0 350208 95476 ?        Sl   Jun04  41:47 hotapp\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;Docker容器统计API中的内存部分：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;curl --unix-socket /var/run/docker.sock &#39;http:/v1.24/containers/a45fc651e7b12f527b677e6a46e2902786bee6620484922016a135e317a42b4e/stats?stream=false&#39; | jq . # yields:\n\n&amp;quot;memory_stats&amp;quot;: {\n  &amp;quot;usage&amp;quot;: 5327712256,\n  &amp;quot;max_usage&amp;quot;: 5368344576,\n  &amp;quot;stats&amp;quot;: {\n    &amp;quot;active_anon&amp;quot;: 609095680,\n    &amp;quot;active_file&amp;quot;: 74457088,\n    &amp;quot;cache&amp;quot;: 109944832,\n    &amp;quot;dirty&amp;quot;: 28672,\n    &amp;quot;hierarchical_memory_limit&amp;quot;: 5368709120,\n    &amp;quot;inactive_anon&amp;quot;: 1687552,\n    &amp;quot;inactive_file&amp;quot;: 29974528,\n    &amp;quot;mapped_file&amp;quot;: 1675264,\n    &amp;quot;pgfault&amp;quot;: 295316278,\n    &amp;quot;pgmajfault&amp;quot;: 77,\n    &amp;quot;pgpgin&amp;quot;: 85138921,\n    &amp;quot;pgpgout&amp;quot;: 84964308,\n    &amp;quot;rss&amp;quot;: 605270016,\n    &amp;quot;rss_huge&amp;quot;: 0,\n    &amp;quot;shmem&amp;quot;: 5513216,\n    &amp;quot;total_active_anon&amp;quot;: 609095680,\n    &amp;quot;total_active_file&amp;quot;: 74457088,\n    &amp;quot;total_cache&amp;quot;: 109944832,\n    &amp;quot;total_dirty&amp;quot;: 28672,\n    &amp;quot;total_inactive_anon&amp;quot;: 1687552,\n    &amp;quot;total_inactive_file&amp;quot;: 29974528,\n    &amp;quot;total_mapped_file&amp;quot;: 1675264,\n    &amp;quot;total_pgfault&amp;quot;: 295316278,\n    &amp;quot;total_pgmajfault&amp;quot;: 77,\n    &amp;quot;total_pgpgin&amp;quot;: 85138921,\n    &amp;quot;total_pgpgout&amp;quot;: 84964308,\n    &amp;quot;total_rss&amp;quot;: 605270016,\n    &amp;quot;total_rss_huge&amp;quot;: 0,\n    &amp;quot;total_shmem&amp;quot;: 5513216,\n    &amp;quot;total_unevictable&amp;quot;: 0,\n    &amp;quot;total_writeback&amp;quot;: 0,\n    &amp;quot;unevictable&amp;quot;: 0,\n    &amp;quot;writeback&amp;quot;: 0\n  },\n  &amp;quot;limit&amp;quot;: 5368709120\n},\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;对断言的注释：&lt;br&gt;\n总计（memory.usage_in_bytes）=rss+缓存&lt;br&gt;\n说：&lt;br&gt;\n用法\\u字节：为了提高效率，与其他内核组件一样，内存组使用一些优化&lt;br&gt;\n避免不必要的缓存线错误共享。使用率受&lt;br&gt;\n方法，不显示内存（和交换）使用的“精确”值，这是一个模糊的&lt;br&gt;\n有效访问的值。（当然，必要时，它是同步的。）&lt;br&gt;\n如果您想知道更精确的内存使用情况，应该使用rss+cache（+swap）&lt;br&gt;\n内存中的值。stat（见5.2）。&lt;br&gt;\n说：&lt;br&gt;\n注意：在Linux上，Docker CLI通过从总内存使用量中减去页面缓存使用量来报告内存使用情况。API不执行这样的计算，而是提供总内存使用量和页面缓存的数量，以便客户机可以根据需要使用数据。&lt;br&gt;\n实际上，容器中/sys/fs/cgroup/memory/memory.stat中的大多数内容都出现在上面的docker stats api响应中（与在不同时间采集样本略有不同，抱歉）：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;meme@myapp-56b947bf6d-2lcr7:/app# cat /sys/fs/cgroup/memory/memory.stat\ncache 119492608\nrss 607436800\nrss_huge 0\nshmem 5525504\nmapped_file 1675264\ndirty 69632\nwriteback 0\npgpgin 85573974\npgpgout 85396501\npgfault 296366011\npgmajfault 80\ninactive_anon 1687552\nactive_anon 611213312\ninactive_file 32800768\nactive_file 81166336\nunevictable 0\nhierarchical_memory_limit 5368709120\ntotal_cache 119492608\ntotal_rss 607436800\ntotal_rss_huge 0\ntotal_shmem 5525504\ntotal_mapped_file 1675264\ntotal_dirty 69632\ntotal_writeback 0\ntotal_pgpgin 85573974\ntotal_pgpgout 85396501\ntotal_pgfault 296366011\ntotal_pgmajfault 80\ntotal_inactive_anon 1687552\ntotal_active_anon 611213312\ntotal_inactive_file 32800768\ntotal_active_file 81166336\ntotal_unevictable 0\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;内存信息来自：&lt;br&gt;\nLimits:&lt;br&gt;\nmemory:  5Gi&lt;br&gt;\nRequests:&lt;br&gt;\nmemory:   4Gi&lt;/p&gt;\n&lt;p&gt;下面是容器内的提示。在这一行程序中，我获取所有进程ID，在它们上运行pmap-x，并从pmap结果中提取kbytes列。总的结果是256兆字节（远小于PS的RSS，我认为部分原因是许多进程没有从PMAP-X返回输出）：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;ps aux | awk &#39;{print $2}&#39; | grep -v PID | xargs sudo pmap -x | grep total | grep -v grep | awk &#39;{print $3}&#39; | awk &#39;{s+=$1} END {printf &amp;quot;%.0f&amp;quot;, s}&#39;; echo\n256820\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;https://github.com/google/cadvisor/issues/638中提到了。它检查kubectl describe pod &lt;pod&gt;和pmap。这里没有照明（同样，它似乎忽略了一些过程）：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;# python ps_mem.py\nPrivate  +   Shared  =  RAM used    Program\n\n  1.7 MiB +   1.0 MiB =   2.7 MiB   apache2\n  2.0 MiB +   1.0 MiB =   3.0 MiB   bash (3)\n---------------------------------\n                          5.7 MiB\n=================================\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;最佳答案&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;有一件事我没看到您检查这里是内核内存。这在memory.usage_in_bytes图中也有说明，但在memory.stat中没有出现。您可以通过查看/sys/fs/cgroup/memory/memory.kmem.usage_in_bytes找到它。\n有一次，我看到我们的一个.NET核心应用程序也发生了类似的事情，但我不知道到底发生了什么（可能是.NET核心内存泄漏，因为它是我们的应用程序无法控制的非托管内存）。这将取决于您的应用程序使用是否正常，但就cgroups而言，我相信内核内存使用在默认情况下是不受约束的。\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;docker-nei-cun-shi-yong-lu-chai-yi-cgroup-memoryusage_in_bytes-yu-docker-rong-qi-nei-de-rss&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;docker - 内存使用率差异:cgroup memory.usage_in_bytes与docker容器内的RSS&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;docker&#34;,&#34;slug&#34;:&#34;h7T6l7ql6&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/h7T6l7ql6/&#34;},{&#34;name&#34;:&#34;memory&#34;,&#34;slug&#34;:&#34;svMa3rpx6&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/svMa3rpx6/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;kubernetes&#34;,&#34;slug&#34;:&#34;KWlALzyzR&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/KWlALzyzR/&#34;},{&#34;name&#34;:&#34;cgroups cadvisor&#34;,&#34;slug&#34;:&#34;0C44DhwtVY&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/0C44DhwtVY/&#34;}],&#34;date&#34;:&#34;2020-06-17 11:46:43&#34;,&#34;dateFormat&#34;:&#34;2020-06-17&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/docker-nei-cun-shi-yong-lu-chai-yi-cgroup-memoryusage_in_bytes-yu-docker-rong-qi-nei-de-rss/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;7 min read&#34;,&#34;time&#34;:396000,&#34;words&#34;:1364,&#34;minutes&#34;:7},&#34;description&#34;:&#34;&amp;quot;kubernetes&amp;quot;（v1.10.2）表示我的pod（包含一个容器）使用了大约5GB的内存。在容器内部，RSS说的更像681Mib。任何人都能用以下数据解释如何从681Mib到5GB吗（或者用我省略的另一个命令来描述...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;go-mod-使用\&#34;&gt;go mod 使用&lt;/h1&gt;\n&lt;p&gt;解决的问题是golang不再依赖gopath的设置，下载下来的包可以直接使用。&lt;br&gt;\ngo mod init ./&lt;br&gt;\ngo build main.go 或 go build -mod=vendor main.go&lt;br&gt;\ngo mod vendor #将包打到vendor文件夹下&lt;/p&gt;\n&lt;h1 id=\&#34;go-vendor\&#34;&gt;go vendor&lt;/h1&gt;\n&lt;p&gt;管理Golang项目依赖，应该是一个第三方的，但是比较好用。&lt;/p&gt;\n&lt;p&gt;安装&lt;br&gt;\ngo get -u github.com/kardianos/govendor&lt;/p&gt;\n&lt;p&gt;使用一套连招：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;govendor init # 创建vendor目录，创建vendor.json文件  \ngovendor add +external #生成依赖包  \ngovendor update +vendor # 更新vendor的包命令  \n状态\t缩写状态\t含义\n+local\tl\t本地包，即项目自身的包组织\n+external\te\t外部包，即被 $GOPATH 管理，但不在 vendor 目录下\n+vendor\tv\t已被 govendor 管理，即在 vendor 目录下\n+std\ts\t标准库中的包\n+unused\tu\t未使用的包，即包在 vendor 目录下，但项目并没有用到\n+missing\tm\t代码引用了依赖包，但该包并没有找到\n+program\tp\t主程序包，意味着可以编译为执行文件\n+outside\t \t外部包和缺失的包\n+all\t \t所有的包\n命令\t功能\ninit\t初始化 vendor 目录\nlist\t列出所有的依赖包\nadd\t添加包到 vendor 目录，如 govendor add +external 添加所有外部包\nadd PKG_PATH\t添加指定的依赖包到 vendor 目录\nupdate\t从 $GOPATH 更新依赖包到 vendor 目录\nremove\t从 vendor 管理中删除依赖\nstatus\t列出所有缺失、过期和修改过的包\nfetch\t添加或更新包到本地 vendor 目录\nsync\t本地存在 vendor.json 时候拉去依赖包，匹配所记录的版本\nget\t类似 go get 目录，拉取依赖包到 vendor 目录\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;golang-bi-ji-go-mod-yu-go-vendor&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;golang笔记-go mod与go vendor&#34;,&#34;tags&#34;:[],&#34;date&#34;:&#34;2020-06-10 16:40:20&#34;,&#34;dateFormat&#34;:&#34;2020-06-10&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/golang-bi-ji-go-mod-yu-go-vendor/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;2 min read&#34;,&#34;time&#34;:95000,&#34;words&#34;:399,&#34;minutes&#34;:2},&#34;description&#34;:&#34;go mod 使用\n解决的问题是golang不再依赖gopath的设置，下载下来的包可以直接使用。\ngo mod init ./\ngo build main.go 或 go build -mod=vendor main.go\ngo mod ...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#go-mod-%E4%BD%BF%E7%94%A8\&#34;&gt;go mod 使用&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#go-vendor\&#34;&gt;go vendor&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;p&gt;在beego中使用zap管理日志，很方便👍&lt;br&gt;\n需要在conf/app.conf定义几个参数&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;appname = web-terminal\nhttpport = &amp;quot;9600&amp;quot;\n\nrunmode = &amp;quot;prod&amp;quot;\nLogLevel = &amp;quot;info&amp;quot;\nautorender = true\nrecoverpanic = false\ncopyrequestbody = true\nviewspath = &amp;quot;static&amp;quot;\nLogPath = &amp;quot;logs/k8s-websocket.log&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;logger.go&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;package controllers\n\nimport (\n\t&amp;quot;os&amp;quot;\n\n\t&amp;quot;github.com/astaxie/beego&amp;quot;\n\t&amp;quot;github.com/natefinch/lumberjack&amp;quot;\n\t&amp;quot;go.uber.org/zap&amp;quot;\n\t&amp;quot;go.uber.org/zap/zapcore&amp;quot;\n)\n\n//MyLogger初始化zaplogger日志库\nvar MyLogger *zap.Logger\n\nfunc initLogger(logpath string, loglevel string) *zap.Logger {\n\n\t// 设置日志级别\n\tvar level zapcore.Level\n\tswitch loglevel {\n\tcase &amp;quot;debug&amp;quot;:\n\t\tlevel = zap.DebugLevel\n\tcase &amp;quot;info&amp;quot;:\n\t\tlevel = zap.InfoLevel\n\tcase &amp;quot;error&amp;quot;:\n\t\tlevel = zap.ErrorLevel\n\tdefault:\n\t\tlevel = zap.InfoLevel\n\t}\n\n\thook := lumberjack.Logger{\n\t\tFilename:   logpath, // 日志文件路径\n\t\tMaxSize:    20,      // 每个日志文件保存的最大尺寸 单位：M\n\t\tMaxBackups: 10,      // 日志文件最多保存多少个备份\n\t\tMaxAge:     7,       // 文件最多保存多少天\n\t\tCompress:   true,    // 是否压缩\n\t}\n\n\tencoderConfig := zapcore.EncoderConfig{\n\t\tTimeKey:        &amp;quot;time&amp;quot;,\n\t\tLevelKey:       &amp;quot;level&amp;quot;,\n\t\tNameKey:        &amp;quot;logger&amp;quot;,\n\t\tCallerKey:      &amp;quot;linenum&amp;quot;,\n\t\tMessageKey:     &amp;quot;msg&amp;quot;,\n\t\tStacktraceKey:  &amp;quot;stacktrace&amp;quot;,\n\t\tLineEnding:     zapcore.DefaultLineEnding,\n\t\tEncodeLevel:    zapcore.LowercaseLevelEncoder,  // 小写编码器\n\t\tEncodeTime:     zapcore.ISO8601TimeEncoder,     // ISO8601 UTC 时间格式\n\t\tEncodeDuration: zapcore.SecondsDurationEncoder, //\n\t\tEncodeCaller:   zapcore.ShortCallerEncoder,     // 短路径编码器\n\t\tEncodeName:     zapcore.FullNameEncoder,\n\t}\n\n\tcore := zapcore.NewCore(\n\t\tzapcore.NewJSONEncoder(encoderConfig),                                           // 编码器配置\n\t\tzapcore.NewMultiWriteSyncer(zapcore.AddSync(os.Stdout), zapcore.AddSync(&amp;amp;hook)), // 打印到控制台和文件\n\t\tlevel, // 日志级别\n\t)\n\n\t// 开启开发模式，堆栈跟踪\n\tcaller := zap.AddCaller()\n\t// 开启文件及行号\n\tdevelopment := zap.Development()\n\n\t// 设置初始化字段\n\tfiled := zap.Fields(zap.String(&amp;quot;serviceName&amp;quot;, &amp;quot;k8s-websocket-dashboard&amp;quot;))\n\t// 构造日志\n\tlogger := zap.New(core, caller, development, filed)\n\n\tlogger.Info(&amp;quot;initlog 初始化成功&amp;quot;)\n\treturn logger\n}\n\n// 初始化日志\nfunc init() {\n\tlogPath := beego.AppConfig.String(&amp;quot;LogPath&amp;quot;)\n\tloglevel := beego.AppConfig.String(&amp;quot;LogLevel&amp;quot;)\n\tMyLogger = initLogger(logPath, loglevel)\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;日志输出：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;{&amp;quot;level&amp;quot;:&amp;quot;info&amp;quot;,&amp;quot;time&amp;quot;:&amp;quot;2020-06-02T19:41:37.799+0800&amp;quot;,&amp;quot;linenum&amp;quot;:&amp;quot;controllers/logger.go:59&amp;quot;,&amp;quot;msg&amp;quot;:&amp;quot;log 初始化成功&amp;quot;,&amp;quot;serviceName&amp;quot;:&amp;quot;k8s-websocket-dashboard&amp;quot;}\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;golang-bi-ji-zip-ri-zhi-shi-yong&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;golang笔记-zip日志使用&#34;,&#34;tags&#34;:[{&#34;name&#34;:&#34;zap&#34;,&#34;slug&#34;:&#34;t0_qVBCTZ&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/t0_qVBCTZ/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;golang&#34;,&#34;slug&#34;:&#34;oHQV9cP9p&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/oHQV9cP9p/&#34;},{&#34;name&#34;:&#34;beego&#34;,&#34;slug&#34;:&#34;WCSltmB8m&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/WCSltmB8m/&#34;}],&#34;date&#34;:&#34;2020-06-03 14:17:23&#34;,&#34;dateFormat&#34;:&#34;2020-06-03&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/golang-bi-ji-zip-ri-zhi-shi-yong/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;3 min read&#34;,&#34;time&#34;:138000,&#34;words&#34;:444,&#34;minutes&#34;:3},&#34;description&#34;:&#34;在beego中使用zap管理日志，很方便👍\n需要在conf/app.conf定义几个参数\nappname = web-terminal\nhttpport = &amp;quot;9600&amp;quot;\n\nrunmode = &amp;quot;prod&amp;q...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;获取参数\&#34;&gt;获取参数&lt;/h1&gt;\n&lt;p&gt;我们经常需要获取用户传递的数据，包括Get、POST等方式的请求，beego里面会自动解析这些数据，你可以通过如下方式获取数据：&lt;/p&gt;\n&lt;p&gt;GetString(key string) string&lt;br&gt;\nGetStrings(key string) []string&lt;br&gt;\nGetInt(key string) (int64, error)&lt;br&gt;\nGetBool(key string) (bool, error)&lt;br&gt;\nGetFloat(key string) (float64, error)&lt;br&gt;\n示例1：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;func (this *MainController) Post() {\n    jsoninfo := &amp;lt;strong&amp;gt;this.GetString&amp;lt;/strong&amp;gt;(&amp;quot;jsoninfo&amp;quot;)\n    if jsoninfo == &amp;quot;&amp;quot; {\n        this.Ctx.WriteString(&amp;quot;jsoninfo is empty&amp;quot;)\n        return\n    }\n}```\n如果你需要的数据可能是其它类型，例如是int类型而不是int64，那么你需要这样处理：\n\n示例2：\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;func (this *MainController) Post() {&lt;br&gt;\nid := &lt;strong&gt;this.Input().Get&lt;/strong&gt;(&amp;quot;id&amp;quot;)&lt;br&gt;\nintid, err := &lt;strong&gt;strconv.Atoi&lt;/strong&gt;(id)&lt;br&gt;\n}```&lt;br&gt;\n更多其他的request的信息，用户可以通过this.Ctx.Request获取信息。&lt;/p&gt;\n&lt;p&gt;关于该对象的属性和方法可参考request官方手册https://gowalker.org/net/http#Request&lt;/p&gt;\n&lt;h1 id=\&#34;直接解析到struct\&#34;&gt;直接解析到struct&lt;/h1&gt;\n&lt;p&gt;如果要把表单里的内容赋值到一个struct里，除了用上面的方法一个一个获取再赋值之外，&lt;br&gt;\nbeego提供了通过另外一个更便捷的方式，就是通过struct的字段名或tag与表单字段对应直接解析到struct。&lt;/p&gt;\n&lt;p&gt;示例3：&lt;/p&gt;\n&lt;p&gt;定义struct&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;type user struct {\n    Id    int         `form:&amp;quot;-&amp;quot;`\n    Name  interface{} `form:&amp;quot;&amp;lt;strong&amp;gt;username&amp;lt;/strong&amp;gt;&amp;quot;`\n    Age   int         `form:&amp;quot;&amp;lt;strong&amp;gt;age&amp;lt;/strong&amp;gt;&amp;quot;`\n    Email string\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;表单&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;&amp;lt;form id=&amp;quot;&amp;lt;strong&amp;gt;user&amp;lt;/strong&amp;gt;&amp;quot;&amp;gt;\n    名字：&amp;lt;input name=&amp;quot;&amp;lt;strong&amp;gt;username&amp;lt;/strong&amp;gt;&amp;quot; type=&amp;quot;text&amp;quot; /&amp;gt;\n    年龄：&amp;lt;input name=&amp;quot;&amp;lt;strong&amp;gt;age&amp;lt;/strong&amp;gt;&amp;quot; type=&amp;quot;text&amp;quot; /&amp;gt;\n    邮箱：&amp;lt;input name=&amp;quot;Email&amp;quot; type=&amp;quot;text&amp;quot; /&amp;gt;\n    &amp;lt;input type=&amp;quot;submit&amp;quot; value=&amp;quot;提交&amp;quot; /&amp;gt;\n&amp;lt;/form&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;controller里解析&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;func (this *MainController) Post() {\n    u := &amp;lt;strong&amp;gt;user{}&amp;lt;/strong&amp;gt;\n    if err := this.ParseForm(&amp;lt;strong&amp;gt;&amp;amp;u&amp;lt;/strong&amp;gt;); err != nil {\n        //handle error\n    }\n}　　\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;需要说明的是：&lt;/p&gt;\n&lt;p&gt;（1）structTag form的定义和renderform方法共用一个标签。&lt;/p&gt;\n&lt;p&gt;（2）定义struct时，字段名后如果有form这个tag，则会把form表单里的name和tag的名字一样的字段赋值给这个字段，&lt;/p&gt;\n&lt;p&gt;否则就会把form表单里与字段名一样的表单内容赋值给这个字段。&lt;/p&gt;\n&lt;p&gt;例如上面的例子中，会把表单中的username和age分别赋值给user里的Name和Age字段，而Email里的内容则会赋值给Email这个字段。&lt;/p&gt;\n&lt;p&gt;（3）调用Controller PraseForm这个方法的时候，传入的参数必须为一个struct的指针，否则对struct的赋值不会成功并返回xx must be a struct pointer的错误。&lt;/p&gt;\n&lt;p&gt;（4）如果要忽略一个字段，有两种方法，一是：字段名小写开头，二是：form标签设置为_&lt;/p&gt;\n&lt;h1 id=\&#34;获取request-body里的内容\&#34;&gt;获取Request Body里的内容&lt;/h1&gt;\n&lt;p&gt;在API的开发中，我们经常会用到JSON或XML来作为数据交互的格式，如何在beego中获取Request Body里的JSON或XML的数据呢？&lt;/p&gt;\n&lt;p&gt;首先，在配置文件里设置copyrequestbody = true&lt;/p&gt;\n&lt;p&gt;其次，在Controller中：&lt;/p&gt;\n&lt;p&gt;示例4：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;func (this *ObjectController) Post() {\n    var ob models.Object\n    var err error\n    if err = json.Unmarshal(this.Ctx.Input.RequestBody, &amp;amp;ob); err == nil {\n        objectid := models.AddOne(ob)\n        this.Data[&amp;quot;json&amp;quot;] = &amp;quot;{\\&amp;quot;ObjectId\\&amp;quot;:\\&amp;quot;&amp;quot; + objectid + &amp;quot;\\&amp;quot;}&amp;quot;\n    } else {\n        this.Data[&amp;quot;json&amp;quot;] = err.Error()\n    }\n    this.ServeJSON()\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;&lt;/p&gt;\n&lt;h1 id=\&#34;文件上传\&#34;&gt;文件上传&lt;/h1&gt;\n&lt;p&gt;在beego中你可以很容易的处理文件上传，就是别忘记在你的form表单中增加“enctype=&amp;quot;multipart/form-data”，否则你的浏览器不会传输你的上传文件。&lt;/p&gt;\n&lt;p&gt;文件上传之后一般是放在系统的内存里面，如果文件的size大于设置的缓存大小，那么就放在临时文件中，&lt;/p&gt;\n&lt;p&gt;默认的缓存内存是64M，你可以通过如下方式来调整这个缓存内存的大小。&lt;/p&gt;\n&lt;p&gt;beego.MaxMemory = 1&amp;lt;&amp;lt;22&lt;br&gt;\n或者在配置文件中通过如下设置：&lt;/p&gt;\n&lt;p&gt;maxmemory = 1&amp;lt;&amp;lt;22&lt;br&gt;\nBeego提供了两个很方便的方法来处理文件上传：&lt;/p&gt;\n&lt;p&gt;（1）GetFile(key string) (multipart.File, *multipart.FileHeader, error)&lt;/p&gt;\n&lt;p&gt;该方法主要用于用户读取表单中的文件名the_file，然后返回相应的信息，用户根据这些变量来处理文件上传：过滤、保存文件等。&lt;/p&gt;\n&lt;p&gt;（2）SaveToFile(fromfile, tofile string) error&lt;/p&gt;\n&lt;p&gt;该方法是在GetFile的基础上实现了快速保存的功能，fromfile是提交的时候html表单中的name。&lt;/p&gt;\n&lt;p&gt;示例5&lt;/p&gt;\n&lt;p&gt;表单：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;&amp;lt;form enctype=&amp;quot;multipart/form-data&amp;quot; method=&amp;quot;post&amp;quot;&amp;gt;\n    &amp;lt;input type=&amp;quot;file&amp;quot; name=&amp;quot;uploadname&amp;quot; /&amp;gt;\n    &amp;lt;input type=&amp;quot;submit&amp;quot;&amp;gt;\n&amp;lt;/form&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;Controller中代码：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;func (c *FormController) Post() {\n    f, h, err := c.GetFile(&amp;quot;uploadname&amp;quot;)\n    if err != nil {\n        log.Fatal(&amp;quot;getfile err &amp;quot;, err)\n    }\n    defer f.Close()\n    c.SaveToFile(&amp;quot;uploadname&amp;quot;, &amp;quot;static/upload/&amp;quot; + h.Filename) // 保存位置在 static/upload, 没有文件夹要先创建\n     \n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;数据绑定\&#34;&gt;数据绑定&lt;/h1&gt;\n&lt;p&gt;支持从用户请求中直接数据bind到指定的对象。&lt;/p&gt;\n&lt;p&gt;示例6：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var id int\nthis.Ctx.Input.Bind(&amp;amp;id, &amp;quot;id&amp;quot;)  //id ==123\n \nvar isok bool\nthis.Ctx.Input.Bind(&amp;amp;isok, &amp;quot;isok&amp;quot;)  //isok ==true\n \nvar ft float64\nthis.Ctx.Input.Bind(&amp;amp;ft, &amp;quot;ft&amp;quot;)  //ft ==1.2\n \nol := make([]int, 0, 2)\nthis.Ctx.Input.Bind(&amp;amp;ol, &amp;quot;ol&amp;quot;)  //ol ==[1 2]\n \nul := make([]string, 0, 2)\nthis.Ctx.Input.Bind(&amp;amp;ul, &amp;quot;ul&amp;quot;)  //ul ==[str array]\n \nuser struct{Name}\nthis.Ctx.Input.Bind(&amp;amp;user, &amp;quot;user&amp;quot;)  //user =={Name:&amp;quot;astaxie&amp;quot;}\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;golang-bi-ji-beego-huo-qu-url-qing-qiu-de-can-shu&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;golang笔记-beego获取url请求的参数&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;golang&#34;,&#34;slug&#34;:&#34;oHQV9cP9p&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/oHQV9cP9p/&#34;},{&#34;name&#34;:&#34;beego&#34;,&#34;slug&#34;:&#34;WCSltmB8m&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/WCSltmB8m/&#34;}],&#34;date&#34;:&#34;2020-06-02 12:00:50&#34;,&#34;dateFormat&#34;:&#34;2020-06-02&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/golang-bi-ji-beego-huo-qu-url-qing-qiu-de-can-shu/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;6 min read&#34;,&#34;time&#34;:340000,&#34;words&#34;:1252,&#34;minutes&#34;:6},&#34;description&#34;:&#34;获取参数\n我们经常需要获取用户传递的数据，包括Get、POST等方式的请求，beego里面会自动解析这些数据，你可以通过如下方式获取数据：\nGetString(key string) string\nGetStrings(key string...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E8%8E%B7%E5%8F%96%E5%8F%82%E6%95%B0\&#34;&gt;获取参数&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%9B%B4%E6%8E%A5%E8%A7%A3%E6%9E%90%E5%88%B0struct\&#34;&gt;直接解析到struct&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E8%8E%B7%E5%8F%96request-body%E9%87%8C%E7%9A%84%E5%86%85%E5%AE%B9\&#34;&gt;获取Request Body里的内容&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0\&#34;&gt;文件上传&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%95%B0%E6%8D%AE%E7%BB%91%E5%AE%9A\&#34;&gt;数据绑定&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;简介\&#34;&gt;简介&lt;/h1&gt;\n&lt;p&gt;Json(Javascript Object Nanotation)是一种数据交换格式，常用于前后端数据传输。任意一端将数据转换成json 字符串，另一端再将该字符串解析成相应的数据结构，如string类型，strcut对象等。&lt;/p&gt;\n&lt;p&gt;go语言本身为我们提供了json的工具包”encoding/json”。&lt;br&gt;\n更多的使用方式，可以参考：https://studygolang.com/articles/6742&lt;/p&gt;\n&lt;h1 id=\&#34;实现\&#34;&gt;实现&lt;/h1&gt;\n&lt;p&gt;Json Marshal：将数据编码成json字符串&lt;br&gt;\n看一个简单的例子&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;type Stu struct {\n    Name  string `json:&amp;quot;name&amp;quot;`\n    Age   int\n    HIgh  bool\n    sex   string\n    Class *Class `json:&amp;quot;class&amp;quot;`\n}\n\ntype Class struct {\n    Name  string\n    Grade int\n}\n\nfunc main() {\n    //实例化一个数据结构，用于生成json字符串\n    stu := Stu{\n        Name: &amp;quot;张三&amp;quot;,\n        Age:  18,\n        HIgh: true,\n        sex:  &amp;quot;男&amp;quot;,\n    }\n\n    //指针变量\n    cla := new(Class)\n    cla.Name = &amp;quot;1班&amp;quot;\n    cla.Grade = 3\n    stu.Class=cla\n\n    //Marshal失败时err!=nil\n    jsonStu, err := json.Marshal(stu)\n    if err != nil {\n        fmt.Println(&amp;quot;生成json字符串错误&amp;quot;)\n    }\n\n    //jsonStu是[]byte类型，转化成string类型便于查看\n    fmt.Println(string(jsonStu))\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;结果：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;{&amp;quot;name&amp;quot;:&amp;quot;张三&amp;quot;,&amp;quot;Age&amp;quot;:18,&amp;quot;HIgh&amp;quot;:true,&amp;quot;class&amp;quot;:{&amp;quot;Name&amp;quot;:&amp;quot;1班&amp;quot;,&amp;quot;Grade&amp;quot;:3}}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;从结果中可以看出&lt;/p&gt;\n&lt;p&gt;只要是可导出成员（变量首字母大写），都可以转成json。因成员变量sex是不可导出的，故无法转成json。&lt;/p&gt;\n&lt;p&gt;如果变量打上了json标签，如Name旁边的 &lt;code&gt;json:&amp;quot;name&amp;quot;&lt;/code&gt; ，那么转化成的json key就用该标签“name”，否则取变量名作为key，如“Age”，“HIgh”。&lt;/p&gt;\n&lt;p&gt;bool类型也是可以直接转换为json的value值。Channel， complex 以及函数不能被编码json字符串。当然，循环的数据结构也不行，它会导致marshal陷入死循环。&lt;/p&gt;\n&lt;p&gt;指针变量，编码时自动转换为它所指向的值，如cla变量。&lt;br&gt;\n（当然，不传指针，Stu struct的成员Class如果换成Class struct类型，效果也是一模一样的。只不过指针更快，且能节省内存空间。）&lt;/p&gt;\n&lt;p&gt;最后，强调一句：json编码成字符串后就是纯粹的字符串了。&lt;/p&gt;\n&lt;p&gt;上面的成员变量都是已知的类型，只能接收指定的类型，比如string类型的Name只能赋值string类型的数据。&lt;br&gt;\n但有时为了通用性，或使代码简洁，我们希望有一种类型可以接受各种类型的数据，并进行json编码。这就用到了interface{}类型。&lt;/p&gt;\n&lt;p&gt;前言：&lt;br&gt;\ninterface{}类型其实是个空接口，即没有方法的接口。go的每一种类型都实现了该接口。因此，任何其他类型的数据都可以赋值给interface{}类型。&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;type Stu struct {\n    Name  interface{} `json:&amp;quot;name&amp;quot;`\n    Age   interface{}\n    HIgh  interface{}\n    sex   interface{}\n    Class interface{} `json:&amp;quot;class&amp;quot;`\n}\n\ntype Class struct {\n    Name  string\n    Grade int\n}\n\nfunc main() {\n    //与前面的例子一样\n    ......\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;结果：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;{&amp;quot;name&amp;quot;:&amp;quot;张三&amp;quot;,&amp;quot;Age&amp;quot;:18,&amp;quot;HIgh&amp;quot;:true,&amp;quot;class&amp;quot;:{&amp;quot;Name&amp;quot;:&amp;quot;1班&amp;quot;,&amp;quot;Grade&amp;quot;:3}}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;从结果中可以看出，无论是string，int，bool，还是指针类型等，都可赋值给interface{}类型，且正常编码，效果与前面的例子一样。&lt;/p&gt;\n&lt;p&gt;补充：&lt;br&gt;\n在实际项目中，编码成json串的数据结构，往往是切片类型。如下定义了一个[]StuRead类型的切片&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;//正确示范\n\n//方式1：只声明，不分配内存\nvar stus1 []*StuRead\n\n//方式2：分配初始值为0的内存\nstus2 := make([]*StuRead,0)\n\n//错误示范\n//new()只能实例化一个struct对象，而[]StuRead是切片，不是对象\nstus := new([]StuRead)\n\nstu1 := StuRead{成员赋值...}\nstu2 := StuRead{成员赋值...}\n\n//由方式1和2创建的切片，都能成功追加数据\n//方式2最好分配0长度，append时会自动增长。反之指定初始长度，长度不够时不会自动增长，导致数据丢失\nstus1 := appen(stus1,stu1,stu2)\nstus2 := appen(stus2,stu1,stu2)\n\n//成功编码\njson1,_ := json.Marshal(stus1)\njson2,_ := json.Marshal(stus2)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;解码时定义对应的切片接受即可&lt;/p&gt;\n&lt;p&gt;Json Unmarshal：将json字符串解码到相应的数据结构&lt;br&gt;\n我们将上面的例子进行解码&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;type StuRead struct {\n    Name  interface{} `json:&amp;quot;name&amp;quot;`\n    Age   interface{}\n    HIgh  interface{}\n    sex   interface{}\n    Class interface{} `json:&amp;quot;class&amp;quot;`\n    Test  interface{}\n}\n\ntype Class struct {\n    Name  string\n    Grade int\n}\n\nfunc main() {\n    //json字符中的&amp;quot;引号，需用\\进行转义，否则编译出错\n    //json字符串沿用上面的结果，但对key进行了大小的修改，并添加了sex数据\n    data:=&amp;quot;{\\&amp;quot;name\\&amp;quot;:\\&amp;quot;张三\\&amp;quot;,\\&amp;quot;Age\\&amp;quot;:18,\\&amp;quot;high\\&amp;quot;:true,\\&amp;quot;sex\\&amp;quot;:\\&amp;quot;男\\&amp;quot;,\\&amp;quot;CLASS\\&amp;quot;:{\\&amp;quot;naME\\&amp;quot;:\\&amp;quot;1班\\&amp;quot;,\\&amp;quot;GradE\\&amp;quot;:3}}&amp;quot;\n    str:=[]byte(data)\n\n    //1.Unmarshal的第一个参数是json字符串，第二个参数是接受json解析的数据结构。\n    //第二个参数必须是指针，否则无法接收解析的数据，如stu仍为空对象StuRead{}\n    //2.可以直接stu:=new(StuRead),此时的stu自身就是指针\n    stu:=StuRead{}\n    err:=json.Unmarshal(str,&amp;amp;stu)\n\n    //解析失败会报错，如json字符串格式不对，缺&amp;quot;号，缺}等。\n    if err!=nil{\n        fmt.Println(err)\n    }\n\n    fmt.Println(stu)\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;结果：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;{张三 18 true &amp;lt;nil&amp;gt; map[naME:1班 GradE:3] &amp;lt;nil&amp;gt;}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;总结：&lt;/p&gt;\n&lt;p&gt;json字符串解析时，需要一个“接收体”接受解析后的数据，且Unmarshal时接收体必须传递指针。否则解析虽不报错，但数据无法赋值到接受体中。如这里用的是StuRead{}接收。&lt;/p&gt;\n&lt;p&gt;解析时，接收体可自行定义。json串中的key自动在接收体中寻找匹配的项进行赋值。匹配规则是：&lt;/p&gt;\n&lt;p&gt;先查找与key一样的json标签，找到则赋值给该标签对应的变量(如Name)。&lt;br&gt;\n没有json标签的，就从上往下依次查找变量名与key一样的变量，如Age。或者变量名忽略大小写后与key一样的变量。如HIgh，Class。第一个匹配的就赋值，后面就算有匹配的也忽略。&lt;br&gt;\n(前提是该变量必需是可导出的，即首字母大写)。&lt;br&gt;\n不可导出的变量无法被解析（如sex变量，虽然json串中有key为sex的k-v，解析后其值仍为nil,即空值）&lt;/p&gt;\n&lt;p&gt;当接收体中存在json串中匹配不了的项时，解析会自动忽略该项，该项仍保留原值。如变量Test，保留空值nil。&lt;/p&gt;\n&lt;p&gt;你一定会发现，变量Class貌似没有解析为我们期待样子。因为此时的Class是个interface{}类型的变量，而json串中key为CLASS的value是个复合结构，不是可以直接解析的简单类型数据（如“张三”，18，true等）。所以解析时，由于没有指定变量Class的具体类型，json自动将value为复合结构的数据解析为map[string]interface{}类型的项。也就是说，此时的struct Class对象与StuRead中的Class变量没有半毛钱关系，故与这次的json解析没有半毛钱关系。&lt;/p&gt;\n&lt;p&gt;让我们看一下这几个interface{}变量解析后的类型&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;func main() {\n    //与前边json解析的代码一致\n    ...\n    fmt.Println(stu) //打印json解析前变量类型\n    err:=json.Unmarshal(str,&amp;amp;stu)\n    fmt.Println(&amp;quot;--------------json 解析后-----------&amp;quot;)\n    ... \n    fmt.Println(stu) //打印json解析后变量类型    \n}\n\n//利用反射，打印变量类型\nfunc printType(stu *StuRead){\n    nameType:=reflect.TypeOf(stu.Name)\n    ageType:=reflect.TypeOf(stu.Age)\n    highType:=reflect.TypeOf(stu.HIgh)\n    sexType:=reflect.TypeOf(stu.sex)\n    classType:=reflect.TypeOf(stu.Class)\n    testType:=reflect.TypeOf(stu.Test)\n\n    fmt.Println(&amp;quot;nameType:&amp;quot;,nameType)\n    fmt.Println(&amp;quot;ageType:&amp;quot;,ageType)\n    fmt.Println(&amp;quot;highType:&amp;quot;,highType)\n    fmt.Println(&amp;quot;sexType:&amp;quot;,sexType)\n    fmt.Println(&amp;quot;classType:&amp;quot;,classType)\n    fmt.Println(&amp;quot;testType:&amp;quot;,testType)\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;结果：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;nameType: &amp;lt;nil&amp;gt;\nageType: &amp;lt;nil&amp;gt;\nhighType: &amp;lt;nil&amp;gt;\nsexType: &amp;lt;nil&amp;gt;\nclassType: &amp;lt;nil&amp;gt;\ntestType: &amp;lt;nil&amp;gt;\n--------------json 解析后-----------\nnameType: string\nageType: float64\nhighType: bool\nsexType: &amp;lt;nil&amp;gt;\nclassType: map[string]interface {}\ntestType: &amp;lt;nil&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;从结果中可见&lt;/p&gt;\n&lt;p&gt;interface{}类型变量在json解析前，打印出的类型都为nil，就是没有具体类型，这是空接口（interface{}类型）的特点。&lt;/p&gt;\n&lt;p&gt;json解析后，json串中value，只要是”简单数据”，都会按照默认的类型赋值，如”张三”被赋值成string类型到Name变量中，数字18对应float64，true对应bool类型。&lt;/p&gt;\n&lt;p&gt;“简单数据”：是指不能再进行二次json解析的数据，如”name”:”张三”只能进行一次json解析。&lt;br&gt;\n“复合数据”：类似”CLASS\\”:{\\”naME\\”:\\”1班\\”,\\”GradE\\”:3}这样的数据，是可进行二次甚至多次json解析的，因为它的value也是个可被解析的独立json。即第一次解析key为CLASS的value，第二次解析value中的key为naME和GradE的value&lt;/p&gt;\n&lt;p&gt;对于”复合数据”，如果接收体中配的项被声明为interface{}类型，go都会默认解析成map[string]interface{}类型。如果我们想直接解析到struct Class对象中，可以将接受体对应的项定义为该struct类型。如下所示：&lt;/p&gt;\n&lt;p&gt;type StuRead struct {&lt;br&gt;\n...&lt;br&gt;\n//普通struct类型&lt;br&gt;\nClass Class &lt;code&gt;json:&amp;quot;class&amp;quot;&lt;/code&gt;&lt;br&gt;\n//指针类型&lt;br&gt;\nClass *Class &lt;code&gt;json:&amp;quot;class&amp;quot;&lt;/code&gt;&lt;br&gt;\n}&lt;/p&gt;\n&lt;p&gt;stu打印结果&lt;/p&gt;\n&lt;p&gt;Class类型：{张三 18 true &lt;nil&gt; {1班 3} &lt;nil&gt;}&lt;br&gt;\n*Class类型：{张三 18 true &lt;nil&gt; 0xc42008a0c0 &lt;nil&gt;}&lt;/p&gt;\n&lt;p&gt;可以看出，传递Class类型的指针时，stu中的Class变量存的是指针，我们可通过该指针直接访问所属的数据，如stu.Class.Name/stu.Class.Grade&lt;/p&gt;\n&lt;p&gt;Class变量解析后类型&lt;/p&gt;\n&lt;p&gt;classType: main.Class&lt;br&gt;\nclassType: *main.Class&lt;br&gt;\n解析时，如果接受体中同时存在2个匹配的项，会发生什么呢？&lt;br&gt;\n测试1&lt;/p&gt;\n&lt;p&gt;type StuRead struct {&lt;br&gt;\nNAme interface{}&lt;br&gt;\nName  interface{}&lt;br&gt;\nNAMe interface{}    &lt;code&gt;json:&amp;quot;name&amp;quot;&lt;/code&gt;&lt;br&gt;\n}&lt;br&gt;\n结果1:&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;//当存在匹配的json标签时，其对应的项被赋值。\n//切记：匹配的标签可以没有，但有时最好只有一个哦\n{&amp;lt;nil&amp;gt; &amp;lt;nil&amp;gt; 张三}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;测试2&lt;/p&gt;\n&lt;p&gt;type StuRead struct {&lt;br&gt;\nNAme interface{}&lt;br&gt;\nName  interface{}&lt;br&gt;\nNAMe interface{}    &lt;code&gt;json:&amp;quot;name&amp;quot;&lt;/code&gt;&lt;br&gt;\nNamE interface{}    &lt;code&gt;json:&amp;quot;name&amp;quot;&lt;/code&gt;&lt;br&gt;\n}&lt;br&gt;\n结果2&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;//当匹配的json标签有多个时，标签对应的项都不会被赋值。\n//忽略标签项，从上往下寻找第一个没有标签且匹配的项赋值\n{张三 &amp;lt;nil&amp;gt; &amp;lt;nil&amp;gt; &amp;lt;nil&amp;gt;}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;测试3&lt;/p&gt;\n&lt;p&gt;type StuRead struct {&lt;br&gt;\nNAme interface{}&lt;br&gt;\nName  interface{}&lt;br&gt;\n}&lt;br&gt;\n结果3&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;//没有json标签时，从上往下，第一个匹配的项会被赋值哦\n{张三 &amp;lt;nil&amp;gt;}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;测试4&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;type StuRead struct {\n    NAMe interface{}    `json:&amp;quot;name&amp;quot;`\n    NamE interface{}    `json:&amp;quot;name&amp;quot;`\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;结果4&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;//当相同的json标签有多个，且没有不带标签的匹配项时，报错了哦\n# command-line-arguments\nsrc/test/b.go:48: stu.Name undefined (type *StuRead has no field or method Name, but does have NAMe)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;可见，与前边说过的匹配规则是一致的。&lt;/p&gt;\n&lt;p&gt;如果不想指定Class变量为具体的类型，仍想保留interface{}类型，但又希望该变量可以解析到struct Class对象中，这时候该怎么办呢？&lt;/p&gt;\n&lt;p&gt;这种需求是很可能存在的，例如笔者我就碰到了&lt;/p&gt;\n&lt;p&gt;办法还是有的，我们可以将该变量定义为json.RawMessage类型&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;type StuRead struct {\n    Name  interface{}\n    Age   interface{}\n    HIgh  interface{}\n    Class json.RawMessage `json:&amp;quot;class&amp;quot;` //注意这里\n}\n\ntype Class struct {\n    Name  string\n    Grade int\n}\n\nfunc main() {\n    data:=&amp;quot;{\\&amp;quot;name\\&amp;quot;:\\&amp;quot;张三\\&amp;quot;,\\&amp;quot;Age\\&amp;quot;:18,\\&amp;quot;high\\&amp;quot;:true,\\&amp;quot;sex\\&amp;quot;:\\&amp;quot;男\\&amp;quot;,\\&amp;quot;CLASS\\&amp;quot;:{\\&amp;quot;naME\\&amp;quot;:\\&amp;quot;1班\\&amp;quot;,\\&amp;quot;GradE\\&amp;quot;:3}}&amp;quot;\n    str:=[]byte(data)\n    stu:=StuRead{}\n    _:=json.Unmarshal(str,&amp;amp;stu)\n\n    //注意这里：二次解析！\n    cla:=new(Class)\n    json.Unmarshal(stu.Class,cla)\n\n    fmt.Println(&amp;quot;stu:&amp;quot;,stu)\n    fmt.Println(&amp;quot;string(stu.Class):&amp;quot;,string(stu.Class))\n    fmt.Println(&amp;quot;class:&amp;quot;,cla)\n    printType(&amp;amp;stu) //函数实现前面例子有\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;结果&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;stu: {张三 18 true [123 34 110 97 77 69 34 58 34 49 231 143 173 34 44 34 71 114 97 100 69 34 58 51 125]}\nstring(stu.Class): {&amp;quot;naME&amp;quot;:&amp;quot;1班&amp;quot;,&amp;quot;GradE&amp;quot;:3}\nclass: &amp;amp;{1班 3}\nnameType: string\nageType: float64\nhighType: bool\nclassType: json.RawMessage\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;从结果中可见&lt;/p&gt;\n&lt;p&gt;接收体中，被声明为json.RawMessage类型的变量在json解析时，变量值仍保留json的原值，即未被自动解析为map[string]interface{}类型。如变量Class解析后的值为：{“naME”:”1班”,”GradE”:3}&lt;/p&gt;\n&lt;p&gt;从打印的类型也可以看出，在第一次json解析时，变量Class的类型是json.RawMessage。此时，我们可以对该变量进行二次json解析，因为其值仍是个独立且可解析的完整json串。我们只需再定义一个新的接受体即可，如json.Unmarshal(stu.Class,cla)&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;golang-bi-ji-json-shu-ju-jie-xi-marshal-yu-unmarshal&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;golang笔记-json数据解析：Marshal与Unmarshal&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;golang&#34;,&#34;slug&#34;:&#34;oHQV9cP9p&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/oHQV9cP9p/&#34;}],&#34;date&#34;:&#34;2020-05-28 18:15:31&#34;,&#34;dateFormat&#34;:&#34;2020-05-28&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/golang-bi-ji-json-shu-ju-jie-xi-marshal-yu-unmarshal/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;13 min read&#34;,&#34;time&#34;:754000,&#34;words&#34;:2981,&#34;minutes&#34;:13},&#34;description&#34;:&#34;简介\nJson(Javascript Object Nanotation)是一种数据交换格式，常用于前后端数据传输。任意一端将数据转换成json 字符串，另一端再将该字符串解析成相应的数据结构，如string类型，strcut对象等。\ngo...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%AE%80%E4%BB%8B\&#34;&gt;简介&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%AE%9E%E7%8E%B0\&#34;&gt;实现&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;数组\&#34;&gt;数组&lt;/h1&gt;\n&lt;p&gt;数组的长度一旦定义了就不能动态增长，并且存储的数据类型必须相同。&lt;/p&gt;\n&lt;h2 id=\&#34;创建方法\&#34;&gt;创建方法：&lt;/h2&gt;\n&lt;p&gt;var 数组名 [长度]数据类型&lt;br&gt;\n例如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;package main\nimport &amp;quot;fmt&amp;quot;\n \nfunc main(){\n    var test [5]int //定义数组名字test，长度为5，数据类型为int的数组\n    test[0] = 1    //赋值\n    test[1] = 2   \n    test[2] = 3\n    test[3] = 4\n    fmt.Println(test) \n    fmt.Println(test[2])\n    fmt.Println(test[1:3]) //输出1到3的数组\n    fmt.Println(test[0:]) //0到结尾\n    fmt.Println(test[:3])  //0到3\n \n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;##结果##\n[1 2 3 4 0]\n3\n[2 3]\n[1 2 3 4 0]\n[1 2 3]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;数组的四种初始化方式\&#34;&gt;数组的四种初始化方式&lt;/h2&gt;\n&lt;p&gt;例如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var s1 [3]int = [3]int{1,2,3}\nfmt.Println(&amp;quot;s1&amp;quot;,s1)\nvar s2 [4]int = [...]int{5,6,7,8} //[...]是固定写法\nfmt.Println(&amp;quot;s2&amp;quot;,s2)\nvar s3 = [2]int{9,10} //第一种的简化\nfmt.Println(&amp;quot;s3&amp;quot;,s3)\nvar s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对\nfmt.Println(&amp;quot;s4&amp;quot;,s4)\nvar s5 = new([5]int)\ns5[4] =12\nfmt.Println(&amp;quot;s5&amp;quot;,s5)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;##结果##\ns1 [1 2 3]\ns2 [5 6 7 8]\ns3 [9 10]\ns4 [40 41 42 43]\ns5 [0 0 0 0 5]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;数组的遍历\&#34;&gt;数组的遍历&lt;/h2&gt;\n&lt;p&gt;例如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对\nfmt.Println(&amp;quot;s4&amp;quot;,s4)\n     \nfor index,value := range s4{\nfmt.Println(index,value)\n}\n \n#结果##\n0 40\n1 41\n2 42\n3 43\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;var s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对\nfor i := 0;i &amp;lt;len(s4);i++{\nfmt.Println(i,s4[i])\n}\n \n#结果##\n0 40\n1 41\n2 42\n3 43\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;slice切片\&#34;&gt;slice切片&lt;/h1&gt;\n&lt;p&gt;1、切片是数组的引用(切片是数组的一部分)&lt;br&gt;\n2、切片的使用类似数组，如遍历&lt;br&gt;\n3、切片的长度是可变的&lt;/p&gt;\n&lt;h2 id=\&#34;创建语法\&#34;&gt;创建语法&lt;/h2&gt;\n&lt;p&gt;var 切片名 []类型&lt;br&gt;\n如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var qiepian []int\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;切片示例\&#34;&gt;切片示例:&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;###例子一&amp;lt;br&amp;gt;var suzhu [4]int = [...]int{5,6,7,8}\nslice := suzhu[1:4] //1到4的值，不包含4\nfmt.Println(suzhu)\nfmt.Println(slice)\nfmt.Println(&amp;quot;切片的容量&amp;quot;,cap(slice))\n \n##结果\n[5 6 7 8]\n[6 7 8]\n切片的容量 3&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;\n###例子二、使用make创建切片\nvar slice []int = make([]int,4,10) //类型，大小(长度),容量（可选），容量必须大于长度\nslice[0] = 10\nslice[1] = 11\nfmt.Println(slice)\n \n##结果##\n[10 11 0 0]\n \n \n###例子三\nvar slice []int = []int {2,4,6}\nfmt.Println(slice)\n \n##结果##\n2 4 6\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;切片的append追加\&#34;&gt;切片的append追加&lt;/h2&gt;\n&lt;p&gt;例如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var slice []int = []int {2,4,6}\nfmt.Println(slice)\n//使用append直接追加切片内容（类似python list的append）\nslice = append(slice,8,10)\nfmt.Println(slice)\nslice = append(slice,slice...) //追加切片，...是固定写法\nfmt.Println(slice)\n \n###结果###\n[2 4 6]\n[2 4 6 8 10]\n[2 4 6 8 10 2 4 6 8 10]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;切片的copy操作\&#34;&gt;切片的copy操作&lt;/h2&gt;\n&lt;p&gt;使用copy内置函数&lt;br&gt;\n例如:&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var slice []int = []int {2,4,6}\nfmt.Println(slice)\nvar slice2 []int = make([]int,5)\nfmt.Println(slice2)\ncopy(slice2,slice) //将slice复制给slice2\nfmt.Println(slice)\nfmt.Println(slice2)\n \n##结果##\n[2 4 6]\n[0 0 0 0 0]\n[2 4 6]\n[2 4 6 0 0]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;使用切片改变字符串的内容\&#34;&gt;使用切片改变字符串的内容&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;var str string = &amp;quot;hello&amp;quot;\nfmt.Println(str)\narr := []byte(str)\narr[1] = &#39;a&#39; //转成字符串\narr1 := []rune(str) //中文转换\narr1[0] = &#39;狗&#39;\nfmt.Println(arr)\nstr = string(arr)\nfmt.Println(str)\nstr = string(arr1)\nfmt.Println(str)\n \n##结果##\nhello\n[104 97 108 108 111]\nhello\n狗hello\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;map\&#34;&gt;map&lt;/h1&gt;\n&lt;p&gt;map是key-value数据结构(类似python的dict)&lt;br&gt;\nmap是无序存储的&lt;/p&gt;\n&lt;p&gt;创建map语法&lt;br&gt;\nvar map 变量名 map[keytype]valuetype&lt;/p&gt;\n&lt;p&gt;如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var m1 map[string]string\nvar m2 map[string]int\nvar m3 map[int]string\nvar m4 map[string]map[string]string\n```　　\n\n使用例子：\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;package main&lt;br&gt;\nimport &amp;quot;fmt&amp;quot;&lt;/p&gt;\n&lt;p&gt;func main(){&lt;br&gt;\nvar m1 map[string]string&lt;br&gt;\n//在使用map前,需要先make，make的作用技术给map分配数据空间&lt;br&gt;\nm1 = make(map[string]string)&lt;br&gt;\nm2 := map[string]string{  //使用方式二&lt;br&gt;\n&amp;quot;a1&amp;quot; : &amp;quot;q1&amp;quot;,&lt;br&gt;\n&amp;quot;a2&amp;quot; : &amp;quot;a2&amp;quot;,&lt;br&gt;\n}&lt;br&gt;\nm1[&amp;quot;s1&amp;quot;] = &amp;quot;亚索&amp;quot;&lt;br&gt;\nm1[&amp;quot;s2&amp;quot;] = &amp;quot;盖伦&amp;quot;&lt;br&gt;\nfmt.Println(m1)&lt;br&gt;\nfmt.Println(m1[&amp;quot;s1&amp;quot;])&lt;br&gt;\nfmt.Println(m2)&lt;br&gt;\n}&lt;/p&gt;\n&lt;p&gt;###结果###&lt;br&gt;\nmap[s1:亚索 s2:盖伦]&lt;br&gt;\n亚索&lt;br&gt;\nmap[a1:q1 a2:a2]&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;\n \n\nmap的增删改查\n增、改\nmap[key] = value //没有就增加，存在就修改\n\n删\ndelete(map,key)\n\n查\nmap[key]   //对应的value，和python的dict一样&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;golang-bi-ji-shu-zu-qie-pian-map&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;golang笔记-数组、切片、map&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;golang&#34;,&#34;slug&#34;:&#34;oHQV9cP9p&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/oHQV9cP9p/&#34;}],&#34;date&#34;:&#34;2020-05-26 13:29:04&#34;,&#34;dateFormat&#34;:&#34;2020-05-26&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/golang-bi-ji-shu-zu-qie-pian-map/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;5 min read&#34;,&#34;time&#34;:292000,&#34;words&#34;:966,&#34;minutes&#34;:5},&#34;description&#34;:&#34;数组\n数组的长度一旦定义了就不能动态增长，并且存储的数据类型必须相同。\n创建方法：\nvar 数组名 [长度]数据类型\n例如：\npackage main\nimport &amp;quot;fmt&amp;quot;\n \nfunc main(){\n    va...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%95%B0%E7%BB%84\&#34;&gt;数组&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%9B%E5%BB%BA%E6%96%B9%E6%B3%95\&#34;&gt;创建方法：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%95%B0%E7%BB%84%E7%9A%84%E5%9B%9B%E7%A7%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E5%BC%8F\&#34;&gt;数组的四种初始化方式&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%95%B0%E7%BB%84%E7%9A%84%E9%81%8D%E5%8E%86\&#34;&gt;数组的遍历&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#slice%E5%88%87%E7%89%87\&#34;&gt;slice切片&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%9B%E5%BB%BA%E8%AF%AD%E6%B3%95\&#34;&gt;创建语法&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%87%E7%89%87%E7%A4%BA%E4%BE%8B\&#34;&gt;切片示例:&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%87%E7%89%87%E7%9A%84append%E8%BF%BD%E5%8A%A0\&#34;&gt;切片的append追加&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%87%E7%89%87%E7%9A%84copy%E6%93%8D%E4%BD%9C\&#34;&gt;切片的copy操作&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E4%BD%BF%E7%94%A8%E5%88%87%E7%89%87%E6%94%B9%E5%8F%98%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%86%85%E5%AE%B9\&#34;&gt;使用切片改变字符串的内容&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#map\&#34;&gt;map&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;pre&gt;&lt;code&gt;HPA全称Horizontal Pod Autoscaling，是K8s实现pod自动水平扩容缩容的特性，这个特性使整个kubernetes集群马上高大上起来了。\n要使用HPA也不是这么简单的，HPA api分v1、v2beta1、v2bate2三种，v1只支持通过CPU衡量扩缩容，v2bate1加入针对内存作为度量，v2bate2可以用customer metrics例如网络等，所以v2bate1开始才比较实用。\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;要使用HPA必须要开启以下两个特性：&lt;/p&gt;\n&lt;p&gt;Aggregation Layer 聚合层，通过与核心的apiserver分离，实现自定义的扩展功能&lt;br&gt;\nmetrics-server 数据收集，能够收集pod、node等实时运行指标（cpu、内存），给k8s集群使用，例如kubectl top命令、HPA&lt;br&gt;\n比较老的版本使用heapster&lt;/p&gt;\n&lt;h1 id=\&#34;aggregation-layer\&#34;&gt;Aggregation Layer&lt;/h1&gt;\n&lt;p&gt;要打开Aggregation Layer，需要配置一下apiserver，增加相关认证证书。认证流程是client发起请求到apiserver，apiserver与aggergated apiserver建立tls安全链接，把请求proxy到aggergated apiserver，继续进行–requestheader-*参数的相关认证。&lt;/p&gt;\n&lt;p&gt;认证流程&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1589865988979.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;需要生成aggregate使用的证书，参考cfssl生成证书方法，proxy-client-cert-file的CN需要与requestheader-allowed-names匹配。&lt;br&gt;\n在apiserver增加如下启动参数&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;--requestheader-client-ca-file=/etc/kubernetes/pki/agg-ca.pem\n--proxy-client-cert-file=/etc/kubernetes/pki/aggregate.pem\n--proxy-client-key-file=/etc/kubernetes/pki/aggregate-key.pem\n--requestheader-allowed-names=aggregator\n--requestheader-extra-headers-prefix=X-Remote-Extra-\n--requestheader-group-headers=X-Remote-Group\n--requestheader-username-headers=X-Remote-User\n#如果kube-proxy没有在Master上面运行，还需要配置\n--enable-aggregator-routing=true\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;metrics-server\&#34;&gt;metrics server&lt;/h1&gt;\n&lt;p&gt;从k8s 1.8开始，集群的资源使用情况都通过metrics api收集，例如容器CPU、内存。这些指标可用于kuberctl top或者k8s的HPA等特性。&lt;br&gt;\nmetrice server可以在github找到并部署&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/kubernetes-incubator/metrics-server\ncd metrics-server\nkubectl create -f deploy/1.8+/\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;注1：metrics-server默认使用node的主机名，但是coredns里面没有物理机主机名的解析，一种是部署的时候添加一个参数： –kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP,第二种是使用dnsmasq构建一个上游的dns服务\n注2：kubelet 的10250端口使用的是https协议，连接需要验证tls证书。可以在metrics server启动命令添加参数–kubelet-insecure-tls不验证客户端证书\n注3：yaml文件中的image地址k8s.gcr.io/metrics-server-amd64:v0.3.3 需要梯子，需要改成中国可以访问的image地址，可以使用aliyun的。这里使用hub.docker.com里的google镜像地址 image: mirrorgooglecontainers/metrics-server-amd64:v0.3.3\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;成功运行kubectl top命令&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;ubuntu@k8s-dev-m1:~/k8sssl/agglayer$ kubectl top nodes\nNAME                   CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nk8s-dev-node2          103m         5%      2696Mi                 72%       \nk8s-dev-node3.bxr.cn   115m      2%     5312Mi                  67%  \nk8s-dev-node4          57m          2%       2634Mi                  70%     \nk8s-dev-node5          148m         7%       2443Mi                  65%\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;hpa\&#34;&gt;HPA&lt;/h1&gt;\n&lt;pre&gt;&lt;code&gt;有了metrics就可以开始使用HPA特性了。hpa有几个特点\ndeploy或者rs等需要设置resources才能使用hpa\n如果我们创建一个HPA controller，它会每隔15s（可以通过–horizontal-pod-autoscaler-sync-period修改）检测一次hpa定义的资源与实际资源使用情况，如果达到阀值就会调整pod数量。\nHPA设置的阀值不是绝对的，允许设置一个浮动范围，–horizontal-pod-autoscaler-tolerance默认是0.1\npod调整算法 desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]\nscale有一个窗口期，期间每次变化会记录下来，选择最优的调整建议再进行scale，这样可以保证资源平滑变动，通过–horizontal-pod-autoscaler-downscale-stabilization设定，默认5分钟。\n通过hpa调整新增的pod不会马上ready，这时候收集的metrics就不准，为了减少影响，hpa一开始不会收集新pod的metrics。通过–horizontal-pod-autoscaler-initial-readiness-delay（默认30s）和 –horizontal-pod-autoscaler-cpu-initialization-period（默认为 5 分钟）调整\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1589866316410.svg\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n示例hpa.yml:&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: hpa-test\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: podinfo\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 75\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization \n        averageUtilization: 160\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;上面的示例包括cpu和memory指标，averageUtilization这个百分比是根据deployment的resources.requests计算的。例如有deployment限制requests是512Mi，replicas是2，实际pod1用了612Mi，pod2用了598Mi，计算公式是 (612+598)/2/512 = 118%&lt;/p&gt;\n&lt;p&gt;查看hpa的情况，targets第一个是memory，第二个是cpu指标，REPLICAS是根据计算后的当前pod数&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;ubuntu@k8s-m1:~/k8s/hpa$ kubectl get hpa\nNAME       REFERENCE            TARGETS             MINPODS   MAXPODS   REPLICAS   AGE\nhpa-test   Deployment/podinfo   120%/160%, 6%/75%   2         4         3          97m\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;官方示例还包括packets-per-second、requests-per-second这些指标，需要进一步验证&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: php-apache\n  namespace: default\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: php-apache\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: AverageUtilization\n        averageUtilization: 50\n  - type: Pods\n    pods:\n      metric:\n        name: packets-per-second\n      targetAverageValue: 1k\n  - type: Object\n    object:\n      metric:\n        name: requests-per-second\n      describedObject:\n        apiVersion: networking.k8s.io/v1beta1\n        kind: Ingress\n        name: main-route\n      target:\n        kind: Value\n        value: 10k\nstatus:\n  observedGeneration: 1\n  lastScaleTime: &amp;lt;some-time&amp;gt;\n  currentReplicas: 1\n  desiredReplicas: 1\n  currentMetrics:\n  - type: Resource\n    resource:\n      name: cpu\n    current:\n      averageUtilization: 0\n      averageValue: 0\n  - type: Object\n    object:\n      metric:\n        name: requests-per-second\n      describedObject:\n        apiVersion: networking.k8s.io/v1beta1\n        kind: Ingress\n        name: main-route\n      current:\n        value: 10k\n\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;&lt;a href=\&#34;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#create-horizontal-pod-autoscaler\&#34;&gt;官方hpa参数&lt;/a&gt;&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;k8s-hpa-dan-xing-kuo-rong-pei-zhi&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;k8s hpa弹性扩容配置&#34;,&#34;tags&#34;:[{&#34;name&#34;:&#34;hpa&#34;,&#34;slug&#34;:&#34;1VCnVUFme&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/1VCnVUFme/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-05-19 13:24:13&#34;,&#34;dateFormat&#34;:&#34;2020-05-19&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/k8s-hpa-dan-xing-kuo-rong-pei-zhi/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;6 min read&#34;,&#34;time&#34;:328000,&#34;words&#34;:1211,&#34;minutes&#34;:6},&#34;description&#34;:&#34;HPA全称Horizontal Pod Autoscaling，是K8s实现pod自动水平扩容缩容的特性，这个特性使整个kubernetes集群马上高大上起来了。\n要使用HPA也不是这么简单的，HPA api分v1、v2beta1、v2ba...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#aggregation-layer\&#34;&gt;Aggregation Layer&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#metrics-server\&#34;&gt;metrics server&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#hpa\&#34;&gt;HPA&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;p&gt;Go 语言内部其实已经提供了 http.ServeFile，通过这个函数可以实现静态文件的服务。&lt;/p&gt;\n&lt;p&gt;beego 针对这个功能进行了一层封装，通过下面的方式进行静态文件注册：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;beego.SetStaticPath(&amp;quot;/static&amp;quot;,&amp;quot;public&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;第一个参数是路径，url 路径信息&lt;br&gt;\n第二个参数是静态文件目录（相对应用所在的目录）&lt;br&gt;\nbeego 支持多个目录的静态文件注册，用户可以注册如下的静态文件目录：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;beego.SetStaticPath(&amp;quot;/images&amp;quot;,&amp;quot;images&amp;quot;)\nbeego.SetStaticPath(&amp;quot;/css&amp;quot;,&amp;quot;css&amp;quot;)\nbeego.SetStaticPath(&amp;quot;/js&amp;quot;,&amp;quot;js&amp;quot;)\n```　　\n\n设置了如上的静态目录之后，用户访问 /images/login/login.png，那么就会访问应用对应的目录下面的 images/login/login.png 文件。\n\n如果是访问 /static/img/logo.png，那么就访问 public/img/logo.png文件。\n\n默认情况下 beego 会判断目录下文件是否存在，不存在直接返回 404 页面，如果请求的是 index.html，那么由于 http.ServeFile 默认是会跳转的，不提供该页面的显示。\n\n因此 beego 可以设置 beego.BConfig.WebConfig.DirectoryIndex=true 这样来使得显示 index.html 页面。而且开启该功能之后，用户访问目录就会显示该目录下所有的文件列表。&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;beego-jing-tai-wen-jian-jia-zai-lu-jing-xiu-gai&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;beego静态文件加载路径修改&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;golang&#34;,&#34;slug&#34;:&#34;oHQV9cP9p&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/oHQV9cP9p/&#34;},{&#34;name&#34;:&#34;beego&#34;,&#34;slug&#34;:&#34;WCSltmB8m&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/WCSltmB8m/&#34;}],&#34;date&#34;:&#34;2020-05-11 12:49:08&#34;,&#34;dateFormat&#34;:&#34;2020-05-11&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/beego-jing-tai-wen-jian-jia-zai-lu-jing-xiu-gai/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;2 min read&#34;,&#34;time&#34;:77000,&#34;words&#34;:328,&#34;minutes&#34;:2},&#34;description&#34;:&#34;Go 语言内部其实已经提供了 http.ServeFile，通过这个函数可以实现静态文件的服务。\nbeego 针对这个功能进行了一层封装，通过下面的方式进行静态文件注册：\nbeego.SetStaticPath(&amp;quot;/static&amp;...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;p&gt;Ceph v0.55 及后续版本默认开启了 cephx 认证。从用户空间（ FUSE ）挂载一 Ceph 文件系统前，确保客户端主机有一份 Ceph 配置副本、和具备 Ceph 元数据服务器能力的密钥环。&lt;/p&gt;\n&lt;p&gt;1、 在客户端主机上，把监视器主机上的 Ceph 配置文件拷贝到 /etc/ceph/ 目录下&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;sudo mkdir -p /etc/ceph\nsudo scp {user}@{server-machine}:/etc/ceph/ceph.conf /etc/ceph/ceph.conf\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;2、 在客户端主机上，把监视器主机上的 Ceph 密钥环拷贝到 /etc/ceph 目录下&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;sudo scp {user}@{server-machine}:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;3、 确保客户端机器上的 Ceph 配置文件和密钥环都有合适的权限位，如 chmod 644 。&lt;br&gt;\ncephx 如何配置请参考 CEPHX 配置参考。&lt;br&gt;\n要把 Ceph 文件系统挂载为用户空间文件系统，可以用 ceph-fuse 命令，例如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;sudo mkdir /home/usernname/cephfs\nsudo ceph-fuse -m 192.168.0.1:6789 /home/username/cephfs\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;详情见 &lt;a href=\&#34;http://docs.ceph.org.cn/man/8/ceph-fuse/\&#34;&gt;ceph-fuse&lt;/a&gt;&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;yong-hu-kong-jian-gua-zai-ceph-wen-jian-xi-tong&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;用户空间挂载 CEPH 文件系统&#34;,&#34;tags&#34;:[{&#34;name&#34;:&#34;rook-ceph&#34;,&#34;slug&#34;:&#34;hu0mNDfuy&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/hu0mNDfuy/&#34;}],&#34;date&#34;:&#34;2020-05-08 14:03:36&#34;,&#34;dateFormat&#34;:&#34;2020-05-08&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/yong-hu-kong-jian-gua-zai-ceph-wen-jian-xi-tong/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;2 min read&#34;,&#34;time&#34;:61000,&#34;words&#34;:242,&#34;minutes&#34;:2},&#34;description&#34;:&#34;Ceph v0.55 及后续版本默认开启了 cephx 认证。从用户空间（ FUSE ）挂载一 Ceph 文件系统前，确保客户端主机有一份 Ceph 配置副本、和具备 Ceph 元数据服务器能力的密钥环。\n1、 在客户端主机上，把监视器主机...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;问题\&#34;&gt;问题&lt;/h1&gt;\n&lt;pre&gt;&lt;code&gt;curl https://192.168.0.200:8443\n提示curl: (60) Peer&#39;s Certificate issuer is not recognized.\ncurl: (60) Peer&#39;s Certificate issuer is not recognized.\nMore details here: http://curl.haxx.se/docs/sslcerts.html\n\ncurl performs SSL certificate verification by default, using a &amp;quot;bundle&amp;quot;\n of Certificate Authority (CA) public keys (CA certs). If the default\n bundle file isn&#39;t adequate, you can specify an alternate file\n using the --cacert option.\nIf this HTTPS server uses a certificate signed by a CA represented in\n the bundle, the certificate verification probably failed due to a\n problem with the certificate (it might be expired, or the name might\n not match the domain name in the URL).\nIf you&#39;d like to turn off curl&#39;s verification of the certificate, use\n the -k (or --insecure) option.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;mac-os-x\&#34;&gt;Mac OS X&lt;/h1&gt;\n&lt;h2 id=\&#34;添加证书\&#34;&gt;添加证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ~/new-root-certificate.crt\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;移除证书\&#34;&gt;移除证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;sudo security delete-certificate -c &amp;quot;&amp;lt;name of existing certificate&amp;gt;&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;windows\&#34;&gt;Windows&lt;/h1&gt;\n&lt;h2 id=\&#34;添加证书-2\&#34;&gt;添加证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;certutil -addstore -f &amp;quot;ROOT&amp;quot; new-root-certificate.crt\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;移除证书-2\&#34;&gt;移除证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;certutil -delstore &amp;quot;ROOT&amp;quot; serial-number-hex\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;linux-ubuntu-debian\&#34;&gt;Linux (Ubuntu, Debian)&lt;/h1&gt;\n&lt;h2 id=\&#34;添加证书-3\&#34;&gt;添加证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;复制 CA 文件到目录： /usr/local/share/ca-certificates/\n执行:\nsudo cp foo.crt /usr/local/share/ca-certificates/foo.crt\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;更新-ca-证书库\&#34;&gt;更新 CA 证书库:&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;sudo update-ca-certificates\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;移除证书-3\&#34;&gt;移除证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;Remove your CA.\n\nUpdate the CA store:\n\nsudo update-ca-certificates --fresh\n\nRestart Kerio Connect to reload the certificates in the 32-bit versions or Debian 7.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;linux-centos-6\&#34;&gt;Linux (CentOs 6)&lt;/h1&gt;\n&lt;h2 id=\&#34;添加证书-4\&#34;&gt;添加证书：&lt;/h2&gt;\n&lt;p&gt;// root-ca.crt 为ca证书&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;安装 ca-certificates package:\n\nyum install ca-certificates\n\n启用dynamic CA configuration feature:\n\nupdate-ca-trust force-enable\n\nAdd it as a new file to /etc/pki/ca-trust/source/anchors/:\ncp root-ca.crt /etc/pki/ca-trust/source/anchors/\n\n执行:\n\nupdate-ca-trust extract\n\nRestart Kerio Connect to reload the certificates in the 32-bit version.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;linux-centos-5\&#34;&gt;Linux (CentOs 5)&lt;/h1&gt;\n&lt;h2 id=\&#34;添加证书-5\&#34;&gt;添加证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;Append your trusted certificate to file /etc/pki/tls/certs/ca-bundle.crt\n\ncat foo.crt &amp;gt;&amp;gt; /etc/pki/tls/certs/ca-bundle.crt\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;测试访问\&#34;&gt;测试访问&lt;/h1&gt;\n&lt;pre&gt;&lt;code&gt; curl -v &amp;quot;https:/gitlab.test.com/micro-lib/server?go-get=1&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;linux-ru-he-dao-ru-zi-ding-yi-zheng-shu&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;linux如何导入自定义证书&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;linux&#34;,&#34;slug&#34;:&#34;qf8arSPKY&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/qf8arSPKY/&#34;},{&#34;name&#34;:&#34;ssl&#34;,&#34;slug&#34;:&#34;EVZAZcQaN&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/EVZAZcQaN/&#34;}],&#34;date&#34;:&#34;2020-05-06 16:33:15&#34;,&#34;dateFormat&#34;:&#34;2020-05-06&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/linux-ru-he-dao-ru-zi-ding-yi-zheng-shu/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;3 min read&#34;,&#34;time&#34;:141000,&#34;words&#34;:406,&#34;minutes&#34;:3},&#34;description&#34;:&#34;问题\ncurl https://192.168.0.200:8443\n提示curl: (60) Peer&#39;s Certificate issuer is not recognized.\ncurl: (60) Peer&#39;s Certifica...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%97%AE%E9%A2%98\&#34;&gt;问题&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#mac-os-x\&#34;&gt;Mac OS X&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6\&#34;&gt;添加证书：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%A7%BB%E9%99%A4%E8%AF%81%E4%B9%A6\&#34;&gt;移除证书：&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#windows\&#34;&gt;Windows&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6-2\&#34;&gt;添加证书：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%A7%BB%E9%99%A4%E8%AF%81%E4%B9%A6-2\&#34;&gt;移除证书：&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#linux-ubuntu-debian\&#34;&gt;Linux (Ubuntu, Debian)&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6-3\&#34;&gt;添加证书：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%9B%B4%E6%96%B0-ca-%E8%AF%81%E4%B9%A6%E5%BA%93\&#34;&gt;更新 CA 证书库:&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%A7%BB%E9%99%A4%E8%AF%81%E4%B9%A6-3\&#34;&gt;移除证书：&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#linux-centos-6\&#34;&gt;Linux (CentOs 6)&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6-4\&#34;&gt;添加证书：&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#linux-centos-5\&#34;&gt;Linux (CentOs 5)&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6-5\&#34;&gt;添加证书：&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B5%8B%E8%AF%95%E8%AE%BF%E9%97%AE\&#34;&gt;测试访问&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h3 id=\&#34;前提\&#34;&gt;前提&lt;/h3&gt;\n&lt;p&gt;由于istio部署到IDC、非云环境无法获取客户端真实IP地址，web应用无法使用该组件，早期版本的istio做了很多测试，最终无法实现获取remote client address；&lt;/p&gt;\n&lt;h3 id=\&#34;环境\&#34;&gt;环境&lt;/h3&gt;\n&lt;p&gt;kubernetes版本：k8s-1.16.9&lt;br&gt;\nistio版本：1.5&lt;/p&gt;\n&lt;h3 id=\&#34;方法\&#34;&gt;方法&lt;/h3&gt;\n&lt;pre&gt;&lt;code&gt;kubectl -n istio-system edit  deployments. istio-pilot\n添加如下：\n       env:\n       - name: PILOT_SIDECAR_USE_REMOTE_ADDRESS\n          value: &amp;quot;true&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;以下是github相应的issue&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588232244321.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;其他测试\&#34;&gt;其他测试&lt;/h3&gt;\n&lt;p&gt;istio-1.5版本回归单体，各个组件优化了很多，后期测试http链接与tcp链接应用&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;istio-ke-hu-duan-yuan-di-zhi-ru-he-xian-shi&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;istio-客户端源地址如何显示&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;istio&#34;,&#34;slug&#34;:&#34;d8Hk7igEb&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/d8Hk7igEb/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-04-30 15:32:39&#34;,&#34;dateFormat&#34;:&#34;2020-04-30&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/istio-ke-hu-duan-yuan-di-zhi-ru-he-xian-shi/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;1 min read&#34;,&#34;time&#34;:32000,&#34;words&#34;:133,&#34;minutes&#34;:1},&#34;description&#34;:&#34;前提\n由于istio部署到IDC、非云环境无法获取客户端真实IP地址，web应用无法使用该组件，早期版本的istio做了很多测试，最终无法实现获取remote client address；\n环境\nkubernetes版本：k8s-1.16...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%89%8D%E6%8F%90\&#34;&gt;前提&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%8E%AF%E5%A2%83\&#34;&gt;环境&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%96%B9%E6%B3%95\&#34;&gt;方法&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%85%B6%E4%BB%96%E6%B5%8B%E8%AF%95\&#34;&gt;其他测试&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;前提\&#34;&gt;前提：&lt;/h1&gt;\n&lt;p&gt;在kubernetes集群中部署elk组件，es集群部署3个节点，kibana部署一个，容器数据持久化需要storageclass&lt;/p&gt;\n&lt;p&gt;依赖：&lt;br&gt;\nHelm&lt;br&gt;\nPersistent Volumes&lt;/p&gt;\n&lt;h1 id=\&#34;准备配置\&#34;&gt;准备配置&lt;/h1&gt;\n&lt;p&gt;由于repo在线安装太慢，建议下载char本地修改参数后安装&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/elastic/helm-charts.git\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;部署-elk\&#34;&gt;部署 ELK&lt;/h1&gt;\n&lt;h2 id=\&#34;创建elk命名空间\&#34;&gt;创建elk命名空间&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;#cat elk-ns.yml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: elk\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;部署elasticsearch\&#34;&gt;部署elasticsearch&lt;/h2&gt;\n&lt;p&gt;cd helm-charts/elasticsearch&lt;/p&gt;\n&lt;p&gt;helm install --namespace=elk  --name=elasticsearch .&lt;/p&gt;\n&lt;h2 id=\&#34;部署-kibana\&#34;&gt;部署 Kibana&lt;/h2&gt;\n&lt;p&gt;cd helm-charts/kibana&lt;/p&gt;\n&lt;p&gt;helm install --namespace=elk --name=kibana .&lt;br&gt;\n通过 kubectl get deploy 和 pod 了解部署状态；&lt;/p&gt;\n&lt;h1 id=\&#34;小知识\&#34;&gt;小知识&lt;/h1&gt;\n&lt;p&gt;Kibana 直接通过 K8S 内部 DNS 域名 访问 ES。&lt;/p&gt;\n&lt;p&gt;查看容器内的配置&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kubectl  exec kibana-kibana-7cbc5db55c-6qct7 -c kibana -- cat /usr/share/kibana/config/kibana.yml\n\n# Default Kibana configuration for docker target\nserver.name: kibana\nserver.host: &amp;quot;0&amp;quot;\nelasticsearch.hosts: [ &amp;quot;http://elasticsearch:9200&amp;quot; ]\nxpack.monitoring.ui.container.elasticsearch.enabled: true\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;kibana-添加-ingress\&#34;&gt;Kibana 添加 Ingress&lt;/h1&gt;\n&lt;p&gt;通过 Ingress 添加访问入口&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: kibana\n  namespace: default\nspec:\n  rules:\n  - host: &amp;lt;YourDomain&amp;gt;  ## 访问 Kibana 的域名 \n    http:\n      paths:\n      - backend:\n          serviceName: kibana-kibana\n          servicePort: 5601\n        path: /\n status:\n  loadBalancer:\n    ingress:\n    - ip: &amp;lt;YourLoadBalancerIP&amp;gt;  ## LB 的 IP\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;访问测试\&#34;&gt;访问测试&lt;/h1&gt;\n&lt;p&gt;访问域名，即可打开 Kibana 7.3 版本；&lt;/p&gt;\n&lt;figure data-type=\&#34;image\&#34; tabindex=\&#34;1\&#34;&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588231324831.jpg\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/figure&gt;\n&lt;p&gt;查看集群的运行状态&lt;/p&gt;\n&lt;figure data-type=\&#34;image\&#34; tabindex=\&#34;2\&#34;&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588231313286.jpg\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/figure&gt;\n&lt;p&gt;也可以通过命令行查看&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;~$ curl  -s &amp;lt;YourESHost&amp;gt;/_cluster/health | jq .\n{\n  &amp;quot;cluster_name&amp;quot;: &amp;quot;elasticsearch&amp;quot;,\n  &amp;quot;status&amp;quot;: &amp;quot;yellow&amp;quot;,\n  &amp;quot;timed_out&amp;quot;: false,\n  &amp;quot;number_of_nodes&amp;quot;: 3,\n  &amp;quot;number_of_data_nodes&amp;quot;: 3,\n  &amp;quot;active_primary_shards&amp;quot;: 19,\n  &amp;quot;active_shards&amp;quot;: 35,\n  &amp;quot;relocating_shards&amp;quot;: 0,\n  &amp;quot;initializing_shards&amp;quot;: 0,\n  &amp;quot;unassigned_shards&amp;quot;: 3,\n  &amp;quot;delayed_unassigned_shards&amp;quot;: 0,\n  &amp;quot;number_of_pending_tasks&amp;quot;: 0,\n  &amp;quot;number_of_in_flight_fetch&amp;quot;: 0,\n  &amp;quot;task_max_waiting_in_queue_millis&amp;quot;: 0,\n  &amp;quot;active_shards_percent_as_number&amp;quot;: 92.10526315789474\n}\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;k8s-tong-guo-helm-bu-shu-elk-73&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;K8S通过helm 部署 ELK 7.3&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;elk&#34;,&#34;slug&#34;:&#34;Diytd8lD3&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Diytd8lD3/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-04-30 15:12:33&#34;,&#34;dateFormat&#34;:&#34;2020-04-30&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/k8s-tong-guo-helm-bu-shu-elk-73/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;2 min read&#34;,&#34;time&#34;:111000,&#34;words&#34;:366,&#34;minutes&#34;:2},&#34;description&#34;:&#34;前提：\n在kubernetes集群中部署elk组件，es集群部署3个节点，kibana部署一个，容器数据持久化需要storageclass\n依赖：\nHelm\nPersistent Volumes\n准备配置\n由于repo在线安装太慢，建议下载...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%89%8D%E6%8F%90\&#34;&gt;前提：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%87%86%E5%A4%87%E9%85%8D%E7%BD%AE\&#34;&gt;准备配置&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%83%A8%E7%BD%B2-elk\&#34;&gt;部署 ELK&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%9B%E5%BB%BAelk%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4\&#34;&gt;创建elk命名空间&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%83%A8%E7%BD%B2elasticsearch\&#34;&gt;部署elasticsearch&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%83%A8%E7%BD%B2-kibana\&#34;&gt;部署 Kibana&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%B0%8F%E7%9F%A5%E8%AF%86\&#34;&gt;小知识&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#kibana-%E6%B7%BB%E5%8A%A0-ingress\&#34;&gt;Kibana 添加 Ingress&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E8%AE%BF%E9%97%AE%E6%B5%8B%E8%AF%95\&#34;&gt;访问测试&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;p&gt;最近又出来个kubesphere的工具用来管理k8s，今天特意来安装体验下；&lt;br&gt;\ngithub地址：https://github.com/pixiake/ks-installer&lt;/p&gt;\n&lt;p&gt;官方使用文档：https://kubesphere.io/docs/advanced-v2.0/zh-CN/installation/all-in-one/&lt;/p&gt;\n&lt;p&gt;先放上安装效果图，UI界面还是很清爽的：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588152934567.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;当前环境\&#34;&gt;当前环境：&lt;/h3&gt;\n&lt;pre&gt;&lt;code&gt;k8s集群已经安装完成，用kubesphere管理现有的k8s集群；\n\nk8s版本为1.14  \n\n系统为centos7.6\n\nkubesphere使用要求：\n\nkubernetes version &amp;gt; 1.13.0\n\nhelm version &amp;gt; 2.10.0\n\na default storage class must be in kubernetes cluster\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;安装完成后默认用户名密码：&lt;/p&gt;\n&lt;p&gt;用户名：admin&lt;/p&gt;\n&lt;p&gt;密码：P@88w0rd&lt;/p&gt;\n&lt;h3 id=\&#34;开始安装\&#34;&gt;开始安装&lt;/h3&gt;\n&lt;p&gt;安装步骤大概记录：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kubectl create ns kubesphere-system\nkubectl create ns kubesphere-monitoring-system\n\n#访问etcd用到的secret\nkubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  --from-file=etcd-client-ca.crt=ca.pem  --from-file=etcd-client.crt=etcd-key.pem  --from-file=etcd-client.key=etcd.pem\n\n#管理k8s用到的secret\n\nkubectl -n kubesphere-system create secret generic kubesphere-ca  --from-file=ca.crt=ca.pem --from-file=ca.key=ca-key.pem\n\n#clone好github项目，执行下面的这条命令\n\ncd deploy\nkubectl apply -f kubesphere-installer.yaml\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;执行完上面的命令，可以通过下面的命令，查看安装过程日志&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l job-name=kubesphere-installer -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -f\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;查看安装结果，STATUS跟下面保持一致才说明安装成功&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;[root@ks-allinone deploy]# kubectl get pods -n kubesphere-system\nNAME                                     READY   STATUS      RESTARTS   AGE\nks-account-6db466d8dc-srrwj              1/1     Running     0          149m\nks-apigateway-7d77cb9495-jzmg6           1/1     Running     0          170m\nks-apiserver-f8469fd47-b58rm             1/1     Running     0          166m\nks-console-54c849bdc9-dfkbf              1/1     Running     0          168m\nks-console-54c849bdc9-z2d5q              1/1     Running     0          168m\nks-controller-manager-569456b4cd-gngm5   1/1     Running     0          170m\nks-docs-6bbdcc9bfb-6jldz                 1/1     Running     0          3h7m\nkubesphere-installer-7ph6l               0/1     Completed   1          3h11m\nopenldap-5c986c5bff-rzqwv                1/1     Running     0          3h25m\nredis-76dc4db5dd-lv6kg                   1/1     Running     0          149m\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h3 id=\&#34;安装过程出现的错误\&#34;&gt;安装过程出现的错误&lt;/h3&gt;\n&lt;p&gt;1.在安装的时提示metrics-server已经安装，导致安装中断；&lt;/p&gt;\n&lt;p&gt;解析办法：在kubesphere-installer.yaml的configMap增加配置：metrics_server_enable: False（默认是没有的）&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: v1\ndata:\n  ks-config.yaml: |\n    kube_apiserver_host: 10.10.5.208:6443\n    etcd_tls_enable: True\n    etcd_endpoint_ips: 10.10.5.169,10.10.5.183,10.10.5.184\n    disableMultiLogin: True\n    elk_prefix: logstash\n    metrics_server_enable: False\n  #  local_registry: 192.168.1.2:5000\nkind: ConfigMap\nmetadata:\n  name: kubesphere-config\n  namespace: kubesphere-system\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;增加Ingress配置：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: kubesphere\n  namespace: kubesphere-system\n  annotations:\n    #kubernetes.io/ingress.class: traefik\n    kubernetes.io/ingress.class: nginx\nspec:\n  rules:\n  - host: ks.staplescn.com\n    http:\n      paths:\n      - path:\n        backend:\n          serviceName: ks-console\n          servicePort: 80\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;访问界面：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588153130667.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;kubesphere-an-zhuang-shi-yong-ti-yan&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;kubesphere安装使用体验&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;kubesphere&#34;,&#34;slug&#34;:&#34;BU7sbQs51&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/BU7sbQs51/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-04-29 17:33:05&#34;,&#34;dateFormat&#34;:&#34;2020-04-29&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/kubesphere-an-zhuang-shi-yong-ti-yan/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;3 min read&#34;,&#34;time&#34;:176000,&#34;words&#34;:576,&#34;minutes&#34;:3},&#34;description&#34;:&#34;最近又出来个kubesphere的工具用来管理k8s，今天特意来安装体验下；\ngithub地址：https://github.com/pixiake/ks-installer\n官方使用文档：https://kubesphere.io/doc...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BD%93%E5%89%8D%E7%8E%AF%E5%A2%83\&#34;&gt;当前环境：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BC%80%E5%A7%8B%E5%AE%89%E8%A3%85\&#34;&gt;开始安装&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E5%87%BA%E7%8E%B0%E7%9A%84%E9%94%99%E8%AF%AF\&#34;&gt;安装过程出现的错误&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;前言\&#34;&gt;前言&lt;/h1&gt;\n&lt;p&gt;我们通常的开发流程是，在本地开发完成应用之后，使用git作为版本管理工具，将本地代码提交到类似Github这样的仓库中做持久化存储，当我们可能来自多个仓库、可能涉及到多个中间件作为底层依赖一起部署到生产环境中时，相信不少在大中型企业工作的小伙伴都知道公司内部通常会有发布系统，那么云原生技术栈中有没有为我们提供类似的发布系统呢？档案是肯定的，而且不乏竞争者，业内知名的有knavtie Build/jekinsX/spinnaker/orgo/tekton，其中，tekon凭借其众多优良特性在一众竞争者中胜出，成为领域内的事实标准，今天我们就来揭开Tekton的神秘面纱。&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588236885419.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h1 id=\&#34;正文\&#34;&gt;正文&lt;/h1&gt;\n&lt;h3 id=\&#34;什么是tekton\&#34;&gt;什么是Tekton&lt;/h3&gt;\n&lt;p&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588236902661.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n那Tekton都提供了哪些CRD呢？&lt;br&gt;\n•\tTask：顾名思义，task表示一个构建任务，task里可以定义一系列的steps，例如编译代码、构建镜像、推送镜像等，每个step实际由一个Pod执行。&lt;br&gt;\n•\tTaskRun：task只是定义了一个模版，taskRun才真正代表了一次实际的运行，当然你也可以自己手动创建一个taskRun，taskRun创建出来之后，就会自动触发task描述的构建任务。&lt;br&gt;\n•\tPipeline：一个或多个task、PipelineResource以及各种定义参数的集合。&lt;br&gt;\n•\tPipelineRun：类似task和taskRun的关系，pipelineRun也表示某一次实际运行的pipeline，下发一个pipelineRun CRD实例到kubernetes后，同样也会触发一次pipeline的构建。&lt;br&gt;\n•\tPipelineResource：表示pipeline input资源，比如github上的源码，或者pipeline output资源，例如一个容器镜像或者构建生成的jar包等。&lt;br&gt;\n他们大概有如下图所示的关系：&lt;br&gt;\n官方介绍：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;Tekton 是一个功能强大且灵活的 Kubernetes 原生开源框架，用于创建持续集成和交付（CI/CD）系统。通过抽象底层实现细节，用户可以跨多云平台和本地系统进行构建、测试和部署。\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;个人理解：\n•\t以yaml文件编排应用构建及部署流程\n•\tknavtive build模块升级版，社区最终采用 Tekton 替代 knavtive Build作为云原生领域的CI/CD 解决方案\n•\t标准化CI/CD流水线构建、测试及部署流程的工具\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;Tekton在一众竞争对手的比拼中PK胜出：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588236947460.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;下面就让我们一起来深入详细了解下Tekton到底怎么玩。&lt;br&gt;\nTekton Pipeline中有5类对象，核心理念是通过定义yaml定义构建过程。&lt;br&gt;\n•\tTask：一个任务的执行模板，用于描述单个任务的构建过程&lt;br&gt;\n•\tTaskRun：需要通过定义TaskRun任务去运行Task。&lt;br&gt;\n•\tPipeline：包含多个Task,并在此基础上定义input和output,input和output以PipelineResource作为交付。&lt;br&gt;\n•\tPipelineRun：需要定义PipelineRun才会运行Pipeline。&lt;br&gt;\n•\tPipelineResource：可用于input和output的对象集合。&lt;/p&gt;\n&lt;h3 id=\&#34;task\&#34;&gt;Task&lt;/h3&gt;\n&lt;p&gt;Task 就是一个任务执行模板，之所以说 Task 是一个模板是因为 Task 定义中可以包含变量，Task 在真正执行的时候需要给定变量的具体值。如果把 Tekton 的 Task 有点儿类似于定义一个函数，Task 通过 inputs.params 定义需要哪些入参，并且每一个入参还可以指定默认值。Task 的 steps 字段表示当前 Task 是有哪些步骤组成的，每一个步骤具体就是基于镜像启动一个 container 执行一些操作，container 的启动参数可以通过 Task 的入参使用模板语法进行配置。下面是一个Demo：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588236983683.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;taskrun\&#34;&gt;TaskRun&lt;/h3&gt;\n&lt;p&gt;Task 定义好以后是不能执行的，就像一个函数定义好以后需要调用才能执行一样。所以需要再定义一个 TaskRun 去执行 Task。&lt;br&gt;\nTaskRun 主要是负责设置 Task 需要的参数，并通过 taskRef 字段引用要执行的 Task。下面是一个Demo：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237005091.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;但是在实际使用过程中，我们一般很少使用TaskRun，因为它只能给不一个Task 传参，Tekton提供了给多个Task同时传参的解决方案Pipeline和PipelineRun，且看下文详解，这里只是多嘴一下，这个TaskRun很少使用，稍微了解下就可以了。&lt;/p&gt;\n&lt;h3 id=\&#34;pipeline\&#34;&gt;Pipeline&lt;/h3&gt;\n&lt;p&gt;一个 TaskRun 只能执行一个 Task，当需要编排多个 Task 的时候就需要 Pipeline 出马了。Pipeline 是一个编排 Task 的模板。Pipeline 的 params 声明了执行时需要的入参。 Pipeline 的 spec.tasks 定义了需要编排的 Task。Tasks 是一个数组，数组中的 task 并不是通过数组声明的顺序去执行的，而是通过 runAfter 来声明 task 执行的顺序。Tekton controller 在解析 CRD 的时候会解析 Task 的顺序，然后根据 runAfter 设置生成的依次树依次去执行。Pipeline 在编排 Task 的时候需要给每一个 Task 传入必须的参数，这些参数的值可以来自 Pipeline 自身的 params 设置。下面是一个Demo：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237033509.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237113343.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;pipelinerun\&#34;&gt;PipelineRun&lt;/h3&gt;\n&lt;p&gt;和 Task 一样 Pipeline 定义完成以后也是不能直接执行的，需要 PipelineRun 才能执行 Pipeline。PipelineRun 的主要作用是给 Pipeline 传入必要的入参，并执行 Pipeline。下面是一个Demo：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237145814.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237151158.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;pipelineresource\&#34;&gt;PipelineResource&lt;/h3&gt;\n&lt;p&gt;可能你还想在 Task 之间共享资源，这就是 PipelineResource 的作用。比如我们可以把 git 仓库信息放在 PipelineResource 中。这样所有 Task 就可以共享这些信息了。&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237189249.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237194640.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237221271.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h1 id=\&#34;实战\&#34;&gt;实战&lt;/h1&gt;\n&lt;p&gt;关于Tekton的实战，可以参考Github里面的这个完整的Demo，里面是一个go语言吧编写的web服务，接口可以打印&amp;quot;Hello world&amp;quot;。时间有限，就不做演示了，感兴趣的可以在自己的k8s集群上面跑一下感受一下，相关的yaml文件也可以拷贝下来，作为后面改写的模板。&lt;br&gt;\n准备 PIpeline 的资源&lt;br&gt;\nkubectl apply -f tasks/source-to-image.yaml -f tasks/deploy-using-kubectl.yaml  -f resources/picalc-git.yaml -f image-secret.yaml -f pipeline-account.yaml -f pipeline/build-and-deploy-pipeline.yaml&lt;br&gt;\n执行 create 把 pipelieRun 提交到 Kubernetes 集群。之所以这里使用 create 而不是使用 apply 是因为 PIpelineRun 每次都会创建一个新的，kubectl 的 create 指令会基于 generateName 创建新的 PIpelineRun 资源。&lt;br&gt;\nkubectl create -f run/picalc-pipeline-run.yaml&lt;/p&gt;\n&lt;h1 id=\&#34;总结\&#34;&gt;总结&lt;/h1&gt;\n&lt;p&gt;Tekton以K8S为依托，成为云原生领域CI/CD的事实性标准，帮助我们提高云原生环境下的应用构建和部署效率。&lt;br&gt;\n来一张图对全文做一个简单的总结：&lt;/p&gt;\n&lt;figure data-type=\&#34;image\&#34; tabindex=\&#34;1\&#34;&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588236724398.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/figure&gt;\n&lt;h1 id=\&#34;参考资料\&#34;&gt;参考资料&lt;/h1&gt;\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\&#34;https://cloud.google.com/tekton/\&#34;&gt;Tekton官网&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;https://github.com/knative-sample/tekton-knative/tree/b1.0?spm=ata.13261165.0.0.21213a182xyMm5&amp;amp;file=b1.0\&#34;&gt;Tekton Demo&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&#34;,&#34;fileName&#34;:&#34;kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;kubernetes Tekton-CI/CD 持续集成流水线&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;tekton&#34;,&#34;slug&#34;:&#34;nN9-7w-_h&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/nN9-7w-_h/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-01-20 16:48:40&#34;,&#34;dateFormat&#34;:&#34;2020-01-20&#34;,&#34;feature&#34;:&#34;http://lvelvis.github.io/post-images/kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian.png&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;7 min read&#34;,&#34;time&#34;:363000,&#34;words&#34;:1633,&#34;minutes&#34;:7},&#34;description&#34;:&#34;前言\n我们通常的开发流程是，在本地开发完成应用之后，使用git作为版本管理工具，将本地代码提交到类似Github这样的仓库中做持久化存储，当我们可能来自多个仓库、可能涉及到多个中间件作为底层依赖一起部署到生产环境中时，相信不少在大中型企业工...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%89%8D%E8%A8%80\&#34;&gt;前言&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%AD%A3%E6%96%87\&#34;&gt;正文&lt;/a&gt;&lt;br&gt;\n*\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E4%BB%80%E4%B9%88%E6%98%AFtekton\&#34;&gt;什么是Tekton&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#task\&#34;&gt;Task&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#taskrun\&#34;&gt;TaskRun&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#pipeline\&#34;&gt;Pipeline&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#pipelinerun\&#34;&gt;PipelineRun&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#pipelineresource\&#34;&gt;PipelineResource&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%AE%9E%E6%88%98\&#34;&gt;实战&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%80%BB%E7%BB%93\&#34;&gt;总结&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99\&#34;&gt;参考资料&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;简介\&#34;&gt;简介&lt;/h1&gt;\n&lt;p&gt;Rook官网：https://rook.io&lt;br&gt;\nRook是云原生计算基金会(CNCF)的孵化级项目.&lt;br&gt;\nRook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。&lt;br&gt;\n至于CEPH，官网在这：https://ceph.com/&lt;br&gt;\nceph官方提供的helm部署，至今我没成功过，所以转向使用rook提供的方案&lt;br&gt;\n有道笔记原文：http://note.youdao.com/noteshare?id=281719f1f0374f787effc90067e0d5ad&amp;amp;sub=0B59EA339D4A4769B55F008D72C1A4C0&lt;/p&gt;\n&lt;h1 id=\&#34;环境\&#34;&gt;环境&lt;/h1&gt;\n&lt;pre&gt;&lt;code&gt;centos 7.5\nkernel 4.18.7-1.el7.elrepo.x86_64\ndocker 18.06\nkubernetes v1.12.2\n    kubeadm部署：\n        网络: canal\n        DNS: coredns\n    集群成员：    \n    192.168.1.1 kube-master\n    192.168.1.2 kube-node1\n    192.168.1.3 kube-node2\n    192.168.1.4 kube-node3\n    192.168.1.5 kube-node4\n\n所有node节点准备一块200G的磁盘：/dev/sdb\nkubernetes搭建rook-ceph\n&lt;/code&gt;&lt;/pre&gt;\n&lt;figure data-type=\&#34;image\&#34; tabindex=\&#34;1\&#34;&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233297037.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/figure&gt;\n&lt;h1 id=\&#34;准备工作\&#34;&gt;准备工作&lt;/h1&gt;\n&lt;p&gt;所有节点开启ip_forward&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;  /etc/sysctl.d/ceph.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;sysctl -p&lt;/p&gt;\n&lt;h1 id=\&#34;开始部署operator\&#34;&gt;开始部署Operator&lt;/h1&gt;\n&lt;h2 id=\&#34;部署rook-operator\&#34;&gt;部署Rook Operator&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;cd $HOME\ngit clone https://github.com/rook/rook.git\n\ncd rook\ncd cluster/examples/kubernetes/ceph\nkubectl apply -f operator.yaml\n&lt;/code&gt;&lt;/pre&gt;\n&lt;figure data-type=\&#34;image\&#34; tabindex=\&#34;2\&#34;&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233371696.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/figure&gt;\n&lt;h2 id=\&#34;查看operator的状态\&#34;&gt;查看Operator的状态&lt;/h2&gt;\n&lt;p&gt;执行apply之后稍等一会&lt;br&gt;\noperator会在集群内的每个主机创建两个pod:rook-discover,rook-ceph-agent&lt;/p&gt;\n&lt;p&gt;kubectl -n rook-ceph-system get pod -o wide&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233523024.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h2 id=\&#34;给节点打标签\&#34;&gt;给节点打标签&lt;/h2&gt;\n&lt;p&gt;运行ceph-mon的节点打上：ceph-mon=enabled&lt;br&gt;\nkubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-mon=enabled&lt;br&gt;\n运行ceph-osd的节点，也就是存储节点，打上：ceph-osd=enabled&lt;br&gt;\nkubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-osd=enabled&lt;br&gt;\n运行ceph-mgr的节点，打上：ceph-mgr=enabled&lt;br&gt;\nmgr只能支持一个节点运行，这是ceph跑k8s里的局限&lt;br&gt;\nkubectl label nodes kube-node1 ceph-mgr=enabled&lt;/p&gt;\n&lt;h2 id=\&#34;配置clusteryaml文件\&#34;&gt;配置cluster.yaml文件&lt;/h2&gt;\n&lt;p&gt;官方配置文件详解：https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html&lt;br&gt;\n文件中有几个地方要注意：&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;dataDirHostPath: 这个路径是会在宿主机上生成的，保存的是ceph的相关的配置文件，再重新生成*集群的时候要确保这个目录为空，否则mon会无法启动&lt;/li&gt;\n&lt;li&gt;useAllDevices: 使用所有的设备，建议为false，否则会把宿主机所有可用的磁盘都干掉&lt;/li&gt;\n&lt;li&gt;useAllNodes：使用所有的node节点，建议为false，肯定不会用k8s集群内的所有node来搭建ceph的&lt;/li&gt;\n&lt;li&gt;databaseSizeMB和journalSizeMB：当磁盘大于100G的时候，就注释这俩项就行了&lt;br&gt;\n本次实验用到的 cluster.yaml 文件内容如下：&lt;/li&gt;\n&lt;/ul&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: v1\nkind: Namespace\nmetadata:\n  name: rook-ceph\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: rook-ceph-cluster\n  namespace: rook-ceph\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: rook-ceph-cluster\n  namespace: rook-ceph\nrules:\n- apiGroups: [&amp;quot;&amp;quot;]\n  resources: [&amp;quot;configmaps&amp;quot;]\n  verbs: [ &amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;delete&amp;quot; ]\n---\n# Allow the operator to create resources in this cluster&#39;s namespace\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: rook-ceph-cluster-mgmt\n  namespace: rook-ceph\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: rook-ceph-cluster-mgmt\nsubjects:\n- kind: ServiceAccount\n  name: rook-ceph-system\n  namespace: rook-ceph-system\n---\n# Allow the pods in this namespace to work with configmaps\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: rook-ceph-cluster\n  namespace: rook-ceph\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: rook-ceph-cluster\nsubjects:\n- kind: ServiceAccount\n  name: rook-ceph-cluster\n  namespace: rook-ceph\n---\napiVersion: ceph.rook.io/v1beta1\nkind: Cluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).\n    # v12 is luminous, v13 is mimic, and v14 is nautilus.\n    # RECOMMENDATION: In production, use a specific version tag instead of the general v13 flag, which pulls the latest release and could result in different\n    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.\n    image: ceph/ceph:v13\n    # Whether to allow unsupported versions of Ceph. Currently only luminous and mimic are supported.\n    # After nautilus is released, Rook will be updated to support nautilus.\n    # Do not set to true in production.\n    allowUnsupported: false\n  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).\n  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.\n  # In Minikube, the &#39;/data&#39; directory is configured to persist across reboots. Use &amp;quot;/data/rook&amp;quot; in Minikube environment.\n  dataDirHostPath: /var/lib/rook\n  # The service account under which to run the daemon pods in this cluster if the default account is not sufficient (OSDs)\n  serviceAccount: rook-ceph-cluster\n  # set the amount of mons to be started\n  # count可以定义ceph-mon运行的数量，这里默认三个就行了\n  mon:\n    count: 3\n    allowMultiplePerNode: true\n  # enable the ceph dashboard for viewing cluster status\n  # 开启ceph资源面板\n  dashboard:\n    enabled: true\n    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)\n    # urlPrefix: /ceph-dashboard\n  network:\n    # toggle to use hostNetwork\n    # 使用宿主机的网络进行通讯\n    # 使用宿主机的网络貌似可以让集群外的主机挂载ceph\n    # 但是我没试过，有兴趣的兄弟可以试试改成true\n    # 反正这里只是集群内用，我就不改了\n    hostNetwork: false\n  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.\n  # The example under &#39;all&#39; would have all services scheduled on kubernetes nodes labeled with &#39;role=storage-node&#39; and\n  # tolerate taints with a key of &#39;storage-node&#39;.\n  placement:\n#    all:\n#      nodeAffinity:\n#        requiredDuringSchedulingIgnoredDuringExecution:\n#          nodeSelectorTerms:\n#          - matchExpressions:\n#            - key: role\n#              operator: In\n#              values:\n#              - storage-node\n#      podAffinity:\n#      podAntiAffinity:\n#      tolerations:\n#      - key: storage-node\n#        operator: Exists\n# The above placement information can also be specified for mon, osd, and mgr components\n#    mon:\n#    osd:\n#    mgr:\n# nodeAffinity：通过选择标签的方式，可以限制pod被调度到特定的节点上\n# 建议限制一下，为了让这几个pod不乱跑\n    mon:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: ceph-mon\n              operator: In\n              values:\n              - enabled\n    osd:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: ceph-osd\n              operator: In\n              values:\n              - enabled\n    mgr:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: ceph-mgr\n              operator: In\n              values:\n              - enabled\n  resources:\n# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory\n#    mgr:\n#      limits:\n#        cpu: &amp;quot;500m&amp;quot;\n#        memory: &amp;quot;1024Mi&amp;quot;\n#      requests:\n#        cpu: &amp;quot;500m&amp;quot;\n#        memory: &amp;quot;1024Mi&amp;quot;\n# The above example requests/limits can also be added to the mon and osd components\n#    mon:\n#    osd:\n  storage: # cluster level storage configuration and selection\n    useAllNodes: false\n    useAllDevices: false\n    deviceFilter:\n    location:\n    config:\n      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.\n      # Set the storeType explicitly only if it is required not to use the default.\n      # storeType: bluestore\n      # databaseSizeMB: &amp;quot;1024&amp;quot; # this value can be removed for environments with normal sized disks (100 GB or larger)\n      # journalSizeMB: &amp;quot;1024&amp;quot;  # this value can be removed for environments with normal sized disks (20 GB or larger)\n# Cluster level list of directories to use for storage. These values will be set for all nodes that have no `directories` set.\n#    directories:\n#    - path: /rook/storage-dir\n# Individual nodes and their config can be specified as well, but &#39;useAllNodes&#39; above must be set to false. Then, only the named\n# nodes below will be used as storage resources.  Each node&#39;s &#39;name&#39; field should match their &#39;kubernetes.io/hostname&#39; label.\n#建议磁盘配置方式如下：\n#name: 选择一个节点，节点名字为kubernetes.io/hostname的标签，也就是kubectl get nodes看到的名字\n#devices: 选择磁盘设置为OSD\n# - name: &amp;quot;sdb&amp;quot;:将/dev/sdb设置为osd\n    nodes:\n    - name: &amp;quot;kube-node1&amp;quot;\n      devices:\n      - name: &amp;quot;sdb&amp;quot;\n    - name: &amp;quot;kube-node2&amp;quot;\n      devices:\n      - name: &amp;quot;sdb&amp;quot;\n    - name: &amp;quot;kube-node3&amp;quot;\n      devices:\n      - name: &amp;quot;sdb&amp;quot;\n\n#      directories: # specific directories to use for storage can be specified for each node\n#      - path: &amp;quot;/rook/storage-dir&amp;quot;\n#      resources:\n#        limits:\n#          cpu: &amp;quot;500m&amp;quot;\n#          memory: &amp;quot;1024Mi&amp;quot;\n#        requests:\n#          cpu: &amp;quot;500m&amp;quot;\n#          memory: &amp;quot;1024Mi&amp;quot;\n#    - name: &amp;quot;172.17.4.201&amp;quot;\n#      devices: # specific devices to use for storage can be specified for each node\n#      - name: &amp;quot;sdb&amp;quot;\n#      - name: &amp;quot;sdc&amp;quot;\n#      config: # configuration can be specified at the node level which overrides the cluster level config\n#        storeType: filestore\n#    - name: &amp;quot;172.17.4.301&amp;quot;\n#      deviceFilter: &amp;quot;^sd.&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;开始部署ceph\&#34;&gt;开始部署ceph&lt;/h2&gt;\n&lt;p&gt;部署ceph&lt;br&gt;\nkubectl apply -f cluster.yaml&lt;br&gt;\ncluster会在rook-ceph这个namesapce创建资源&lt;br&gt;\n看到所有的pod都Running就行了&lt;br&gt;\n注意看一下pod分布的宿主机，跟我们打标签的主机是一致的&lt;br&gt;\nkubectl -n rook-ceph get pod -o wide&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233756856.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;切换到其他主机看一下磁盘&lt;/p&gt;\n&lt;p&gt;切换到kube-node1&lt;br&gt;\nlsblk&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233803458.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h2 id=\&#34;配置ceph-dashboard\&#34;&gt;配置ceph dashboard&lt;/h2&gt;\n&lt;p&gt;看一眼dashboard在哪个service上&lt;br&gt;\nkubectl -n rook-ceph get service&lt;br&gt;\n可以看到dashboard监听了8443端口&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233853544.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;创建个nodeport类型的service以便集群外部访问&lt;br&gt;\nkubectl apply -f dashboard-external-https.yaml&lt;/p&gt;\n&lt;p&gt;查看一下nodeport在哪个端口&lt;br&gt;\nkubectl -n rook-ceph get service&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233909644.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;找出Dashboard的登陆账号和密码&lt;br&gt;\nMGR_POD=&lt;code&gt;kubectl get pod -n rook-ceph | grep mgr | awk &#39;{print $1}&#39;&lt;/code&gt;&lt;br&gt;\nkubectl -n rook-ceph logs $MGR_POD | grep password&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233955731.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;打开浏览器输入任意一个Node的IP+nodeport端口&lt;br&gt;\n这里我的就是：https://192.168.1.2:30290&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234024005.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234065656.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h2 id=\&#34;配置ceph为storageclass\&#34;&gt;配置ceph为storageclass&lt;/h2&gt;\n&lt;p&gt;官方给了一个样本文件：storageclass.yaml&lt;br&gt;\n这个文件使用的是 RBD 块存储&lt;br&gt;\npool创建详解：https://rook.io/docs/rook/v0.8/ceph-pool-crd.html&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: ceph.rook.io/v1beta1\nkind: Pool\nmetadata:\n  #这个name就是创建成ceph pool之后的pool名字\n  name: replicapool\n  namespace: rook-ceph\nspec:\n  replicated:\n    size: 1\n  # size 池中数据的副本数,1就是不保存任何副本\n  failureDomain: osd\n  #  failureDomain：数据块的故障域，\n  #  值为host时，每个数据块将放置在不同的主机上\n  #  值为osd时，每个数据块将放置在不同的osd上\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n   name: ceph\n   # StorageClass的名字，pvc调用时填的名字\nprovisioner: ceph.rook.io/block\nparameters:\n  pool: replicapool\n  # Specify the namespace of the rook cluster from which to create volumes.\n  # If not specified, it will use `rook` as the default namespace of the cluster.\n  # This is also the namespace where the cluster will be\n  clusterNamespace: rook-ceph\n  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.\n  fstype: xfs\n# 设置回收策略默认为：Retain\nreclaimPolicy: Retain\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;创建storageclass\&#34;&gt;创建StorageClass&lt;/h2&gt;\n&lt;p&gt;kubectl apply -f storageclass.yaml&lt;br&gt;\nkubectl get storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;\nkubectl describe storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;创建个nginx pod尝试挂载&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; nginx.yaml\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nginx-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: ceph\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  selector:\n    app: nginx\n  ports: \n  - port: 80\n    name: nginx-port\n    targetPort: 80\n    protocol: TCP\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      name: nginx\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /html\n          name: http-file\n      volumes:\n      - name: http-file\n        persistentVolumeClaim:\n          claimName: nginx-pvc\nEOF\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;kubectl apply -f nginx.yaml&lt;br&gt;\n查看pv,pvc是否创建了&lt;br&gt;\nkubectl get pv,pvc&lt;/p&gt;\n&lt;p&gt;看一下nginx这个pod也运行了&lt;br&gt;\nkubectl get pod&lt;/p&gt;\n&lt;p&gt;删除这个pod,看pv是否还存在&lt;br&gt;\nkubectl delete -f nginx.yaml&lt;/p&gt;\n&lt;p&gt;kubectl get pv,pvc&lt;br&gt;\n可以看到，pod和pvc都已经被删除了，但是pv还在！！！&lt;/p&gt;\n&lt;h2 id=\&#34;添加新节点进入集群\&#34;&gt;添加新节点进入集群&lt;/h2&gt;\n&lt;p&gt;这次我们要把node4添加进集群，先打标签&lt;br&gt;\nkubectl label nodes kube-node4 ceph-osd=enabled&lt;br&gt;\n重新编辑cluster.yaml文件&lt;br&gt;\n原来的基础上添加node4的信息&lt;/p&gt;\n&lt;p&gt;cd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;\nvi cluster.yam&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234207475.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;\nkubectl apply -f cluster.yaml&lt;/p&gt;\n&lt;p&gt;盯着rook-ceph名称空间,集群会自动添加node4进来&lt;br&gt;\nkubectl -n rook-ceph get pod -o wide -w&lt;br&gt;\nkubectl -n rook-ceph get pod -o wide&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234283568.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234307875.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n去node4节点看一下磁盘&lt;br&gt;\nlsblk&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234334204.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h2 id=\&#34;删除一个节点\&#34;&gt;删除一个节点&lt;/h2&gt;\n&lt;p&gt;去掉node3的标签&lt;br&gt;\nkubectl label nodes kube-node3 ceph-osd-&lt;br&gt;\n重新编辑cluster.yaml文件&lt;br&gt;\n删除node3的信息&lt;br&gt;\ncd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;\nvi cluster.yam&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234380826.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;\nkubectl apply -f cluster.yaml&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234493936.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;kubectl -n rook-ceph get pod -o wide&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234548709.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234579591.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n最后记得删除宿主机的/var/lib/rook文件夹&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;kubernetes搭建rook-ceph&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;kubernetes搭建rook-ceph&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;},{&#34;name&#34;:&#34;rook-ceph&#34;,&#34;slug&#34;:&#34;hu0mNDfuy&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/hu0mNDfuy/&#34;},{&#34;name&#34;:&#34;ceph&#34;,&#34;slug&#34;:&#34;MkN4-Vurh-&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/MkN4-Vurh-/&#34;}],&#34;date&#34;:&#34;2019-12-12 00:00:00&#34;,&#34;dateFormat&#34;:&#34;2019-12-12&#34;,&#34;feature&#34;:&#34;http://lvelvis.github.io/post-images/kubernetes搭建rook-ceph.png&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/kubernetes搭建rook-ceph/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;13 min read&#34;,&#34;time&#34;:779000,&#34;words&#34;:2524,&#34;minutes&#34;:13},&#34;description&#34;:&#34;简介\nRook官网：https://rook.io\nRook是云原生计算基金会(CNCF)的孵化级项目.\nRook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。\n至于CEPH...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%AE%80%E4%BB%8B\&#34;&gt;简介&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%8E%AF%E5%A2%83\&#34;&gt;环境&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C\&#34;&gt;准备工作&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2operator\&#34;&gt;开始部署Operator&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%83%A8%E7%BD%B2rook-operator\&#34;&gt;部署Rook Operator&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%9F%A5%E7%9C%8Boperator%E7%9A%84%E7%8A%B6%E6%80%81\&#34;&gt;查看Operator的状态&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%BB%99%E8%8A%82%E7%82%B9%E6%89%93%E6%A0%87%E7%AD%BE\&#34;&gt;给节点打标签&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%85%8D%E7%BD%AEclusteryaml%E6%96%87%E4%BB%B6\&#34;&gt;配置cluster.yaml文件&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2ceph\&#34;&gt;开始部署ceph&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%85%8D%E7%BD%AEceph-dashboard\&#34;&gt;配置ceph dashboard&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%85%8D%E7%BD%AEceph%E4%B8%BAstorageclass\&#34;&gt;配置ceph为storageclass&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%9B%E5%BB%BAstorageclass\&#34;&gt;创建StorageClass&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9%E8%BF%9B%E5%85%A5%E9%9B%86%E7%BE%A4\&#34;&gt;添加新节点进入集群&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%A0%E9%99%A4%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9\&#34;&gt;删除一个节点&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;blockquote&gt;\n&lt;p&gt;欢迎来到我的小站呀，很高兴遇见你！🤝&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\&#34;关于本站\&#34;&gt;🏠 关于本站&lt;/h2&gt;\n&lt;p&gt;记录平时运维相关的技术:&lt;br&gt;\n1.网络&lt;br&gt;\n2.系统&lt;br&gt;\n3.虚拟化&lt;br&gt;\n4.数据库&lt;br&gt;\n5.中间件&lt;br&gt;\n...&lt;/p&gt;\n&lt;h2 id=\&#34;博主是谁\&#34;&gt;👨‍💻 博主是谁&lt;/h2&gt;\n&lt;p&gt;目前在职于巨人网络&lt;/p&gt;\n&lt;h2 id=\&#34;兴趣爱好\&#34;&gt;⛹ 兴趣爱好&lt;/h2&gt;\n&lt;p&gt;番剧 FPS射击游戏 dota&lt;/p&gt;\n&lt;h2 id=\&#34;联系我呀\&#34;&gt;📬 联系我呀&lt;/h2&gt;\n&#34;,&#34;fileName&#34;:&#34;about&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;关于&#34;,&#34;tags&#34;:[],&#34;date&#34;:&#34;2019-01-25 19:09:48&#34;,&#34;dateFormat&#34;:&#34;2019-01-25&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/about/&#34;,&#34;hideInList&#34;:true,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;1 min read&#34;,&#34;time&#34;:16000,&#34;words&#34;:77,&#34;minutes&#34;:1},&#34;description&#34;:&#34;\n欢迎来到我的小站呀，很高兴遇见你！🤝\n\n🏠 关于本站\n记录平时运维相关的技术:\n1.网络\n2.系统\n3.虚拟化\n4.数据库\n5.中间件\n...\n👨‍💻 博主是谁\n目前在职于巨人网络\n⛹ 兴趣爱好\n番剧 FPS射击游戏 dota\n\ud83d...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%85%B3%E4%BA%8E%E6%9C%AC%E7%AB%99\&#34;&gt;🏠 关于本站&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%8D%9A%E4%B8%BB%E6%98%AF%E8%B0%81\&#34;&gt;👨‍💻 博主是谁&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%85%B4%E8%B6%A3%E7%88%B1%E5%A5%BD\&#34;&gt;⛹ 兴趣爱好&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E8%81%94%E7%B3%BB%E6%88%91%E5%91%80\&#34;&gt;📬 联系我呀&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;}]";
  // var json = escape.substr(1, escape.length - 2);
  // var datas = json.split(',');
  // for (let i=0; i < datas.length; i++) {
  //   let item = datas[i];
  //   let attrs = item.split('34;:&#34')
  //   debugger
  //   console.log(datas[i])
  // }
  let escapeMap = new Map();
  escapeMap.set('&#34;', '"');
  escapeMap.set('&gt;', '>');
  escapeMap.set('&#39;', "'");
  escapeMap.set('&lt;', '<');
  escapeMap.set('&quot;', '"');
  escapeMap.set('&amp;', '&');
</script> -->

<script src="/media/js/mouse/peace.js"></script>


<script src=" /media/js/cool.js"></script>

</html>