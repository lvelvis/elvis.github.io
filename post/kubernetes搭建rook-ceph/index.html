<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
<meta name="keywords" content="lvelvis个人博客">
<meta name="description" content="时光,浓淡相宜;人心,远近相安;这就是最好的生活">
<meta name="theme-color" content="#000">
<title>kubernetes搭建rook-ceph | lvelvis</title>
<link rel="shortcut icon" href="/favicon.ico?v=1588227001433">
<link rel="stylesheet" href="/styles/main.css">
<link rel="stylesheet" href="/media/css/mist.css">

<link rel="stylesheet" href="/media/fonts/font-awesome.css">
<link
  href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Rosario:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext"
  rel="stylesheet" type="text/css">

<link href="/media/hljs/styles/androidstudio.css"
  rel="stylesheet">

<script src="/media/hljs/highlight.js"></script>
<script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.0/velocity.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.0/velocity.ui.min.js"></script>

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



  <meta name="description" content="kubernetes搭建rook-ceph" />
  <meta name="keywords" content="k8s,rook-ceph,ceph" />
</head>

<body>
  <div class="head-top-line"></div>
  <div class="header-box">
    
<div class="mist">
  <header class="header bg-color ">
    <div class="blog-header box-shadow-wrapper  " id="header">
      <div class="nav-toggle" id="nav_toggle">
        <div class="toggle-box">
          <div class="line line-top"></div>
          <div class="line line-center"></div>
          <div class="line line-bottom"></div>
        </div>
      </div>
      <div class="site-meta">       
        <div class="site-title">
          
            <a href="/" class="">
              <span class="logo-line-before">
                <i class=""></i>
              </span>
              <span class="main-title">lvelvis</span>
              <span class="logo-line-after">
                <i class=""></i>
              </span>
            </a>  
          
        </div>
        
      </div>
      <nav class="site-nav" id="site_nav">
        <ul id="nav_ul">
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/" target="_self">
                  <i class="fa fa-home"></i> 首页
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/archives/" target="_self">
                  <i class="fa fa-archive"></i> 归档
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/tags/" target="_self">
                  <i class="fa fa-tags"></i> 标签
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/post/about/" target="_self">
                  <i class="fa fa-user"></i> 关于
                </a>
              
            </li>
          
          
            
              <li class="nav-item ">
                <a href="/friends/" target="_self">
                  
                    <i class="fa fa-address-book"></i> 友情链接
                  
                </a>
              </li>
            
          
        </ul>
      </nav>
    </div>
  </header>
</div>

<script type="text/javascript"> 
 
  let showNav = true;

  let navToggle = document.querySelector('#nav_toggle'),
  siteNav = document.querySelector('#site_nav');
  
  function navClick() {
    let sideBar = document.querySelector('.sidebar');
    let navUl = document.querySelector('#nav_ul');
    navToggle.classList.toggle('nav-toggle-active');
    siteNav.classList.toggle('nav-menu-active');
    if (siteNav.classList.contains('nav-menu-active')) {
      siteNav.style = "height: " + (navUl.children.length * 42) +"px !important";
    } else {
      siteNav.style = "";
    }
  }

  navToggle.addEventListener('click',navClick);  
</script>
  </div>
  <div class="main-continer">
    
    <div
      class="section-layout mist bg-color">
      <div class="section-layout-wrapper">
        

<div class="sidebar">
  
    <div class="sidebar-box box-shadow-wrapper  right-motion" id="sidebar">
      
        <div class="post-list-sidebar">
          <div class="sidebar-title">
            <span id="tocSideBar" class="sidebar-title-item sidebar-title-active">文章目录</span>
            <span id="metaSideBar" class="sidebar-title-item">站点概览</span>
          </div>
        </div>
      
      <div class="sidebar-body mist" id="sidebar_body">
        
          
            <div class="post-side-meta" id="post_side_meta">
              
<div class="sidebar-wrapper box-shadow-wrapper ">
  <div class="sidebar-item">
    <img class="site-author-image right-motion" src="/images/avatar.png"/>
    <p class="site-author-name">lvelvis</p>
    
    <div class="site-description right-motion">
      
        <p id="binft">时光,浓淡相宜;人心,远近相安;这就是最好的生活</p>
      
    </div>
    
  </div>
  <div class="sidebar-item side-item-stat right-motion">
    <div class="sidebar-item-box">
      <a href="/archives/">
        
        <span class="site-item-stat-count">2</span>
        <span class="site-item-stat-name">文章</span>
      </a>
    </div>
    <div class="sidebar-item-box">
      <a href="">
        <span class="site-item-stat-count">4</span>
        <span class="site-item-stat-name">分类</span>
      </a>
    </div>
    <div class="sidebar-item-box">
      <a href="/tags/">
        <span class="site-item-stat-count">4</span>
        <span class="site-item-stat-name">标签</span>
      </a>
    </div>
  </div>
  
  
    <div class="sidebar-item sidebar-item-social">
      <div class="social-item">
        
          
            <a href="https://github.com/lvelvis">
              <i class="fa fa-github-alt" title="github"></i>
            </a>
          
            <a href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=421220622&amp;website=www.oicqzone.com&#34;&gt;">
              <i class="fa fa-qq" title="QQ"></i>
            </a>
          
        
        
          
            <a class="social-img" href="#">
              <img src="\media\images\custom-array-imgSocials-1588146527513-socialImg.jpg" />
              <i class="fa fa-wechat" title="weixin" ></i>
            </a>
          
        
      </div>
    </div>
  


</div>
            </div>
            <div class="post-toc sidebar-body-active" id="post_toc" style="opacity: 1;">
              <div class="toc-box right-motion">
  <div class="toc-wrapper auto-number auto"
    id="toc_wrapper">
    <ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#%E7%AE%80%E4%BB%8B">简介</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83">环境</a></li>
<li><a href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C">准备工作</a></li>
<li><a href="#%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2operator">开始部署Operator</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
</div>

<script>

  let lastTop = 0, lList = [], hList = [], postBody, lastIndex = -1;
  let active = 'active-show', activeClass = 'active-current';
  let tocWrapper = document.querySelector('#toc_wrapper');
  let tocContent = tocWrapper.children[0];
  let autoNumber = tocWrapper && tocWrapper.classList.contains('auto-number');

  function addTocNumber(elem, deep) {
    if (!elem) {
      return;
    }
    let prop = elem.__proto__;

    if (prop === HTMLUListElement.prototype) {
      for (let i = 0; i < elem.children.length; i++) {
        addTocNumber(elem.children[i], deep + (i + 1) + '.');
      }
    } else if (prop === HTMLLIElement.prototype) {
      // 保存li元素
      if (elem.children[0].__proto__ === HTMLAnchorElement.prototype) {
        lList.push(elem);
      }
      for (let i = 0; i < elem.children.length; i++) {
        let cur = elem.children[i];
        if (cur.__proto__ === HTMLAnchorElement.prototype) {
          if (autoNumber) {
            cur.text = deep + ' ' + cur.text;
          }
        } else if (cur.__proto__ === HTMLUListElement.prototype) {
          addTocNumber(cur, deep);
        }
      }
    }
  }

  function removeParentActiveClass() {
    let parents = tocContent.querySelectorAll('.' + active)
    parents.forEach(function (elem) {
      elem.classList.remove(active);
    });
  }

  function addActiveClass(index) {
    if (index >= 0 && index < hList.length) {
      lList[index].classList.add(activeClass);
    }
  }

  function removeActiveClass(index) {
    if (index >= 0 && index < hList.length) {
      lList[index].classList.remove(activeClass);
    }
  }

  function addActiveLiElemment(elem, parent) {
    if (!elem || elem === parent) {
      return;
    } else {
      if (elem.__proto__ === HTMLLIElement.prototype) {
        elem.classList.add(active);
      }
      addActiveLiElemment(elem.parentElement, parent);
    }
  }

  function showToc() {
    if (tocWrapper) {
      postBody = document.querySelector('#post_body');
      for (let i = 0; i < postBody.children.length; i++) {
        if (postBody.children[i].__proto__ === HTMLHeadingElement.prototype) {
          hList.push(postBody.children[i]);
        }
      }
      if (tocWrapper.classList.contains('compress')) {
        tocContent.classList.add('closed');
      } else if (tocWrapper.classList.contains('no_compress')) {
        tocContent.classList.add('expanded');
      } else {
        if (hList.length > 10) {
          active = 'active-hidden'
          tocContent.classList.add('closed');
        } else {
          tocContent.classList.add('expanded');
        }
      }
    }
  }

  (function () {
    // 处理不是从#一级标题开始目录
    if (tocContent.children.length === 1 && tocContent.children[0].__proto__ === HTMLLIElement.prototype) {
      let con = tocContent.children[0].children[0];
      tocContent.innerHTML = con.innerHTML;
    }
    let markdownItTOC = document.querySelector('.markdownIt-TOC');
    let innerHeight = window.innerHeight;
    markdownItTOC.style = `max-height: ${innerHeight - 80 > 0 ? innerHeight - 80 : innerHeight}px`
    addTocNumber(tocContent, '');
  })();

  document.addEventListener('scroll', function (e) {
    if (lList.length <= 0) {
      return;
    }
    let scrollTop = document.scrollingElement.scrollTop + 10;
    let dir;

    if (lastTop - scrollTop > 0) {
      dir = 'up';
    } else {
      dir = 'down';
    }

    lastTop = scrollTop;
    if (scrollTop <= 0) {
      if (lastIndex >= 0 && lastIndex < hList.length) {
        lList[lastIndex].classList.remove(activeClass);
      }
      return;
    }

    let current = 0, hasFind = false;
    for (let i = 0; i < hList.length; i++) {
      if (hList[i].offsetTop > scrollTop) {
        current = i;
        hasFind = true;
        break;
      }
    }
    if (!hasFind && scrollTop > lList[lList.length - 1].offsetTop) {
      current = hList.length - 1;
    } else {
      current--;
    }
    if (dir === 'down') {
      if (current > lastIndex) {
        addActiveClass(current);
        removeActiveClass(lastIndex)
        lastIndex = current;
        removeParentActiveClass();
        lList[current] && addActiveLiElemment(lList[current].parentElement, tocContent);
      }
    } else {
      if (current < lastIndex) {
        addActiveClass(current);
        removeActiveClass(lastIndex);
        lastIndex = current;
        removeParentActiveClass();
        lList[current] && addActiveLiElemment(lList[current].parentElement, tocContent);
      }
    }
  });


  window.addEventListener('load', function () {
    showToc();
    document.querySelector('#sidebar').style = 'display: block;';
    tocWrapper.classList.add('toc-active');
    setTimeout(function () {
      if ("createEvent" in document) {
        let evt = document.createEvent("HTMLEvents");
        evt.initEvent("scroll", false, true);
        document.dispatchEvent(evt);
      }
      else {
        document.fireEvent("scroll");
      }
    }, 500)
  })

</script>
            </div>
          
        
      </div>
    </div>
  
</div>
<script>
  const SIDEBAR_TITLE_ACTIVE = 'sidebar-title-active';
  const SIDEBAR_BODY_ACTIVE = 'sidebar-body-active';
  const SLIDE_UP_IN = 'slide-up-in';

  let sidebar = document.querySelector('#sidebar'),
  tocSideBar = document.querySelector('#tocSideBar'),
  metaSideBar = document.querySelector('#metaSideBar'),
  postToc = document.querySelector('#post_toc'),
  postSiteMeta = document.querySelector('#post_side_meta'),
  sidebarTitle = document.querySelector('.sidebar-title'),
  sidebarBody = document.querySelector('#sidebar_body');

  tocSideBar && tocSideBar.addEventListener('click', (e) => {
    toggleSidebar(e);
  });

  metaSideBar && metaSideBar.addEventListener('click', (e) => {
    toggleSidebar(e);
  });

  function toggleSidebar(e) {
    let currentTitle = document.querySelector("."+SIDEBAR_TITLE_ACTIVE);
    if (currentTitle == e.srcElement) {
      return ;
    }
    let current, showElement, hideElement;
    if (e.srcElement == metaSideBar) {
      showElement = postSiteMeta;
      hideElement = postToc;
    } else if (e.srcElement == tocSideBar){
      showElement = postToc;
      hideElement = postSiteMeta;
    }
    currentTitle.classList.remove(SIDEBAR_TITLE_ACTIVE);
    e.srcElement.classList.add(SIDEBAR_TITLE_ACTIVE);

    window.Velocity(hideElement, 'stop');
    window.Velocity(hideElement, 'transition.slideUpOut', {
      display: 'none',
      duration: 200,
      complete: function () {
        window.Velocity(showElement, 'transition.slideDownIn', {
          duration: 200
        });
      }
    })
    hideElement.classList.remove(SIDEBAR_BODY_ACTIVE);
    showElement.classList.add(SIDEBAR_BODY_ACTIVE);
  }

  postToc && postToc.addEventListener('transitionend', function() {
    this.classList.remove(SLIDE_UP_IN);
  });

  if (sidebarBody) {
    if (sidebarBody.classList.contains('pisces') || sidebarBody.classList.contains('gemini')) {
      let hasFix = false;
      let scrollEl = document.querySelector('.main-continer');
      let limitTop = document.querySelector('#nav_ul').children.length * 42 + 162;
      window.addEventListener('scroll', function(e) {
        if (document.scrollingElement.scrollTop >= limitTop) {
          if (!hasFix) {
            sidebar.classList.add('sidebar-fixed');
            hasFix = true;
          }
        } else {
          if (hasFix) {
            sidebar.classList.remove('sidebar-fixed');
            hasFix = false;
          }
        }
      });
    }
  }
  
</script>
        <div class="section-box box-shadow-wrapper">
          <div class="section bg-color post post-page">
            <header class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://lvelvis.github.io/post/kubernetes搭建rook-ceph/">
      kubernetes搭建rook-ceph
    </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span>发布于</span>
      <span>2018-12-12</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-folder-o"></i>
      <span class="pc-show">分类于</span>
      
      
      <a href="https://lvelvis.github.io/tag/Q617Y3Kh2/">
        <span>k8s</span>
      </a>、
      
      
      
      <a href="https://lvelvis.github.io/tag/hu0mNDfuy/">
        <span>rook-ceph</span>
      </a>、
      
      
      
      <a href="https://lvelvis.github.io/tag/MkN4-Vurh-/">
        <span>ceph</span>
      </a>
      
      
    </span>
    <span class="post-meta-divider">|</span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span>14分钟</span>
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span>2695<span class="pc-show">字数</span></span>
    </span>
    
    
    
    <span id="/post/kubernetes搭建rook-ceph/" data-flag-title="kubernetes搭建rook-ceph" class="meta-item pc-show leancloud_visitors">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-eye"></i>
      <span>浏览量：<span class="leancloud-visitors-count"></span></span>
    </span>
    
  </div>
</header>
            <div class="post-body next-md-body" id="post_body">
              <h3 id="简介">简介</h3>
<p>Rook官网：https://rook.io<br>
Rook是云原生计算基金会(CNCF)的孵化级项目.<br>
Rook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。<br>
至于CEPH，官网在这：https://ceph.com/<br>
ceph官方提供的helm部署，至今我没成功过，所以转向使用rook提供的方案<br>
有道笔记原文：http://note.youdao.com/noteshare?id=281719f1f0374f787effc90067e0d5ad&amp;sub=0B59EA339D4A4769B55F008D72C1A4C0</p>
<h3 id="环境">环境</h3>
<p>centos 7.5<br>
kernel 4.18.7-1.el7.elrepo.x86_64</p>
<p>docker 18.06</p>
<p>kubernetes v1.12.2<br>
kubeadm部署：<br>
网络: canal<br>
DNS: coredns<br>
集群成员：<br>
192.168.1.1 kube-master<br>
192.168.1.2 kube-node1<br>
192.168.1.3 kube-node2<br>
192.168.1.4 kube-node3<br>
192.168.1.5 kube-node4</p>
<p>所有node节点准备一块200G的磁盘：/dev/sdb<br>
kubernetes搭建rook-ceph</p>
<h3 id="准备工作">准备工作</h3>
<p>所有节点开启ip_forward</p>
<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
</code></pre>
<p>sysctl -p</p>
<h3 id="开始部署operator">开始部署Operator</h3>
<p>部署Rook Operator<br>
#无另外说明，全部操作都在master操作</p>
<pre><code>cd $HOME
git clone https://github.com/rook/rook.git

cd rook
cd cluster/examples/kubernetes/ceph
kubectl apply -f operator.yaml
</code></pre>
<p>kubernetes搭建rook-ceph</p>
<p>查看Operator的状态<br>
#执行apply之后稍等一会。<br>
#operator会在集群内的每个主机创建两个pod:rook-discover,rook-ceph-agent</p>
<p>kubectl -n rook-ceph-system get pod -o wide<br>
kubernetes搭建rook-ceph</p>
<p>给节点打标签<br>
运行ceph-mon的节点打上：ceph-mon=enabled<br>
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-mon=enabled<br>
运行ceph-osd的节点，也就是存储节点，打上：ceph-osd=enabled<br>
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-osd=enabled<br>
运行ceph-mgr的节点，打上：ceph-mgr=enabled<br>
#mgr只能支持一个节点运行，这是ceph跑k8s里的局限<br>
kubectl label nodes kube-node1 ceph-mgr=enabled<br>
配置cluster.yaml文件<br>
官方配置文件详解：https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html</p>
<p>文件中有几个地方要注意：</p>
<p>dataDirHostPath: 这个路径是会在宿主机上生成的，保存的是ceph的相关的配置文件，再重新生成集群的时候要确保这个目录为空，否则mon会无法启动<br>
useAllDevices: 使用所有的设备，建议为false，否则会把宿主机所有可用的磁盘都干掉<br>
useAllNodes：使用所有的node节点，建议为false，肯定不会用k8s集群内的所有node来搭建ceph的<br>
databaseSizeMB和journalSizeMB：当磁盘大于100G的时候，就注释这俩项就行了<br>
本次实验用到的 cluster.yaml 文件内容如下：</p>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;configmaps&quot;]
  verbs: [ &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;delete&quot; ]
---
# Allow the operator to create resources in this cluster's namespace
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster-mgmt
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-cluster-mgmt
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph-system
---
# Allow the pods in this namespace to work with configmaps
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-cluster
subjects:
- kind: ServiceAccount
  name: rook-ceph-cluster
  namespace: rook-ceph
---
apiVersion: ceph.rook.io/v1beta1
kind: Cluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
    # v12 is luminous, v13 is mimic, and v14 is nautilus.
    # RECOMMENDATION: In production, use a specific version tag instead of the general v13 flag, which pulls the latest release and could result in different
    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
    image: ceph/ceph:v13
    # Whether to allow unsupported versions of Ceph. Currently only luminous and mimic are supported.
    # After nautilus is released, Rook will be updated to support nautilus.
    # Do not set to true in production.
    allowUnsupported: false
  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).
  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
  # In Minikube, the '/data' directory is configured to persist across reboots. Use &quot;/data/rook&quot; in Minikube environment.
  dataDirHostPath: /var/lib/rook
  # The service account under which to run the daemon pods in this cluster if the default account is not sufficient (OSDs)
  serviceAccount: rook-ceph-cluster
  # set the amount of mons to be started
  # count可以定义ceph-mon运行的数量，这里默认三个就行了
  mon:
    count: 3
    allowMultiplePerNode: true
  # enable the ceph dashboard for viewing cluster status
  # 开启ceph资源面板
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
  network:
    # toggle to use hostNetwork
    # 使用宿主机的网络进行通讯
    # 使用宿主机的网络貌似可以让集群外的主机挂载ceph
    # 但是我没试过，有兴趣的兄弟可以试试改成true
    # 反正这里只是集群内用，我就不改了
    hostNetwork: false
  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
  # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
  # tolerate taints with a key of 'storage-node'.
  placement:
#    all:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: role
#              operator: In
#              values:
#              - storage-node
#      podAffinity:
#      podAntiAffinity:
#      tolerations:
#      - key: storage-node
#        operator: Exists
# The above placement information can also be specified for mon, osd, and mgr components
#    mon:
#    osd:
#    mgr:
# nodeAffinity：通过选择标签的方式，可以限制pod被调度到特定的节点上
# 建议限制一下，为了让这几个pod不乱跑
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mon
              operator: In
              values:
              - enabled
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-osd
              operator: In
              values:
              - enabled
    mgr:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mgr
              operator: In
              values:
              - enabled
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
#    mgr:
#      limits:
#        cpu: &quot;500m&quot;
#        memory: &quot;1024Mi&quot;
#      requests:
#        cpu: &quot;500m&quot;
#        memory: &quot;1024Mi&quot;
# The above example requests/limits can also be added to the mon and osd components
#    mon:
#    osd:
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    deviceFilter:
    location:
    config:
      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.
      # Set the storeType explicitly only if it is required not to use the default.
      # storeType: bluestore
      # databaseSizeMB: &quot;1024&quot; # this value can be removed for environments with normal sized disks (100 GB or larger)
      # journalSizeMB: &quot;1024&quot;  # this value can be removed for environments with normal sized disks (20 GB or larger)
# Cluster level list of directories to use for storage. These values will be set for all nodes that have no `directories` set.
#    directories:
#    - path: /rook/storage-dir
# Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
# nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
#建议磁盘配置方式如下：
#name: 选择一个节点，节点名字为kubernetes.io/hostname的标签，也就是kubectl get nodes看到的名字
#devices: 选择磁盘设置为OSD
# - name: &quot;sdb&quot;:将/dev/sdb设置为osd
    nodes:
    - name: &quot;kube-node1&quot;
      devices:
      - name: &quot;sdb&quot;
    - name: &quot;kube-node2&quot;
      devices:
      - name: &quot;sdb&quot;
    - name: &quot;kube-node3&quot;
      devices:
      - name: &quot;sdb&quot;

#      directories: # specific directories to use for storage can be specified for each node
#      - path: &quot;/rook/storage-dir&quot;
#      resources:
#        limits:
#          cpu: &quot;500m&quot;
#          memory: &quot;1024Mi&quot;
#        requests:
#          cpu: &quot;500m&quot;
#          memory: &quot;1024Mi&quot;
#    - name: &quot;172.17.4.201&quot;
#      devices: # specific devices to use for storage can be specified for each node
#      - name: &quot;sdb&quot;
#      - name: &quot;sdc&quot;
#      config: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: filestore
#    - name: &quot;172.17.4.301&quot;
#      deviceFilter: &quot;^sd.&quot;
</code></pre>
<p>开始部署ceph<br>
部署ceph<br>
kubectl apply -f cluster.yaml<br>
cluster会在rook-ceph这个namesapce创建资源<br>
盯着这个namesapce的pod你就会发现，它在按照顺序创建Pod</p>
<p>kubectl -n rook-ceph get pod -o wide  -w</p>
<p>看到所有的pod都Running就行了<br>
注意看一下pod分布的宿主机，跟我们打标签的主机是一致的</p>
<p>kubectl -n rook-ceph get pod -o wide<br>
kubernetes搭建rook-ceph<br>
kubernetes搭建rook-ceph</p>
<p>切换到其他主机看一下磁盘</p>
<p>切换到kube-node1<br>
lsblk<br>
kubernetes搭建rook-ceph</p>
<p>切换到kube-node3<br>
lsblk<br>
kubernetes搭建rook-ceph</p>
<p>配置ceph dashboard<br>
看一眼dashboard在哪个service上<br>
kubectl -n rook-ceph get service<br>
可以看到dashboard监听了8443端口<br>
kubernetes搭建rook-ceph</p>
<p>创建个nodeport类型的service以便集群外部访问<br>
kubectl apply -f dashboard-external-https.yaml</p>
<p>查看一下nodeport在哪个端口<br>
ss -tanl<br>
kubectl -n rook-ceph get service<br>
kubernetes搭建rook-ceph</p>
<p>找出Dashboard的登陆账号和密码<br>
MGR_POD=<code>kubectl get pod -n rook-ceph | grep mgr | awk '{print $1}'</code></p>
<p>kubectl -n rook-ceph logs $MGR_POD | grep password<br>
kubernetes搭建rook-ceph</p>
<p>打开浏览器输入任意一个Node的IP+nodeport端口<br>
这里我的就是：https://192.168.1.2:30290<br>
kubernetes搭建rook-ceph<br>
kubernetes搭建rook-ceph</p>
<p>配置ceph为storageclass<br>
官方给了一个样本文件：storageclass.yaml<br>
这个文件使用的是 RBD 块存储<br>
pool创建详解：https://rook.io/docs/rook/v0.8/ceph-pool-crd.html</p>
<pre><code>apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  #这个name就是创建成ceph pool之后的pool名字
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 1
  # size 池中数据的副本数,1就是不保存任何副本
  failureDomain: osd
  #  failureDomain：数据块的故障域，
  #  值为host时，每个数据块将放置在不同的主机上
  #  值为osd时，每个数据块将放置在不同的osd上
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: ceph
   # StorageClass的名字，pvc调用时填的名字
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use `rook` as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: xfs
# 设置回收策略默认为：Retain
reclaimPolicy: Retain
</code></pre>
<p>创建StorageClass<br>
kubectl apply -f storageclass.yaml<br>
kubectl get storageclasses.storage.k8s.io  -n rook-ceph<br>
kubectl describe storageclasses.storage.k8s.io  -n rook-ceph<br>
kubernetes搭建rook-ceph</p>
<p>创建个nginx pod尝试挂载</p>
<pre><code>cat &lt;&lt; EOF &gt; nginx.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: ceph

---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports: 
  - port: 80
    name: nginx-port
    targetPort: 80
    protocol: TCP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /html
          name: http-file
      volumes:
      - name: http-file
        persistentVolumeClaim:
          claimName: nginx-pvc
EOF
</code></pre>
<p>kubectl apply -f nginx.yaml<br>
查看pv,pvc是否创建了<br>
kubectl get pv,pvc</p>
<p>看一下nginx这个pod也运行了<br>
kubectl get pod<br>
kubernetes搭建rook-ceph</p>
<p>删除这个pod,看pv是否还存在<br>
kubectl delete -f nginx.yaml</p>
<p>kubectl get pv,pvc<br>
可以看到，pod和pvc都已经被删除了，但是pv还在！！！<br>
kubernetes搭建rook-ceph</p>
<p>添加新的OSD进入集群<br>
这次我们要把node4添加进集群，先打标签<br>
kubectl label nodes kube-node4 ceph-osd=enabled<br>
重新编辑cluster.yaml文件<br>
原来的基础上添加node4的信息</p>
<p>cd $HOME/rook/cluster/examples/kubernetes/ceph/<br>
vi cluster.yam<br>
kubernetes搭建rook-ceph</p>
<p>apply一下cluster.yaml文件<br>
kubectl apply -f cluster.yaml</p>
<p>盯着rook-ceph名称空间,集群会自动添加node4进来</p>
<p>kubectl -n rook-ceph get pod -o wide -w<br>
kubectl -n rook-ceph get pod -o wide<br>
kubernetes搭建rook-ceph<br>
kubernetes搭建rook-ceph<br>
kubernetes搭建rook-ceph</p>
<p>去node4节点看一下磁盘<br>
lsblk<br>
kubernetes搭建rook-ceph</p>
<p>再打开dashboard看一眼<br>
kubernetes搭建rook-ceph</p>
<p>删除一个节点<br>
去掉node3的标签<br>
kubectl label nodes kube-node3 ceph-osd-<br>
重新编辑cluster.yaml文件<br>
删除node3的信息</p>
<p>cd $HOME/rook/cluster/examples/kubernetes/ceph/<br>
vi cluster.yam<br>
kubernetes搭建rook-ceph</p>
<p>apply一下cluster.yaml文件<br>
kubectl apply -f cluster.yaml</p>
<p>盯着rook-ceph名称空间</p>
<p>kubectl -n rook-ceph get pod -o wide -w<br>
kubectl -n rook-ceph get pod -o wide</p>
<p>最后记得删除宿主机的/var/lib/rook文件夹<br>
kubernetes搭建rook-ceph<br>
kubernetes搭建rook-ceph<br>
kubernetes搭建rook-ceph<br>
kubernetes搭建rook-ceph</p>

            </div>
            <div class="post-footer">
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>本文作者：</strong>
      lvelvis
    </li>
    <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://lvelvis.github.io/post/kubernetes搭建rook-ceph/" title="kubernetes搭建rook-ceph">https://lvelvis.github.io/post/kubernetes搭建rook-ceph/</a>
    </li>
    <li class="post-copyright-license">
      <strong>版权声明： </strong>
      本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！
    </li>
  </ul>
  <div class="tags">
    
      <a href="https://lvelvis.github.io/tag/Q617Y3Kh2/"># k8s</a>
    
      <a href="https://lvelvis.github.io/tag/hu0mNDfuy/"># rook-ceph</a>
    
      <a href="https://lvelvis.github.io/tag/MkN4-Vurh-/"># ceph</a>
    
  </div>
  <div class="nav">
    <div class="nav-prev">
      
        <i class="fa fa-chevron-left"></i>
        <a class="nav-pc-next" title="kubesphere安装使用体验" href="https://lvelvis.github.io/post/kubesphere-an-zhuang-shi-yong-ti-yan/">kubesphere安装使用体验</a class="nav-pc-next">
        <a class="nav-mobile-prev" title="kubesphere安装使用体验" href="https://lvelvis.github.io/post/kubesphere-an-zhuang-shi-yong-ti-yan/">上一篇</a>
      
    </div>
    <div class="nav-next">
      
    </div>
  </div>
</div>
            
  <script src="https://cdn.jsdelivr.net/npm/valine@1.4.4/dist/Valine.min.js"></script>
<div id="vcomments" style="padding: 10px 0px 0px 0px"></div>

<style>
  .v .veditor {
    min-height: 10rem;
    background-image: url('https://upimage.alexhchu.com/2020/04/21/47eda59424daa.gif');
    background-size: contain;
    background-repeat: no-repeat;
    background-position: right;
    background-color: rgba(255, 255, 255, 0);
    resize: none;
  }

  .v .vwrap {
    border: 1px solid #000 !important;
  }

  .v .vbtn {
    padding: .4rem 1.2rem !important;
    border-color: #fff !important;
    background-color: #49b1f5 !important;
    color: #fff !important;
    font-size: .7rem !important;
  }

  .v .vcards .vcard .vh .vmeta .vat {
    padding: 0 .8rem !important;
    border: 1px solid #00c4b6 !important;
    border-radius: 5px !important;
    color: #00c4b6 !important;
  }
</style>
<script>
  new Valine({
    el: '#vcomments',
    appId: 'q6RNtDXsSlHgt7Wml6Krh1My-gzGzoHsz',
    appKey: 'XVXX46bo5q1RnjaGrExJEXOm',
    avatar: 'identicon',
    placeholder: '',
    pageSize: '10',
    lang: 'zh-cn',
    visitor: 'true' === 'true',
    highlight: 'true' === 'true',
    avatarForce: 'true' === 'true',
  });
</script>

          </div>
        </div>
      </div>
    </div>
    <div class="footer-box">
  <footer class="footer">
    <div class="copyright">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | © 2020 Theme By elvis</a>
    </div>
    <div class="poweredby">
      <script async src="https://api.ly522.com/js/jilei.pure.mini.js"></script>
<span id="jilei_container_site_pv">总访问量<span id="jilei_value_site_pv"></span>次</span>
<span class="post-meta-divider">|</span>
<span id="jilei_container_site_uv">在线人数<span id="jilei_value_site_uv"></span>人</span></p>
    </div>
  </footer>
  
  
    <div class="drawer-box left" id="drawer_box">
      <span class="muse-line muse-line-first"></span>
      <span class="muse-line muse-line-middle"></span>
      <span class="muse-line muse-line-last"></span>
    </div>
  
  <div class="mist back-to-top" id="back_to_top">
    <i class="fa fa-arrow-up"></i>
    
  </div>
  
  
    
<link rel="stylesheet" href="/media/live2d/css/live2d.css" />
<div class="box-scale">
  <div id="landlord" style="left: 5px;bottom: px;"
    data-key="">
    <canvas id="live2d" width="500" height="560" class="live2d"></canvas>
    

      <div class="message" style="opacity:0"></div>
      <div class="live_talk_input_body">
        <div class="live_talk_input_name_body">
          <input name="name" type="text" class="live_talk_name white_input" id="AIuserName" autocomplete="off"
            placeholder="你的名字" />
        </div>
        <div class="live_talk_input_text_body">
          <input name="talk" type="text" class="live_talk_talk white_input" id="AIuserText" autocomplete="off"
            placeholder="要和我聊什么呀？" />
          <button type="button" class="live_talk_send_btn" id="talk_send">发送</button>
        </div>
      </div>
      <input name="live_talk" id="live_talk" value="1" type="hidden" />
      <div class="live_ico_box">
        <div class="live_ico_item type_info" id="showInfoBtn"></div>
        <div class="live_ico_item type_talk" id="showTalkBtn"></div>
        
        <div class="live_ico_item type_music" id="musicButton"></div>
        
        <div class="live_ico_item type_youdu" id="youduButton"></div>
        <div class="live_ico_item type_quit" id="hideButton"></div>
        <input name="live_statu_val" id="live_statu_val" value="0" type="hidden" />
        <audio src="" style="display:none;" id="live2d_bgm" data-bgm="0" preload="none"></audio>
        <input id="duType" value="douqilai" type="hidden">
        
        <input name="live2dBGM" value="" type="hidden">
        
      </div>
    
  </div>
</div>
<div id="open_live2d">召唤看板娘</div>
<script src="https://libs.baidu.com/jquery/2.0.0/jquery.min.js"></script>
<script>
  var message_Path = 'https://cdn.jsdelivr.net/gh/hsxyhao/live2d.github.io@master/';
  let landlord = document.querySelector('#landlord');
  var apiKey = landlord.dataset.key;
</script>
<script type="text/javascript" src="/media/live2d/js/live2d.js"></script>
<script>
	var home_Path = document.location.protocol + '//' + window.document.location.hostname + ":" + window.document.location.port + '/';
	var userAgent = window.navigator.userAgent.toLowerCase();
	var norunAI = ["android", "iphone", "ipod", "ipad", "windows phone", "mqqbrowser", "msie", "trident/7.0"];
	var norunFlag = false;

	for (var i = 0; i < norunAI.length; i++) {
		if (userAgent.indexOf(norunAI[i]) > -1) {
			norunFlag = true;
			break;
		}
	}

	if (!window.WebGLRenderingContext) {
		norunFlag = true;
	}

	if (!norunFlag) {
		var hitFlag = false;
		var AIFadeFlag = false;
		var liveTlakTimer = null;
		var sleepTimer_ = null;
		var AITalkFlag = false;
		var talkNum = 0;
		(function () {
			function renderTip(template, context) {
				var tokenReg = /(\\)?\{([^\{\}\\]+)(\\)?\}/g;
				return template.replace(tokenReg, function (word, slash1, token, slash2) {
					if (slash1 || slash2) {
						return word.replace('\\', '');
					}
					var variables = token.replace(/\s/g, '').split('.');
					var currentObject = context;
					var i, length, variable;
					for (i = 0, length = variables.length; i < length; ++i) {
						variable = variables[i];
						currentObject = currentObject[variable];
						if (currentObject === undefined || currentObject === null) return '';
					}
					return currentObject;
				});
			}

			String.prototype.renderTip = function (context) {
				return renderTip(this, context);
			};

			var re = /x/;
			re.toString = function () {
				showMessage('哈哈，你打开了控制台，是想要看看我的秘密吗？', 5000);
				return '';
			};

			$(document).on('copy', function () {
				showMessage('你都复制了些什么呀，转载要记得加上出处哦~~', 5000);
			});

			function initTips() {
				$.ajax({
					cache: true,
					url: message_Path + 'message.json',
					dataType: "json",
					success: function (result) {
						$.each(result.mouseover, function (index, tips) {
							$(tips.selector).mouseover(function () {
								var text = tips.text;
								if (Array.isArray(tips.text)) text = tips.text[Math.floor(Math.random() * tips.text.length + 1) - 1];
								text = text.renderTip({ text: $(this).text() });
								showMessage(text, 3000);
								talkValTimer();
								clearInterval(liveTlakTimer);
								liveTlakTimer = null;
							});
							$(tips.selector).mouseout(function () {
								showHitokoto();
								if (liveTlakTimer == null) {
									liveTlakTimer = window.setInterval(function () {
										showHitokoto();
									}, 15000);
								};
							});
						});
						$.each(result.click, function (index, tips) {
							$(tips.selector).click(function () {
								if (hitFlag) {
									return false
								}
								hitFlag = true;
								setTimeout(function () {
									hitFlag = false;
								}, 8000);
								var text = tips.text;
								if (Array.isArray(tips.text)) text = tips.text[Math.floor(Math.random() * tips.text.length + 1) - 1];
								text = text.renderTip({ text: $(this).text() });
								showMessage(text, 3000);
							});
							clearInterval(liveTlakTimer);
							liveTlakTimer = null;
							if (liveTlakTimer == null) {
								liveTlakTimer = window.setInterval(function () {
									showHitokoto();
								}, 15000);
							};
						});
					}
				});
			}
			initTips();

			var text;
			if (document.referrer !== '') {
				var referrer = document.createElement('a');
				referrer.href = document.referrer;
				text = '嗨！来自 <span style="color:#0099cc;">' + referrer.hostname + '</span> 的朋友！';
				var domain = referrer.hostname.split('.')[1];
				if (domain == 'baidu') {
					text = '嗨！ 来自 百度搜索 的朋友！<br>欢迎访问<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				} else if (domain == 'so') {
					text = '嗨！ 来自 360搜索 的朋友！<br>欢迎访问<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				} else if (domain == 'google') {
					text = '嗨！ 来自 谷歌搜索 的朋友！<br>欢迎访问<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				}
			} else {
				if (window.location.href == home_Path) { //主页URL判断，需要斜杠结尾
					var now = (new Date()).getHours();
					if (now > 23 || now <= 5) {
						text = '你是夜猫子呀？这么晚还不睡觉，明天起的来嘛？';
					} else if (now > 5 && now <= 7) {
						text = '早上好！一日之计在于晨，美好的一天就要开始了！';
					} else if (now > 7 && now <= 11) {
						text = '上午好！工作顺利嘛，不要久坐，多起来走动走动哦！';
					} else if (now > 11 && now <= 14) {
						text = '中午了，工作了一个上午，现在是午餐时间！';
					} else if (now > 14 && now <= 17) {
						text = '午后很容易犯困呢，今天的运动目标完成了吗？';
					} else if (now > 17 && now <= 19) {
						text = '傍晚了！窗外夕阳的景色很美丽呢，最美不过夕阳红~~';
					} else if (now > 19 && now <= 21) {
						text = '晚上好，今天过得怎么样？';
					} else if (now > 21 && now <= 23) {
						text = '已经这么晚了呀，早点休息吧，晚安~~';
					} else {
						text = '嗨~ 快来逗我玩吧！';
					}
				} else {
					text = '欢迎阅读<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				}
			}
			showMessage(text, 12000);
		})();

		liveTlakTimer = setInterval(function () {
			showHitokoto();
		}, 15000);

		function showHitokoto() {
			if (sessionStorage.getItem("Sleepy") !== "1") {
				if (!AITalkFlag) {
					$.getJSON('https://v1.hitokoto.cn/', function (result) {
						talkValTimer();
						showMessage(result.hitokoto, 0);
					});
				}
			} else {
				hideMessage(0);
				if (sleepTimer_ == null) {
					sleepTimer_ = setInterval(function () {
						checkSleep();
					}, 200);
				}
			}
		}

		function checkSleep() {
			var sleepStatu = sessionStorage.getItem("Sleepy");
			if (sleepStatu !== '1') {
				talkValTimer();
				showMessage('你回来啦~', 0);
				clearInterval(sleepTimer_);
				sleepTimer_ = null;
			}
		}

		function showMessage(text, timeout) {
			if (Array.isArray(text)) text = text[Math.floor(Math.random() * text.length + 1) - 1];
			$('.message').stop();
			$('.message').html(text);
			$('.message').fadeTo(200, 1);
			//if (timeout === null) timeout = 5000;
			//hideMessage(timeout);
		}
		function talkValTimer() {
			$('#live_talk').val('1');
		}

		function hideMessage(timeout) {
			//$('.message').stop().css('opacity',1);
			if (timeout === null) timeout = 5000;
			$('.message').delay(timeout).fadeTo(200, 0);
		}

		function initLive2d() {
			$('#hideButton').on('click', function () {
				if (AIFadeFlag) {
					return false;
				} else {
					AIFadeFlag = true;
					localStorage.setItem("live2dhidden", "0");
					$('#landlord').fadeOut(200);
					$('#open_live2d').delay(200).fadeIn(200);
					setTimeout(function () {
						AIFadeFlag = false;
					}, 300);
				}
			});
			$('#open_live2d').on('click', function () {
				if (AIFadeFlag) {
					return false;
				} else {
					AIFadeFlag = true;
					localStorage.setItem("live2dhidden", "1");
					$('#open_live2d').fadeOut(200);
					$('#landlord').delay(200).fadeIn(200);
					setTimeout(function () {
						AIFadeFlag = false;
					}, 300);
				}
			});
			$('#youduButton').on('click', function () {
				if ($('#youduButton').hasClass('doudong')) {
					var typeIs = $('#youduButton').attr('data-type');
					$('#youduButton').removeClass('doudong');
					$('body').removeClass(typeIs);
					$('#youduButton').attr('data-type', '');
				} else {
					var duType = $('#duType').val();
					var duArr = duType.split(",");
					var dataType = duArr[Math.floor(Math.random() * duArr.length)];

					$('#youduButton').addClass('doudong');
					$('#youduButton').attr('data-type', dataType);
					$('body').addClass(dataType);
				}
			});
			if (apiKey) {
				$('#showInfoBtn').on('click', function () {
					var live_statu = $('#live_statu_val').val();
					if (live_statu == "0") {
						return
					} else {
						$('#live_statu_val').val("0");
						$('.live_talk_input_body').fadeOut(500);
						AITalkFlag = false;
						showHitokoto();
						$('#showTalkBtn').show();
						$('#showInfoBtn').hide();
					}
				});
				$('#showTalkBtn').on('click', function () {
					var live_statu = $('#live_statu_val').val();
					if (live_statu == "1") {
						return
					} else {
						$('#live_statu_val').val("1");
						$('.live_talk_input_body').fadeIn(500);
						AITalkFlag = true;
						$('#showTalkBtn').hide();
						$('#showInfoBtn').show();

					}
				});
				$('#talk_send').on('click', function () {
					var info_ = $('#AIuserText').val();
					var userid_ = $('#AIuserName').val();
					if (info_ == "") {
						showMessage('写点什么吧！', 0);
						return;
					}
					if (userid_ == "") {
						showMessage('聊之前请告诉我你的名字吧！', 0);
						return;
					}
					showMessage('思考中~', 0);
					let protocol = window.location.protocol.indexOf("s") > 0 ? "https" : "http";
					$.ajax({
						type: "get",
						url: `${protocol}://www.tuling123.com/openapi/api?key=${apiKey}&info=${info_}`,
						dataType: "json",
						success: function (res) {
							talkValTimer();
							showMessage(res.text, 0);
							$('#AIuserText').val("");
							sessionStorage.setItem("live2duser", userid_);
						},
						error: function (e) {
							talkValTimer();
							showMessage('似乎有什么错误，请和站长联系！', 0);
						}
					});
				});
			} else {
				$('#showInfoBtn').hide();
				$('#showTalkBtn').hide();
			}
			//获取音乐信息初始化
			var bgmListInfo = $('input[name=live2dBGM]');
			if (bgmListInfo.length == 0) {
				$('#musicButton').hide();
			} else {
				var bgmPlayNow = parseInt($('#live2d_bgm').attr('data-bgm'));
				var bgmPlayTime = 0;
				var live2dBGM_Num = sessionStorage.getItem("live2dBGM_Num");
				var live2dBGM_PlayTime = sessionStorage.getItem("live2dBGM_PlayTime");
				if (live2dBGM_Num) {
					if (live2dBGM_Num <= $('input[name=live2dBGM]').length - 1) {
						bgmPlayNow = parseInt(live2dBGM_Num);
					}
				}
				if (live2dBGM_PlayTime) {
					bgmPlayTime = parseInt(live2dBGM_PlayTime);
				}
				var live2dBGMSrc = bgmListInfo.eq(bgmPlayNow).val();
				$('#live2d_bgm').attr('data-bgm', bgmPlayNow);
				$('#live2d_bgm').attr('src', live2dBGMSrc);
				$('#live2d_bgm')[0].currentTime = bgmPlayTime;
				$('#live2d_bgm')[0].volume = 0.5;
				var live2dBGM_IsPlay = sessionStorage.getItem("live2dBGM_IsPlay");
				var live2dBGM_WindowClose = sessionStorage.getItem("live2dBGM_WindowClose");
				if (live2dBGM_IsPlay == '0' && live2dBGM_WindowClose == '0') {
					$('#live2d_bgm')[0].play();
					$('#musicButton').addClass('play');
				}
				sessionStorage.setItem("live2dBGM_WindowClose", '1');
				$('#musicButton').on('click', function () {
					if ($('#musicButton').hasClass('play')) {
						$('#live2d_bgm')[0].pause();
						$('#musicButton').removeClass('play');
						sessionStorage.setItem("live2dBGM_IsPlay", '1');
					} else {
						$('#live2d_bgm')[0].play();
						$('#musicButton').addClass('play');
						sessionStorage.setItem("live2dBGM_IsPlay", '0');
					}
				});
				window.onbeforeunload = function () {
					sessionStorage.setItem("live2dBGM_WindowClose", '0');
					if ($('#musicButton').hasClass('play')) {
						sessionStorage.setItem("live2dBGM_IsPlay", '0');
					}
				}
				document.getElementById('live2d_bgm').addEventListener("timeupdate", function () {
					var live2dBgmPlayTimeNow = document.getElementById('live2d_bgm').currentTime;
					sessionStorage.setItem("live2dBGM_PlayTime", live2dBgmPlayTimeNow);
				});
				document.getElementById('live2d_bgm').addEventListener("ended", function () {
					var listNow = parseInt($('#live2d_bgm').attr('data-bgm'));
					listNow++;
					if (listNow > $('input[name=live2dBGM]').length - 1) {
						listNow = 0;
					}
					var listNewSrc = $('input[name=live2dBGM]').eq(listNow).val();
					sessionStorage.setItem("live2dBGM_Num", listNow);
					$('#live2d_bgm').attr('src', listNewSrc);
					$('#live2d_bgm')[0].play();
					$('#live2d_bgm').attr('data-bgm', listNow);
				});
				document.getElementById('live2d_bgm').addEventListener("error", function () {
					$('#live2d_bgm')[0].pause();
					$('#musicButton').removeClass('play');
					showMessage('音乐似乎加载不出来了呢！', 0);
				});
			}
			//获取用户名
			var live2dUser = sessionStorage.getItem("live2duser");
			if (live2dUser !== null) {
				$('#AIuserName').val(live2dUser);
			}
			//获取位置
			var landL = sessionStorage.getItem("historywidth");
			var landB = sessionStorage.getItem("historyheight");
			if (landL == null || landB == null) {
				landL = '5px'
				landB = '0px'
			}
			$('#landlord').css('left', landL + 'px');
			$('#landlord').css('bottom', landB + 'px');
			//移动
			function getEvent() {
				return window.event || arguments.callee.caller.arguments[0];
			}
			var smcc = document.getElementById("landlord");
			var moveX = 0;
			var moveY = 0;
			var moveBottom = 0;
			var moveLeft = 0;
			var moveable = false;
			var docMouseMoveEvent = document.onmousemove;
			var docMouseUpEvent = document.onmouseup;
			smcc.onmousedown = function () {
				var ent = getEvent();
				moveable = true;
				moveX = ent.clientX;
				moveY = ent.clientY;
				var obj = smcc;
				moveBottom = parseInt(obj.style.bottom);
				moveLeft = parseInt(obj.style.left);
				if (isFirefox = navigator.userAgent.indexOf("Firefox") > 0) {
					window.getSelection().removeAllRanges();
				}
				document.onmousemove = function () {
					if (moveable) {
						var ent = getEvent();
						var x = moveLeft + ent.clientX - moveX;
						var y = moveBottom + (moveY - ent.clientY);
						obj.style.left = x + "px";
						obj.style.bottom = y + "px";
					}
				};
				document.onmouseup = function () {
					if (moveable) {
						var historywidth = obj.style.left;
						var historyheight = obj.style.bottom;
						historywidth = historywidth.replace('px', '');
						historyheight = historyheight.replace('px', '');
						sessionStorage.setItem("historywidth", historywidth);
						sessionStorage.setItem("historyheight", historyheight);
						document.onmousemove = docMouseMoveEvent;
						document.onmouseup = docMouseUpEvent;
						moveable = false;
						moveX = 0;
						moveY = 0;
						moveBottom = 0;
						moveLeft = 0;
					}
				};
			};
		}
		$(document).ready(function () {
			var AIimgSrc = [];
			let chooseLive2d = 'hijiki'
			if (chooseLive2d === 'histoire') {
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_00.png");
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_01.png");
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_02.png");
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_03.png");
			} else if (chooseLive2d === 'rem') {
				AIimgSrc.push(message_Path + "model/rem/remu2048/texture_00.png");
			} else if (chooseLive2d === 'Aoba') {
				AIimgSrc.push(message_Path + "model/Aoba/textures/texture_00.png");
			} else if (chooseLive2d === 'hijiki') {
				AIimgSrc.push(message_Path + "model/hijiki/moc/hijiki.2048/texture_00.png");
			} else if (chooseLive2d === 'tororo') {
				AIimgSrc.push(message_Path + "model/tororo/moc/tororo.2048/texture_00.png");
			}
			var images = [];
			var imgLength = AIimgSrc.length;
			var loadingNum = 0;
			for (var i = 0; i < imgLength; i++) {
				images[i] = new Image();
				images[i].src = AIimgSrc[i];
				images[i].onload = function () {
					loadingNum++;
					if (loadingNum === imgLength) {
						var live2dhidden = localStorage.getItem("live2dhidden");
						if (live2dhidden === "0") {
							setTimeout(function () {
								$('#open_live2d').fadeIn(200);
							}, 1300);
						} else {
							setTimeout(function () {
								$('#landlord').fadeIn(200);
							}, 1300);
						}
						let model = '';
						if (chooseLive2d === 'histoire') {
							model = message_Path + "model/histoire/model.json";
						} else if (chooseLive2d === 'rem') {
							model = message_Path + "model/rem/model.json";
						} else if (chooseLive2d === 'Aoba') {
							model = message_Path + "model/Aoba/model.json";
						} else if (chooseLive2d === 'hijiki') {
							model = message_Path + "model/hijiki/hijiki.model.json";
						} else if (chooseLive2d === 'tororo') {
							model = message_Path + "model/tororo/tororo.model.json";
						}
						setTimeout(function () {
							loadlive2d("live2d", model);
						}, 1000);
						initLive2d();
						images = null;
					}
				}
			}
		});
	}
</script>
  
  
</div>
<script>

  let sideBarOpen = 'sidebar-open';
  let body = document.body;
  let back2Top = document.querySelector('#back_to_top'),
  back2TopText = document.querySelector('#back_to_top_text'),
  drawerBox = document.querySelector('#drawer_box'),
  rightSideBar = document.querySelector('.sidebar'),
  viewport = document.querySelector('body');

  function scrollAnimation(currentY, targetY) {
   
    let needScrollTop = targetY - currentY
    let _currentY = currentY
    setTimeout(() => {
      const dist = Math.ceil(needScrollTop / 10)
      _currentY += dist
      window.scrollTo(_currentY, currentY)
      if (needScrollTop > 10 || needScrollTop < -10) {
        scrollAnimation(_currentY, targetY)
      } else {
        window.scrollTo(_currentY, targetY)
      }
    }, 1)
  }

  back2Top.addEventListener("click", function(e) {
    scrollAnimation(document.scrollingElement.scrollTop, 0);
    e.stopPropagation();
    return false;
  });
  
  window.addEventListener('scroll', function(e) {
    let percent = document.scrollingElement.scrollTop / (document.scrollingElement.scrollHeight - document.scrollingElement.clientHeight) * 100;
    if (percent > 1 && !back2Top.classList.contains('back-top-active')) {
      back2Top.classList.add('back-top-active');
    }
    if (percent == 0) {
      back2Top.classList.remove('back-top-active');
    }
    if (back2TopText) {
      back2TopText.textContent = Math.floor(percent);
    }
  });

  
  let hasCacu = false;
  window.onresize = function() {
    if (window.width > 991) {
      calcuHeight();
    } else {
      hasCacu = false;
    }
  }

  function calcuHeight() {
    // 动态调整站点概览高度
    if (!hasCacu && back2Top.classList.contains('pisces') || back2Top.classList.contains('gemini')) {
      let sideBar = document.querySelector('.sidebar');
      let navUl = document.querySelector('#site_nav');
      sideBar.style = 'margin-top:' + (navUl.offsetHeight + navUl.offsetTop + 15) + 'px;';
      hasCacu = true;
    }
  }
  calcuHeight();
  
  let open = false, MOTION_TIME = 300, RIGHT_MOVE_DIS = '320px';

  if (drawerBox) {
    let rightMotions = document.querySelectorAll('.right-motion');
    let right = drawerBox.classList.contains('right');

    let transitionDir = right ? "transition.slideRightIn" : "transition.slideLeftIn";

    let openProp, closeProp;
    if (right) {
      openProp = {
        paddingRight: RIGHT_MOVE_DIS 
      };
      closeProp = {
        paddingRight: '0px'
      };
    } else {
      openProp = {
        paddingLeft: RIGHT_MOVE_DIS 
      };
      closeProp = {
        paddingLeft: '0px'
      };
    }

    drawerBox.onclick = function() {
      open = !open;
      window.Velocity(rightSideBar, 'stop');
      window.Velocity(viewport, 'stop');
      window.Velocity(rightMotions, 'stop');
      if (open) {
        window.Velocity(rightSideBar, {
          width: RIGHT_MOVE_DIS
        }, {
          duration: MOTION_TIME,
          begin: function() {
            window.Velocity(rightMotions, transitionDir,{ });
          }
        })
        window.Velocity(viewport, openProp,{
          duration: MOTION_TIME
        });
      } else {
        window.Velocity(rightSideBar, {
          width: '0px'
        }, {
          duration: MOTION_TIME,
          begin: function() {
            window.Velocity(rightMotions, {
              opacity: 0
            });
          }
        })
        window.Velocity(viewport, closeProp ,{
          duration: MOTION_TIME
        });
      }
      for (let i = 0; i < drawerBox.children.length; i++) {
        drawerBox.children[i].classList.toggle('muse-line');
      }
      drawerBox.classList.toggle(sideBarOpen);
    }
  }

  // 链接跳转
  let newWindow = 'false'
  if (newWindow === 'true') {
    let links = document.querySelectorAll('.post-body a')
    links.forEach(item => {
      if (!item.classList.contains('btn')) {
        item.setAttribute("target","_blank");
      }
    })
  }
  // 代码高亮
  hljs.initHighlightingOnLoad();

</script>
    <div class="light-box" id="light_box"></div>
<script>
  let imgs = document.querySelectorAll('.post-body img');
  let lightBox = document.querySelector('#light_box');
  lightBox.addEventListener('mousedown', (e) => {
    e.preventDefault()
  })
  lightBox.addEventListener('mousewheel', (e) => {
    e.preventDefault()
  })
  let width = window.innerWidth * 0.8;
  lightBox.onclick = () => {
    let img = lightBox.querySelector('img');
    lightBox.style = '';
    img && img.remove();
  }
  imgs.forEach(item => {
    item.onclick = function (e) {
      let lightImg = document.createElement('img');
      lightImg.src = this.src;
      lightBox.style = `height: 100%; opacity: 1; background-color: rgba(0, 0, 0, 0.5);cursor: zoom-out;`;
      lightImg.style = `width: ${width}px; border: 1px solid #fff; border-radius: 2px;`;
      lightImg.onclick = function () {
        lightBox.style = '';
        this.remove();
      }
      lightBox.append(lightImg);
    }
  })
</script>
  </div>
</body>
<input hidden id="copy" />
<script>
  //拿来主义(真香)^_^，Clipboard 实现摘自掘金 https://juejin.im/post/5aefeb6e6fb9a07aa43c20af
  window.Clipboard = (function (window, document, navigator) {
    var textArea,
      copy;

    // 判断是不是ios端
    function isOS() {
      return navigator.userAgent.match(/ipad|iphone/i);
    }
    //创建文本元素
    function createTextArea(text) {
      textArea = document.createElement('textArea');
      textArea.value = text;
      textArea.style.width = 0;
      textArea.style.height = 0;
      textArea.clientHeight = 0;
      textArea.clientWidth = 0;
      document.body.appendChild(textArea);
    }
    //选择内容
    function selectText() {
      var range,
        selection;

      if (isOS()) {
        range = document.createRange();
        range.selectNodeContents(textArea);
        selection = window.getSelection();
        selection.removeAllRanges();
        selection.addRange(range);
        textArea.setSelectionRange(0, 999999);
      } else {
        textArea.select();
      }
    }

    //复制到剪贴板
    function copyToClipboard() {
      try {
        document.execCommand("Copy")
      } catch (err) {
        alert("复制错误！请手动复制！")
      }
      document.body.removeChild(textArea);
    }

    copy = function (text) {
      createTextArea(text);
      selectText();
      copyToClipboard();
    };

    return {
      copy: copy
    };
  })(window, document, navigator);

  function copyCode(e) {
    if (e.srcElement.tagName === 'SPAN' && e.srcElement.classList.contains('copy-code')) {
      let code = e.currentTarget.querySelector('code');
      var text = code.innerText;
      if (e.srcElement.textContent === '复制成功') {
        console.log('复制操作频率过高');
        return;
      }
      e.srcElement.textContent = '复制成功';
      (function (elem) {
        setTimeout(() => {
          if (elem.textContent === '复制成功') {
            elem.textContent = '复制代码'
          }
        }, 1000);
      })(e.srcElement)
      Clipboard.copy(text);
    }
  }

  let pres = document.querySelectorAll('pre');
  pres.forEach(pre => {
    let code = pre.querySelector('code');
    let copyElem = document.createElement('span');
    copyElem.classList.add('copy-code');
    copyElem.textContent = '复制代码';
    pre.appendChild(copyElem);
    pre.onclick = copyCode
  })
</script>
<script src="/media/js/motion.js"></script>

<script src="https://cdn.jsdelivr.net/gh/cferdinandi/smooth-scroll/dist/smooth-scroll.polyfills.min.js"></script>
<script>
  var scroll = new SmoothScroll('a[href*="#"]', {
    speed: 500
  });
</script>

<!-- <div class="search-mask" id="search_mask">
  <div class="search-box">
    <div class="search-title">
      <i class="fa fa-search"></i>
      <div class="input-box">
        <input type="text" placeholder="搜索">
      </div>
      <i class="fa fa-times-circle"></i>
    </div>
    <div class="result">
      
      <div class="item">
        <a class="result-title" href="https://lvelvis.github.io/post/kubesphere-an-zhuang-shi-yong-ti-yan/"" data-c="
          &lt;p&gt;最近又出来个kubesphere的工具用来管理k8s，今天特意来安装体验下；&lt;br&gt;
github地址：https://github.com/pixiake/ks-installer&lt;/p&gt;
&lt;p&gt;官方使用文档：https://kubesphere.io/docs/advanced-v2.0/zh-CN/installation/all-in-one/&lt;/p&gt;
&lt;p&gt;先放上安装效果图，UI界面还是很清爽的：&lt;br&gt;
&lt;img src=&#34;https://lvelvis.github.io/post-images/1588152934567.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;当前环境&#34;&gt;当前环境：&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;k8s集群已经安装完成，用kubesphere管理现有的k8s集群；

k8s版本为1.14

系统为centos7.6

kubesphere使用要求：

kubernetes version &amp;gt; 1.13.0

helm version &amp;gt; 2.10.0

a default storage class must be in kubernetes cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装完成后默认用户名密码：&lt;/p&gt;
&lt;p&gt;用户名：admin&lt;/p&gt;
&lt;p&gt;密码：P@88w0rd&lt;/p&gt;
&lt;h3 id=&#34;开始安装&#34;&gt;开始安装&lt;/h3&gt;
&lt;p&gt;安装步骤大概记录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create ns kubesphere-system
kubectl create ns kubesphere-monitoring-system

#访问etcd用到的secret
kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  --from-file=etcd-client-ca.crt=ca.pem  --from-file=etcd-client.crt=etcd-key.pem  --from-file=etcd-client.key=etcd.pem

#管理k8s用到的secret

kubectl -n kubesphere-system create secret generic kubesphere-ca  --from-file=ca.crt=ca.pem --from-file=ca.key=ca-key.pem

#clone好github项目，执行下面的这条命令

cd deploy
kubectl apply -f kubesphere-installer.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;执行完上面的命令，可以通过下面的命令，查看安装过程日志&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l job-name=kubesphere-installer -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -f
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看安装结果，STATUS跟下面保持一致才说明安装成功&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@ks-allinone deploy]# kubectl get pods -n kubesphere-system
NAME                                     READY   STATUS      RESTARTS   AGE
ks-account-6db466d8dc-srrwj              1/1     Running     0          149m
ks-apigateway-7d77cb9495-jzmg6           1/1     Running     0          170m
ks-apiserver-f8469fd47-b58rm             1/1     Running     0          166m
ks-console-54c849bdc9-dfkbf              1/1     Running     0          168m
ks-console-54c849bdc9-z2d5q              1/1     Running     0          168m
ks-controller-manager-569456b4cd-gngm5   1/1     Running     0          170m
ks-docs-6bbdcc9bfb-6jldz                 1/1     Running     0          3h7m
kubesphere-installer-7ph6l               0/1     Completed   1          3h11m
openldap-5c986c5bff-rzqwv                1/1     Running     0          3h25m
redis-76dc4db5dd-lv6kg                   1/1     Running     0          149m
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;安装过程出现的错误&#34;&gt;安装过程出现的错误&lt;/h3&gt;
&lt;p&gt;1.在安装的时提示metrics-server已经安装，导致安装中断；&lt;/p&gt;
&lt;p&gt;解析办法：在kubesphere-installer.yaml的configMap增加配置：metrics_server_enable: False（默认是没有的）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
data:
  ks-config.yaml: |
    kube_apiserver_host: 10.10.5.208:6443
    etcd_tls_enable: True
    etcd_endpoint_ips: 10.10.5.169,10.10.5.183,10.10.5.184
    disableMultiLogin: True
    elk_prefix: logstash
    metrics_server_enable: False
  #  local_registry: 192.168.1.2:5000
kind: ConfigMap
metadata:
  name: kubesphere-config
  namespace: kubesphere-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;增加Ingress配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubesphere
  namespace: kubesphere-system
  annotations:
    #kubernetes.io/ingress.class: traefik
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: ks.staplescn.com
    http:
      paths:
      - path:
        backend:
          serviceName: ks-console
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;访问界面：&lt;br&gt;
&lt;img src=&#34;https://lvelvis.github.io/post-images/1588153130667.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
">kubesphere安装使用体验</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="https://lvelvis.github.io/post/about/"" data-c="
          &lt;blockquote&gt;
&lt;p&gt;欢迎来到我的小站呀，很高兴遇见你！🤝&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;关于本站&#34;&gt;🏠 关于本站&lt;/h2&gt;
&lt;h2 id=&#34;博主是谁&#34;&gt;👨‍💻 博主是谁&lt;/h2&gt;
&lt;h2 id=&#34;兴趣爱好&#34;&gt;⛹ 兴趣爱好&lt;/h2&gt;
&lt;h2 id=&#34;联系我呀&#34;&gt;📬 联系我呀&lt;/h2&gt;
">关于</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="https://lvelvis.github.io/post/kubernetes搭建rook-ceph/"" data-c="
          &lt;h3 id=&#34;简介&#34;&gt;简介&lt;/h3&gt;
&lt;p&gt;Rook官网：https://rook.io&lt;br&gt;
Rook是云原生计算基金会(CNCF)的孵化级项目.&lt;br&gt;
Rook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。&lt;br&gt;
至于CEPH，官网在这：https://ceph.com/&lt;br&gt;
ceph官方提供的helm部署，至今我没成功过，所以转向使用rook提供的方案&lt;br&gt;
有道笔记原文：http://note.youdao.com/noteshare?id=281719f1f0374f787effc90067e0d5ad&amp;amp;sub=0B59EA339D4A4769B55F008D72C1A4C0&lt;/p&gt;
&lt;h3 id=&#34;环境&#34;&gt;环境&lt;/h3&gt;
&lt;p&gt;centos 7.5&lt;br&gt;
kernel 4.18.7-1.el7.elrepo.x86_64&lt;/p&gt;
&lt;p&gt;docker 18.06&lt;/p&gt;
&lt;p&gt;kubernetes v1.12.2&lt;br&gt;
kubeadm部署：&lt;br&gt;
网络: canal&lt;br&gt;
DNS: coredns&lt;br&gt;
集群成员：&lt;br&gt;
192.168.1.1 kube-master&lt;br&gt;
192.168.1.2 kube-node1&lt;br&gt;
192.168.1.3 kube-node2&lt;br&gt;
192.168.1.4 kube-node3&lt;br&gt;
192.168.1.5 kube-node4&lt;/p&gt;
&lt;p&gt;所有node节点准备一块200G的磁盘：/dev/sdb&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;h3 id=&#34;准备工作&#34;&gt;准备工作&lt;/h3&gt;
&lt;p&gt;所有节点开启ip_forward&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;sysctl -p&lt;/p&gt;
&lt;h3 id=&#34;开始部署operator&#34;&gt;开始部署Operator&lt;/h3&gt;
&lt;p&gt;部署Rook Operator&lt;br&gt;
#无另外说明，全部操作都在master操作&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd $HOME
git clone https://github.com/rook/rook.git

cd rook
cd cluster/examples/kubernetes/ceph
kubectl apply -f operator.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;查看Operator的状态&lt;br&gt;
#执行apply之后稍等一会。&lt;br&gt;
#operator会在集群内的每个主机创建两个pod:rook-discover,rook-ceph-agent&lt;/p&gt;
&lt;p&gt;kubectl -n rook-ceph-system get pod -o wide&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;给节点打标签&lt;br&gt;
运行ceph-mon的节点打上：ceph-mon=enabled&lt;br&gt;
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-mon=enabled&lt;br&gt;
运行ceph-osd的节点，也就是存储节点，打上：ceph-osd=enabled&lt;br&gt;
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-osd=enabled&lt;br&gt;
运行ceph-mgr的节点，打上：ceph-mgr=enabled&lt;br&gt;
#mgr只能支持一个节点运行，这是ceph跑k8s里的局限&lt;br&gt;
kubectl label nodes kube-node1 ceph-mgr=enabled&lt;br&gt;
配置cluster.yaml文件&lt;br&gt;
官方配置文件详解：https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html&lt;/p&gt;
&lt;p&gt;文件中有几个地方要注意：&lt;/p&gt;
&lt;p&gt;dataDirHostPath: 这个路径是会在宿主机上生成的，保存的是ceph的相关的配置文件，再重新生成集群的时候要确保这个目录为空，否则mon会无法启动&lt;br&gt;
useAllDevices: 使用所有的设备，建议为false，否则会把宿主机所有可用的磁盘都干掉&lt;br&gt;
useAllNodes：使用所有的node节点，建议为false，肯定不会用k8s集群内的所有node来搭建ceph的&lt;br&gt;
databaseSizeMB和journalSizeMB：当磁盘大于100G的时候，就注释这俩项就行了&lt;br&gt;
本次实验用到的 cluster.yaml 文件内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;configmaps&amp;quot;]
  verbs: [ &amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;delete&amp;quot; ]
---
# Allow the operator to create resources in this cluster&#39;s namespace
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster-mgmt
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-cluster-mgmt
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph-system
---
# Allow the pods in this namespace to work with configmaps
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-cluster
subjects:
- kind: ServiceAccount
  name: rook-ceph-cluster
  namespace: rook-ceph
---
apiVersion: ceph.rook.io/v1beta1
kind: Cluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
    # v12 is luminous, v13 is mimic, and v14 is nautilus.
    # RECOMMENDATION: In production, use a specific version tag instead of the general v13 flag, which pulls the latest release and could result in different
    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
    image: ceph/ceph:v13
    # Whether to allow unsupported versions of Ceph. Currently only luminous and mimic are supported.
    # After nautilus is released, Rook will be updated to support nautilus.
    # Do not set to true in production.
    allowUnsupported: false
  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).
  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
  # In Minikube, the &#39;/data&#39; directory is configured to persist across reboots. Use &amp;quot;/data/rook&amp;quot; in Minikube environment.
  dataDirHostPath: /var/lib/rook
  # The service account under which to run the daemon pods in this cluster if the default account is not sufficient (OSDs)
  serviceAccount: rook-ceph-cluster
  # set the amount of mons to be started
  # count可以定义ceph-mon运行的数量，这里默认三个就行了
  mon:
    count: 3
    allowMultiplePerNode: true
  # enable the ceph dashboard for viewing cluster status
  # 开启ceph资源面板
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
  network:
    # toggle to use hostNetwork
    # 使用宿主机的网络进行通讯
    # 使用宿主机的网络貌似可以让集群外的主机挂载ceph
    # 但是我没试过，有兴趣的兄弟可以试试改成true
    # 反正这里只是集群内用，我就不改了
    hostNetwork: false
  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
  # The example under &#39;all&#39; would have all services scheduled on kubernetes nodes labeled with &#39;role=storage-node&#39; and
  # tolerate taints with a key of &#39;storage-node&#39;.
  placement:
#    all:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: role
#              operator: In
#              values:
#              - storage-node
#      podAffinity:
#      podAntiAffinity:
#      tolerations:
#      - key: storage-node
#        operator: Exists
# The above placement information can also be specified for mon, osd, and mgr components
#    mon:
#    osd:
#    mgr:
# nodeAffinity：通过选择标签的方式，可以限制pod被调度到特定的节点上
# 建议限制一下，为了让这几个pod不乱跑
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mon
              operator: In
              values:
              - enabled
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-osd
              operator: In
              values:
              - enabled
    mgr:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mgr
              operator: In
              values:
              - enabled
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
#    mgr:
#      limits:
#        cpu: &amp;quot;500m&amp;quot;
#        memory: &amp;quot;1024Mi&amp;quot;
#      requests:
#        cpu: &amp;quot;500m&amp;quot;
#        memory: &amp;quot;1024Mi&amp;quot;
# The above example requests/limits can also be added to the mon and osd components
#    mon:
#    osd:
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    deviceFilter:
    location:
    config:
      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.
      # Set the storeType explicitly only if it is required not to use the default.
      # storeType: bluestore
      # databaseSizeMB: &amp;quot;1024&amp;quot; # this value can be removed for environments with normal sized disks (100 GB or larger)
      # journalSizeMB: &amp;quot;1024&amp;quot;  # this value can be removed for environments with normal sized disks (20 GB or larger)
# Cluster level list of directories to use for storage. These values will be set for all nodes that have no `directories` set.
#    directories:
#    - path: /rook/storage-dir
# Individual nodes and their config can be specified as well, but &#39;useAllNodes&#39; above must be set to false. Then, only the named
# nodes below will be used as storage resources.  Each node&#39;s &#39;name&#39; field should match their &#39;kubernetes.io/hostname&#39; label.
#建议磁盘配置方式如下：
#name: 选择一个节点，节点名字为kubernetes.io/hostname的标签，也就是kubectl get nodes看到的名字
#devices: 选择磁盘设置为OSD
# - name: &amp;quot;sdb&amp;quot;:将/dev/sdb设置为osd
    nodes:
    - name: &amp;quot;kube-node1&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
    - name: &amp;quot;kube-node2&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
    - name: &amp;quot;kube-node3&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;

#      directories: # specific directories to use for storage can be specified for each node
#      - path: &amp;quot;/rook/storage-dir&amp;quot;
#      resources:
#        limits:
#          cpu: &amp;quot;500m&amp;quot;
#          memory: &amp;quot;1024Mi&amp;quot;
#        requests:
#          cpu: &amp;quot;500m&amp;quot;
#          memory: &amp;quot;1024Mi&amp;quot;
#    - name: &amp;quot;172.17.4.201&amp;quot;
#      devices: # specific devices to use for storage can be specified for each node
#      - name: &amp;quot;sdb&amp;quot;
#      - name: &amp;quot;sdc&amp;quot;
#      config: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: filestore
#    - name: &amp;quot;172.17.4.301&amp;quot;
#      deviceFilter: &amp;quot;^sd.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;开始部署ceph&lt;br&gt;
部署ceph&lt;br&gt;
kubectl apply -f cluster.yaml&lt;br&gt;
cluster会在rook-ceph这个namesapce创建资源&lt;br&gt;
盯着这个namesapce的pod你就会发现，它在按照顺序创建Pod&lt;/p&gt;
&lt;p&gt;kubectl -n rook-ceph get pod -o wide  -w&lt;/p&gt;
&lt;p&gt;看到所有的pod都Running就行了&lt;br&gt;
注意看一下pod分布的宿主机，跟我们打标签的主机是一致的&lt;/p&gt;
&lt;p&gt;kubectl -n rook-ceph get pod -o wide&lt;br&gt;
kubernetes搭建rook-ceph&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;切换到其他主机看一下磁盘&lt;/p&gt;
&lt;p&gt;切换到kube-node1&lt;br&gt;
lsblk&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;切换到kube-node3&lt;br&gt;
lsblk&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;配置ceph dashboard&lt;br&gt;
看一眼dashboard在哪个service上&lt;br&gt;
kubectl -n rook-ceph get service&lt;br&gt;
可以看到dashboard监听了8443端口&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;创建个nodeport类型的service以便集群外部访问&lt;br&gt;
kubectl apply -f dashboard-external-https.yaml&lt;/p&gt;
&lt;p&gt;查看一下nodeport在哪个端口&lt;br&gt;
ss -tanl&lt;br&gt;
kubectl -n rook-ceph get service&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;找出Dashboard的登陆账号和密码&lt;br&gt;
MGR_POD=&lt;code&gt;kubectl get pod -n rook-ceph | grep mgr | awk &#39;{print $1}&#39;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;kubectl -n rook-ceph logs $MGR_POD | grep password&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;打开浏览器输入任意一个Node的IP+nodeport端口&lt;br&gt;
这里我的就是：https://192.168.1.2:30290&lt;br&gt;
kubernetes搭建rook-ceph&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;配置ceph为storageclass&lt;br&gt;
官方给了一个样本文件：storageclass.yaml&lt;br&gt;
这个文件使用的是 RBD 块存储&lt;br&gt;
pool创建详解：https://rook.io/docs/rook/v0.8/ceph-pool-crd.html&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  #这个name就是创建成ceph pool之后的pool名字
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 1
  # size 池中数据的副本数,1就是不保存任何副本
  failureDomain: osd
  #  failureDomain：数据块的故障域，
  #  值为host时，每个数据块将放置在不同的主机上
  #  值为osd时，每个数据块将放置在不同的osd上
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: ceph
   # StorageClass的名字，pvc调用时填的名字
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use `rook` as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: xfs
# 设置回收策略默认为：Retain
reclaimPolicy: Retain
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建StorageClass&lt;br&gt;
kubectl apply -f storageclass.yaml&lt;br&gt;
kubectl get storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;
kubectl describe storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;创建个nginx pod尝试挂载&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; nginx.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: ceph

---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports: 
  - port: 80
    name: nginx-port
    targetPort: 80
    protocol: TCP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /html
          name: http-file
      volumes:
      - name: http-file
        persistentVolumeClaim:
          claimName: nginx-pvc
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;kubectl apply -f nginx.yaml&lt;br&gt;
查看pv,pvc是否创建了&lt;br&gt;
kubectl get pv,pvc&lt;/p&gt;
&lt;p&gt;看一下nginx这个pod也运行了&lt;br&gt;
kubectl get pod&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;删除这个pod,看pv是否还存在&lt;br&gt;
kubectl delete -f nginx.yaml&lt;/p&gt;
&lt;p&gt;kubectl get pv,pvc&lt;br&gt;
可以看到，pod和pvc都已经被删除了，但是pv还在！！！&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;添加新的OSD进入集群&lt;br&gt;
这次我们要把node4添加进集群，先打标签&lt;br&gt;
kubectl label nodes kube-node4 ceph-osd=enabled&lt;br&gt;
重新编辑cluster.yaml文件&lt;br&gt;
原来的基础上添加node4的信息&lt;/p&gt;
&lt;p&gt;cd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;
vi cluster.yam&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;
kubectl apply -f cluster.yaml&lt;/p&gt;
&lt;p&gt;盯着rook-ceph名称空间,集群会自动添加node4进来&lt;/p&gt;
&lt;p&gt;kubectl -n rook-ceph get pod -o wide -w&lt;br&gt;
kubectl -n rook-ceph get pod -o wide&lt;br&gt;
kubernetes搭建rook-ceph&lt;br&gt;
kubernetes搭建rook-ceph&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;去node4节点看一下磁盘&lt;br&gt;
lsblk&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;再打开dashboard看一眼&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;删除一个节点&lt;br&gt;
去掉node3的标签&lt;br&gt;
kubectl label nodes kube-node3 ceph-osd-&lt;br&gt;
重新编辑cluster.yaml文件&lt;br&gt;
删除node3的信息&lt;/p&gt;
&lt;p&gt;cd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;
vi cluster.yam&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;
kubectl apply -f cluster.yaml&lt;/p&gt;
&lt;p&gt;盯着rook-ceph名称空间&lt;/p&gt;
&lt;p&gt;kubectl -n rook-ceph get pod -o wide -w&lt;br&gt;
kubectl -n rook-ceph get pod -o wide&lt;/p&gt;
&lt;p&gt;最后记得删除宿主机的/var/lib/rook文件夹&lt;br&gt;
kubernetes搭建rook-ceph&lt;br&gt;
kubernetes搭建rook-ceph&lt;br&gt;
kubernetes搭建rook-ceph&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
">kubernetes搭建rook-ceph</a>
      </div>
      
    </div>
  </div>
</div>
<script>
  // var escape = "[{&#34;content&#34;:&#34;&lt;p&gt;最近又出来个kubesphere的工具用来管理k8s，今天特意来安装体验下；&lt;br&gt;\ngithub地址：https://github.com/pixiake/ks-installer&lt;/p&gt;\n&lt;p&gt;官方使用文档：https://kubesphere.io/docs/advanced-v2.0/zh-CN/installation/all-in-one/&lt;/p&gt;\n&lt;p&gt;先放上安装效果图，UI界面还是很清爽的：&lt;br&gt;\n&lt;img src=\&#34;https://lvelvis.github.io/post-images/1588152934567.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;当前环境\&#34;&gt;当前环境：&lt;/h3&gt;\n&lt;pre&gt;&lt;code&gt;k8s集群已经安装完成，用kubesphere管理现有的k8s集群；\n\nk8s版本为1.14\n\n系统为centos7.6\n\nkubesphere使用要求：\n\nkubernetes version &amp;gt; 1.13.0\n\nhelm version &amp;gt; 2.10.0\n\na default storage class must be in kubernetes cluster\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;安装完成后默认用户名密码：&lt;/p&gt;\n&lt;p&gt;用户名：admin&lt;/p&gt;\n&lt;p&gt;密码：P@88w0rd&lt;/p&gt;\n&lt;h3 id=\&#34;开始安装\&#34;&gt;开始安装&lt;/h3&gt;\n&lt;p&gt;安装步骤大概记录：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kubectl create ns kubesphere-system\nkubectl create ns kubesphere-monitoring-system\n\n#访问etcd用到的secret\nkubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  --from-file=etcd-client-ca.crt=ca.pem  --from-file=etcd-client.crt=etcd-key.pem  --from-file=etcd-client.key=etcd.pem\n\n#管理k8s用到的secret\n\nkubectl -n kubesphere-system create secret generic kubesphere-ca  --from-file=ca.crt=ca.pem --from-file=ca.key=ca-key.pem\n\n#clone好github项目，执行下面的这条命令\n\ncd deploy\nkubectl apply -f kubesphere-installer.yaml\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;执行完上面的命令，可以通过下面的命令，查看安装过程日志&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l job-name=kubesphere-installer -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -f\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;查看安装结果，STATUS跟下面保持一致才说明安装成功&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;[root@ks-allinone deploy]# kubectl get pods -n kubesphere-system\nNAME                                     READY   STATUS      RESTARTS   AGE\nks-account-6db466d8dc-srrwj              1/1     Running     0          149m\nks-apigateway-7d77cb9495-jzmg6           1/1     Running     0          170m\nks-apiserver-f8469fd47-b58rm             1/1     Running     0          166m\nks-console-54c849bdc9-dfkbf              1/1     Running     0          168m\nks-console-54c849bdc9-z2d5q              1/1     Running     0          168m\nks-controller-manager-569456b4cd-gngm5   1/1     Running     0          170m\nks-docs-6bbdcc9bfb-6jldz                 1/1     Running     0          3h7m\nkubesphere-installer-7ph6l               0/1     Completed   1          3h11m\nopenldap-5c986c5bff-rzqwv                1/1     Running     0          3h25m\nredis-76dc4db5dd-lv6kg                   1/1     Running     0          149m\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h3 id=\&#34;安装过程出现的错误\&#34;&gt;安装过程出现的错误&lt;/h3&gt;\n&lt;p&gt;1.在安装的时提示metrics-server已经安装，导致安装中断；&lt;/p&gt;\n&lt;p&gt;解析办法：在kubesphere-installer.yaml的configMap增加配置：metrics_server_enable: False（默认是没有的）&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: v1\ndata:\n  ks-config.yaml: |\n    kube_apiserver_host: 10.10.5.208:6443\n    etcd_tls_enable: True\n    etcd_endpoint_ips: 10.10.5.169,10.10.5.183,10.10.5.184\n    disableMultiLogin: True\n    elk_prefix: logstash\n    metrics_server_enable: False\n  #  local_registry: 192.168.1.2:5000\nkind: ConfigMap\nmetadata:\n  name: kubesphere-config\n  namespace: kubesphere-system\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;增加Ingress配置：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: kubesphere\n  namespace: kubesphere-system\n  annotations:\n    #kubernetes.io/ingress.class: traefik\n    kubernetes.io/ingress.class: nginx\nspec:\n  rules:\n  - host: ks.staplescn.com\n    http:\n      paths:\n      - path:\n        backend:\n          serviceName: ks-console\n          servicePort: 80\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;访问界面：&lt;br&gt;\n&lt;img src=\&#34;https://lvelvis.github.io/post-images/1588153130667.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;kubesphere-an-zhuang-shi-yong-ti-yan&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;kubesphere安装使用体验&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;kubesphere&#34;,&#34;slug&#34;:&#34;BU7sbQs51&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;https://lvelvis.github.io/tag/BU7sbQs51/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;https://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-04-29 17:33:05&#34;,&#34;dateFormat&#34;:&#34;2020-04-29&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;https://lvelvis.github.io/post/kubesphere-an-zhuang-shi-yong-ti-yan/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;3 min read&#34;,&#34;time&#34;:176000,&#34;words&#34;:576,&#34;minutes&#34;:3},&#34;description&#34;:&#34;最近又出来个kubesphere的工具用来管理k8s，今天特意来安装体验下；\ngithub地址：https://github.com/pixiake/ks-installer\n官方使用文档：https://kubesphere.io/doc...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BD%93%E5%89%8D%E7%8E%AF%E5%A2%83\&#34;&gt;当前环境：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BC%80%E5%A7%8B%E5%AE%89%E8%A3%85\&#34;&gt;开始安装&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E5%87%BA%E7%8E%B0%E7%9A%84%E9%94%99%E8%AF%AF\&#34;&gt;安装过程出现的错误&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;blockquote&gt;\n&lt;p&gt;欢迎来到我的小站呀，很高兴遇见你！🤝&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\&#34;关于本站\&#34;&gt;🏠 关于本站&lt;/h2&gt;\n&lt;h2 id=\&#34;博主是谁\&#34;&gt;👨‍💻 博主是谁&lt;/h2&gt;\n&lt;h2 id=\&#34;兴趣爱好\&#34;&gt;⛹ 兴趣爱好&lt;/h2&gt;\n&lt;h2 id=\&#34;联系我呀\&#34;&gt;📬 联系我呀&lt;/h2&gt;\n&#34;,&#34;fileName&#34;:&#34;about&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;关于&#34;,&#34;tags&#34;:[],&#34;date&#34;:&#34;2019-01-25 19:09:48&#34;,&#34;dateFormat&#34;:&#34;2019-01-25&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;https://lvelvis.github.io/post/about/&#34;,&#34;hideInList&#34;:true,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;1 min read&#34;,&#34;time&#34;:6000,&#34;words&#34;:31,&#34;minutes&#34;:1},&#34;description&#34;:&#34;\n欢迎来到我的小站呀，很高兴遇见你！🤝\n\n🏠 关于本站\n👨‍💻 博主是谁\n⛹ 兴趣爱好\n📬 联系我呀\n...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%85%B3%E4%BA%8E%E6%9C%AC%E7%AB%99\&#34;&gt;🏠 关于本站&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%8D%9A%E4%B8%BB%E6%98%AF%E8%B0%81\&#34;&gt;👨‍💻 博主是谁&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%85%B4%E8%B6%A3%E7%88%B1%E5%A5%BD\&#34;&gt;⛹ 兴趣爱好&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E8%81%94%E7%B3%BB%E6%88%91%E5%91%80\&#34;&gt;📬 联系我呀&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h3 id=\&#34;简介\&#34;&gt;简介&lt;/h3&gt;\n&lt;p&gt;Rook官网：https://rook.io&lt;br&gt;\nRook是云原生计算基金会(CNCF)的孵化级项目.&lt;br&gt;\nRook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。&lt;br&gt;\n至于CEPH，官网在这：https://ceph.com/&lt;br&gt;\nceph官方提供的helm部署，至今我没成功过，所以转向使用rook提供的方案&lt;br&gt;\n有道笔记原文：http://note.youdao.com/noteshare?id=281719f1f0374f787effc90067e0d5ad&amp;amp;sub=0B59EA339D4A4769B55F008D72C1A4C0&lt;/p&gt;\n&lt;h3 id=\&#34;环境\&#34;&gt;环境&lt;/h3&gt;\n&lt;p&gt;centos 7.5&lt;br&gt;\nkernel 4.18.7-1.el7.elrepo.x86_64&lt;/p&gt;\n&lt;p&gt;docker 18.06&lt;/p&gt;\n&lt;p&gt;kubernetes v1.12.2&lt;br&gt;\nkubeadm部署：&lt;br&gt;\n网络: canal&lt;br&gt;\nDNS: coredns&lt;br&gt;\n集群成员：&lt;br&gt;\n192.168.1.1 kube-master&lt;br&gt;\n192.168.1.2 kube-node1&lt;br&gt;\n192.168.1.3 kube-node2&lt;br&gt;\n192.168.1.4 kube-node3&lt;br&gt;\n192.168.1.5 kube-node4&lt;/p&gt;\n&lt;p&gt;所有node节点准备一块200G的磁盘：/dev/sdb&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;h3 id=\&#34;准备工作\&#34;&gt;准备工作&lt;/h3&gt;\n&lt;p&gt;所有节点开启ip_forward&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;  /etc/sysctl.d/ceph.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;sysctl -p&lt;/p&gt;\n&lt;h3 id=\&#34;开始部署operator\&#34;&gt;开始部署Operator&lt;/h3&gt;\n&lt;p&gt;部署Rook Operator&lt;br&gt;\n#无另外说明，全部操作都在master操作&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;cd $HOME\ngit clone https://github.com/rook/rook.git\n\ncd rook\ncd cluster/examples/kubernetes/ceph\nkubectl apply -f operator.yaml\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;kubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;查看Operator的状态&lt;br&gt;\n#执行apply之后稍等一会。&lt;br&gt;\n#operator会在集群内的每个主机创建两个pod:rook-discover,rook-ceph-agent&lt;/p&gt;\n&lt;p&gt;kubectl -n rook-ceph-system get pod -o wide&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;给节点打标签&lt;br&gt;\n运行ceph-mon的节点打上：ceph-mon=enabled&lt;br&gt;\nkubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-mon=enabled&lt;br&gt;\n运行ceph-osd的节点，也就是存储节点，打上：ceph-osd=enabled&lt;br&gt;\nkubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-osd=enabled&lt;br&gt;\n运行ceph-mgr的节点，打上：ceph-mgr=enabled&lt;br&gt;\n#mgr只能支持一个节点运行，这是ceph跑k8s里的局限&lt;br&gt;\nkubectl label nodes kube-node1 ceph-mgr=enabled&lt;br&gt;\n配置cluster.yaml文件&lt;br&gt;\n官方配置文件详解：https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html&lt;/p&gt;\n&lt;p&gt;文件中有几个地方要注意：&lt;/p&gt;\n&lt;p&gt;dataDirHostPath: 这个路径是会在宿主机上生成的，保存的是ceph的相关的配置文件，再重新生成集群的时候要确保这个目录为空，否则mon会无法启动&lt;br&gt;\nuseAllDevices: 使用所有的设备，建议为false，否则会把宿主机所有可用的磁盘都干掉&lt;br&gt;\nuseAllNodes：使用所有的node节点，建议为false，肯定不会用k8s集群内的所有node来搭建ceph的&lt;br&gt;\ndatabaseSizeMB和journalSizeMB：当磁盘大于100G的时候，就注释这俩项就行了&lt;br&gt;\n本次实验用到的 cluster.yaml 文件内容如下：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: v1\nkind: Namespace\nmetadata:\n  name: rook-ceph\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: rook-ceph-cluster\n  namespace: rook-ceph\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: rook-ceph-cluster\n  namespace: rook-ceph\nrules:\n- apiGroups: [&amp;quot;&amp;quot;]\n  resources: [&amp;quot;configmaps&amp;quot;]\n  verbs: [ &amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;delete&amp;quot; ]\n---\n# Allow the operator to create resources in this cluster&#39;s namespace\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: rook-ceph-cluster-mgmt\n  namespace: rook-ceph\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: rook-ceph-cluster-mgmt\nsubjects:\n- kind: ServiceAccount\n  name: rook-ceph-system\n  namespace: rook-ceph-system\n---\n# Allow the pods in this namespace to work with configmaps\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: rook-ceph-cluster\n  namespace: rook-ceph\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: rook-ceph-cluster\nsubjects:\n- kind: ServiceAccount\n  name: rook-ceph-cluster\n  namespace: rook-ceph\n---\napiVersion: ceph.rook.io/v1beta1\nkind: Cluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).\n    # v12 is luminous, v13 is mimic, and v14 is nautilus.\n    # RECOMMENDATION: In production, use a specific version tag instead of the general v13 flag, which pulls the latest release and could result in different\n    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.\n    image: ceph/ceph:v13\n    # Whether to allow unsupported versions of Ceph. Currently only luminous and mimic are supported.\n    # After nautilus is released, Rook will be updated to support nautilus.\n    # Do not set to true in production.\n    allowUnsupported: false\n  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).\n  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.\n  # In Minikube, the &#39;/data&#39; directory is configured to persist across reboots. Use &amp;quot;/data/rook&amp;quot; in Minikube environment.\n  dataDirHostPath: /var/lib/rook\n  # The service account under which to run the daemon pods in this cluster if the default account is not sufficient (OSDs)\n  serviceAccount: rook-ceph-cluster\n  # set the amount of mons to be started\n  # count可以定义ceph-mon运行的数量，这里默认三个就行了\n  mon:\n    count: 3\n    allowMultiplePerNode: true\n  # enable the ceph dashboard for viewing cluster status\n  # 开启ceph资源面板\n  dashboard:\n    enabled: true\n    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)\n    # urlPrefix: /ceph-dashboard\n  network:\n    # toggle to use hostNetwork\n    # 使用宿主机的网络进行通讯\n    # 使用宿主机的网络貌似可以让集群外的主机挂载ceph\n    # 但是我没试过，有兴趣的兄弟可以试试改成true\n    # 反正这里只是集群内用，我就不改了\n    hostNetwork: false\n  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.\n  # The example under &#39;all&#39; would have all services scheduled on kubernetes nodes labeled with &#39;role=storage-node&#39; and\n  # tolerate taints with a key of &#39;storage-node&#39;.\n  placement:\n#    all:\n#      nodeAffinity:\n#        requiredDuringSchedulingIgnoredDuringExecution:\n#          nodeSelectorTerms:\n#          - matchExpressions:\n#            - key: role\n#              operator: In\n#              values:\n#              - storage-node\n#      podAffinity:\n#      podAntiAffinity:\n#      tolerations:\n#      - key: storage-node\n#        operator: Exists\n# The above placement information can also be specified for mon, osd, and mgr components\n#    mon:\n#    osd:\n#    mgr:\n# nodeAffinity：通过选择标签的方式，可以限制pod被调度到特定的节点上\n# 建议限制一下，为了让这几个pod不乱跑\n    mon:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: ceph-mon\n              operator: In\n              values:\n              - enabled\n    osd:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: ceph-osd\n              operator: In\n              values:\n              - enabled\n    mgr:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: ceph-mgr\n              operator: In\n              values:\n              - enabled\n  resources:\n# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory\n#    mgr:\n#      limits:\n#        cpu: &amp;quot;500m&amp;quot;\n#        memory: &amp;quot;1024Mi&amp;quot;\n#      requests:\n#        cpu: &amp;quot;500m&amp;quot;\n#        memory: &amp;quot;1024Mi&amp;quot;\n# The above example requests/limits can also be added to the mon and osd components\n#    mon:\n#    osd:\n  storage: # cluster level storage configuration and selection\n    useAllNodes: false\n    useAllDevices: false\n    deviceFilter:\n    location:\n    config:\n      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.\n      # Set the storeType explicitly only if it is required not to use the default.\n      # storeType: bluestore\n      # databaseSizeMB: &amp;quot;1024&amp;quot; # this value can be removed for environments with normal sized disks (100 GB or larger)\n      # journalSizeMB: &amp;quot;1024&amp;quot;  # this value can be removed for environments with normal sized disks (20 GB or larger)\n# Cluster level list of directories to use for storage. These values will be set for all nodes that have no `directories` set.\n#    directories:\n#    - path: /rook/storage-dir\n# Individual nodes and their config can be specified as well, but &#39;useAllNodes&#39; above must be set to false. Then, only the named\n# nodes below will be used as storage resources.  Each node&#39;s &#39;name&#39; field should match their &#39;kubernetes.io/hostname&#39; label.\n#建议磁盘配置方式如下：\n#name: 选择一个节点，节点名字为kubernetes.io/hostname的标签，也就是kubectl get nodes看到的名字\n#devices: 选择磁盘设置为OSD\n# - name: &amp;quot;sdb&amp;quot;:将/dev/sdb设置为osd\n    nodes:\n    - name: &amp;quot;kube-node1&amp;quot;\n      devices:\n      - name: &amp;quot;sdb&amp;quot;\n    - name: &amp;quot;kube-node2&amp;quot;\n      devices:\n      - name: &amp;quot;sdb&amp;quot;\n    - name: &amp;quot;kube-node3&amp;quot;\n      devices:\n      - name: &amp;quot;sdb&amp;quot;\n\n#      directories: # specific directories to use for storage can be specified for each node\n#      - path: &amp;quot;/rook/storage-dir&amp;quot;\n#      resources:\n#        limits:\n#          cpu: &amp;quot;500m&amp;quot;\n#          memory: &amp;quot;1024Mi&amp;quot;\n#        requests:\n#          cpu: &amp;quot;500m&amp;quot;\n#          memory: &amp;quot;1024Mi&amp;quot;\n#    - name: &amp;quot;172.17.4.201&amp;quot;\n#      devices: # specific devices to use for storage can be specified for each node\n#      - name: &amp;quot;sdb&amp;quot;\n#      - name: &amp;quot;sdc&amp;quot;\n#      config: # configuration can be specified at the node level which overrides the cluster level config\n#        storeType: filestore\n#    - name: &amp;quot;172.17.4.301&amp;quot;\n#      deviceFilter: &amp;quot;^sd.&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;开始部署ceph&lt;br&gt;\n部署ceph&lt;br&gt;\nkubectl apply -f cluster.yaml&lt;br&gt;\ncluster会在rook-ceph这个namesapce创建资源&lt;br&gt;\n盯着这个namesapce的pod你就会发现，它在按照顺序创建Pod&lt;/p&gt;\n&lt;p&gt;kubectl -n rook-ceph get pod -o wide  -w&lt;/p&gt;\n&lt;p&gt;看到所有的pod都Running就行了&lt;br&gt;\n注意看一下pod分布的宿主机，跟我们打标签的主机是一致的&lt;/p&gt;\n&lt;p&gt;kubectl -n rook-ceph get pod -o wide&lt;br&gt;\nkubernetes搭建rook-ceph&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;切换到其他主机看一下磁盘&lt;/p&gt;\n&lt;p&gt;切换到kube-node1&lt;br&gt;\nlsblk&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;切换到kube-node3&lt;br&gt;\nlsblk&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;配置ceph dashboard&lt;br&gt;\n看一眼dashboard在哪个service上&lt;br&gt;\nkubectl -n rook-ceph get service&lt;br&gt;\n可以看到dashboard监听了8443端口&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;创建个nodeport类型的service以便集群外部访问&lt;br&gt;\nkubectl apply -f dashboard-external-https.yaml&lt;/p&gt;\n&lt;p&gt;查看一下nodeport在哪个端口&lt;br&gt;\nss -tanl&lt;br&gt;\nkubectl -n rook-ceph get service&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;找出Dashboard的登陆账号和密码&lt;br&gt;\nMGR_POD=&lt;code&gt;kubectl get pod -n rook-ceph | grep mgr | awk &#39;{print $1}&#39;&lt;/code&gt;&lt;/p&gt;\n&lt;p&gt;kubectl -n rook-ceph logs $MGR_POD | grep password&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;打开浏览器输入任意一个Node的IP+nodeport端口&lt;br&gt;\n这里我的就是：https://192.168.1.2:30290&lt;br&gt;\nkubernetes搭建rook-ceph&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;配置ceph为storageclass&lt;br&gt;\n官方给了一个样本文件：storageclass.yaml&lt;br&gt;\n这个文件使用的是 RBD 块存储&lt;br&gt;\npool创建详解：https://rook.io/docs/rook/v0.8/ceph-pool-crd.html&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: ceph.rook.io/v1beta1\nkind: Pool\nmetadata:\n  #这个name就是创建成ceph pool之后的pool名字\n  name: replicapool\n  namespace: rook-ceph\nspec:\n  replicated:\n    size: 1\n  # size 池中数据的副本数,1就是不保存任何副本\n  failureDomain: osd\n  #  failureDomain：数据块的故障域，\n  #  值为host时，每个数据块将放置在不同的主机上\n  #  值为osd时，每个数据块将放置在不同的osd上\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n   name: ceph\n   # StorageClass的名字，pvc调用时填的名字\nprovisioner: ceph.rook.io/block\nparameters:\n  pool: replicapool\n  # Specify the namespace of the rook cluster from which to create volumes.\n  # If not specified, it will use `rook` as the default namespace of the cluster.\n  # This is also the namespace where the cluster will be\n  clusterNamespace: rook-ceph\n  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.\n  fstype: xfs\n# 设置回收策略默认为：Retain\nreclaimPolicy: Retain\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;创建StorageClass&lt;br&gt;\nkubectl apply -f storageclass.yaml&lt;br&gt;\nkubectl get storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;\nkubectl describe storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;创建个nginx pod尝试挂载&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; nginx.yaml\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nginx-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: ceph\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  selector:\n    app: nginx\n  ports: \n  - port: 80\n    name: nginx-port\n    targetPort: 80\n    protocol: TCP\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      name: nginx\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /html\n          name: http-file\n      volumes:\n      - name: http-file\n        persistentVolumeClaim:\n          claimName: nginx-pvc\nEOF\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;kubectl apply -f nginx.yaml&lt;br&gt;\n查看pv,pvc是否创建了&lt;br&gt;\nkubectl get pv,pvc&lt;/p&gt;\n&lt;p&gt;看一下nginx这个pod也运行了&lt;br&gt;\nkubectl get pod&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;删除这个pod,看pv是否还存在&lt;br&gt;\nkubectl delete -f nginx.yaml&lt;/p&gt;\n&lt;p&gt;kubectl get pv,pvc&lt;br&gt;\n可以看到，pod和pvc都已经被删除了，但是pv还在！！！&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;添加新的OSD进入集群&lt;br&gt;\n这次我们要把node4添加进集群，先打标签&lt;br&gt;\nkubectl label nodes kube-node4 ceph-osd=enabled&lt;br&gt;\n重新编辑cluster.yaml文件&lt;br&gt;\n原来的基础上添加node4的信息&lt;/p&gt;\n&lt;p&gt;cd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;\nvi cluster.yam&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;\nkubectl apply -f cluster.yaml&lt;/p&gt;\n&lt;p&gt;盯着rook-ceph名称空间,集群会自动添加node4进来&lt;/p&gt;\n&lt;p&gt;kubectl -n rook-ceph get pod -o wide -w&lt;br&gt;\nkubectl -n rook-ceph get pod -o wide&lt;br&gt;\nkubernetes搭建rook-ceph&lt;br&gt;\nkubernetes搭建rook-ceph&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;去node4节点看一下磁盘&lt;br&gt;\nlsblk&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;再打开dashboard看一眼&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;删除一个节点&lt;br&gt;\n去掉node3的标签&lt;br&gt;\nkubectl label nodes kube-node3 ceph-osd-&lt;br&gt;\n重新编辑cluster.yaml文件&lt;br&gt;\n删除node3的信息&lt;/p&gt;\n&lt;p&gt;cd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;\nvi cluster.yam&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;\nkubectl apply -f cluster.yaml&lt;/p&gt;\n&lt;p&gt;盯着rook-ceph名称空间&lt;/p&gt;\n&lt;p&gt;kubectl -n rook-ceph get pod -o wide -w&lt;br&gt;\nkubectl -n rook-ceph get pod -o wide&lt;/p&gt;\n&lt;p&gt;最后记得删除宿主机的/var/lib/rook文件夹&lt;br&gt;\nkubernetes搭建rook-ceph&lt;br&gt;\nkubernetes搭建rook-ceph&lt;br&gt;\nkubernetes搭建rook-ceph&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;kubernetes搭建rook-ceph&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;kubernetes搭建rook-ceph&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;https://lvelvis.github.io/tag/Q617Y3Kh2/&#34;},{&#34;name&#34;:&#34;rook-ceph&#34;,&#34;slug&#34;:&#34;hu0mNDfuy&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;https://lvelvis.github.io/tag/hu0mNDfuy/&#34;},{&#34;name&#34;:&#34;ceph&#34;,&#34;slug&#34;:&#34;MkN4-Vurh-&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;https://lvelvis.github.io/tag/MkN4-Vurh-/&#34;}],&#34;date&#34;:&#34;2018-12-12 00:00:00&#34;,&#34;dateFormat&#34;:&#34;2018-12-12&#34;,&#34;feature&#34;:&#34;https://lvelvis.github.io/post-images/kubernetes搭建rook-ceph.png&#34;,&#34;link&#34;:&#34;https://lvelvis.github.io/post/kubernetes搭建rook-ceph/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;14 min read&#34;,&#34;time&#34;:827000,&#34;words&#34;:2695,&#34;minutes&#34;:14},&#34;description&#34;:&#34;简介\nRook官网：https://rook.io\nRook是云原生计算基金会(CNCF)的孵化级项目.\nRook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。\n至于CEPH...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%AE%80%E4%BB%8B\&#34;&gt;简介&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%8E%AF%E5%A2%83\&#34;&gt;环境&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C\&#34;&gt;准备工作&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2operator\&#34;&gt;开始部署Operator&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;}]";
  // var json = escape.substr(1, escape.length - 2);
  // var datas = json.split(',');
  // for (let i=0; i < datas.length; i++) {
  //   let item = datas[i];
  //   let attrs = item.split('34;:&#34')
  //   debugger
  //   console.log(datas[i])
  // }
  let escapeMap = new Map();
  escapeMap.set('&#34;', '"');
  escapeMap.set('&gt;', '>');
  escapeMap.set('&#39;', "'");
  escapeMap.set('&lt;', '<');
  escapeMap.set('&quot;', '"');
  escapeMap.set('&amp;', '&');
</script> -->

<script src="/media/js/mouse/peace.js"></script>


<script src=" /media/js/cool.js"></script>

</html>