<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://lvelvis.github.io</id>
    <title>lvelvis</title>
    <updated>2021-12-20T04:04:11.103Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://lvelvis.github.io"/>
    <link rel="self" href="https://lvelvis.github.io/atom.xml"/>
    <subtitle>时光,浓淡相宜;人心,远近相安;这就是最好的生活</subtitle>
    <logo>https://lvelvis.github.io/images/avatar.png</logo>
    <icon>https://lvelvis.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, lvelvis</rights>
    <entry>
        <title type="html"><![CDATA[jenkins libaray groovy+mysql管理jenkins发布]]></title>
        <id>https://lvelvis.github.io/post/jenkins-libaray-groovymysql-guan-li-jenkins-fa-bu/</id>
        <link href="https://lvelvis.github.io/post/jenkins-libaray-groovymysql-guan-li-jenkins-fa-bu/">
        </link>
        <updated>2021-12-20T03:28:43.000Z</updated>
        <content type="html"><![CDATA[<h2 id="依赖环境">依赖环境</h2>
<pre><code>1. kubernetes 安装jenkinsx
2. jenkins kubernetes cloud配置
2.1 jenkins插件安装：在jenkins插件管理页面安装database、MySQL Database Plugin、Timestamper、ansiColor、build user vars、Groovy Postbuild、build-name-setter等
</code></pre>
<p><img src="https://lvelvis.github.io/post-images/1639971809359.png" alt="" loading="lazy"><br>
1.docker-compose管理启动mysql<br>
2.libaray groovy模块化拆分，管理jenkins发布</p>
<pre><code>2.1 钉钉推送构建成功/失败通知
2.2 支持k8s、单docker应用、物理机应用
2.3 镜像构建、推送仓库
2.4 应用版本发布、回退、服务重启
2.5 数据库数据返回json格式化
</code></pre>
<h2 id="发布配置">发布配置</h2>
<h3 id="k8s发布">k8s发布</h3>
<pre><code>#!groovy
@Library('publick8sdevops@master') 

def  app = &quot;${params.app_name}&quot;.toLowerCase()
def  project_name = &quot;${env.JOB_BASE_NAME}&quot;.toLowerCase().split('_')[0]
def  env_name = &quot;${env.JOB_BASE_NAME}&quot;.toLowerCase().split('_')[1]
def  table_name = &quot;project_test_k8s&quot;
def  branchName = &quot;${params.tags}&quot;
def  git_hash_id = &quot;${params.HashID}&quot;.substring(0,7)
def  Image_tag = &quot;${env.BUILD_NUMBER}-${branchName}-${git_hash_id}&quot;.toLowerCase()
def  Image = &quot;docker.lvelvis.com/${env_name}/${project_name}/${app}&quot;
def  imageName = &quot;${Image}:${Image_tag}&quot;.toLowerCase()
K8SDeploy(env_name,branchName,git_hash_id,app,imageName,Image,Image_tag,table_name)
</code></pre>
<h3 id="非容器发布">非容器发布</h3>
<pre><code>#!groovy
@Library('publick8sdevops@master') 

def  app = &quot;${params.app_name}&quot;.toLowerCase()
def  env_name = &quot;${env.JOB_BASE_NAME}&quot;.toLowerCase().split('_')[1]
def  table_name = &quot;project_test_k8s&quot;
def  branchName = &quot;${params.tags}&quot;
def  git_hash_id = &quot;${params.HashID}&quot;.substring(0,7)

DeployOther(env_name,branchName,git_hash_id,app,table_name)
</code></pre>
<h3 id="单机docker发布">单机docker发布</h3>
<pre><code>#!groovy
@Library('publick8sdevops@master') 

def  app = &quot;${params.app_name}&quot;.toLowerCase()
def  project_name = &quot;${env.JOB_BASE_NAME}&quot;.toLowerCase().split('_')[0]
def  env_name = &quot;${env.JOB_BASE_NAME}&quot;.toLowerCase().split('_')[1]
def  table_name = &quot;project_uat&quot;
def  branchName = &quot;${params.tags}&quot;
def  git_hash_id = &quot;${params.HashID}&quot;.substring(0,7)
def  Image_tag = &quot;${env.BUILD_NUMBER}-${branchName}-${git_hash_id}&quot;.toLowerCase()
def  Image = &quot;docker.lvelvis.com/${env_name}/${project_name}/${app}&quot;
def  imageName = &quot;${Image}:${Image_tag}&quot;.toLowerCase()

DockerDeploy(env_name,branchName,git_hash_id,app,imageName,Image,Image_tag,table_name)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clickhouse删除表分区]]></title>
        <id>https://lvelvis.github.io/post/clickhouse-shan-chu-biao-fen-qu/</id>
        <link href="https://lvelvis.github.io/post/clickhouse-shan-chu-biao-fen-qu/">
        </link>
        <updated>2020-11-04T02:19:49.000Z</updated>
        <content type="html"><![CDATA[<pre><code>ALTER 仅支持 *MergeTree ，Merge以及Distributed等引擎表。
该操作有多种形式 。
列操作 
改变表结构：  

ALTER TABLE [db].name [ON CLUSTER cluster] ADD|DROP|CLEAR|COMMENT|MODIFY COLUMN ...
在语句中，配置一个或多个用逗号分隔的动作。每个动作是对某个列实施的操作行为。
</code></pre>
<p>支持下列动作：</p>
<ul>
<li>ADD COLUMN — 添加列</li>
<li>DROP COLUMN — 删除列</li>
<li>CLEAR COLUMN — 重置列的值</li>
<li>COMMENT COLUMN — 给列增加注释说明</li>
<li>MODIFY COLUMN — 改变列的值类型，默认表达式以及TTL<br>
方法一：</li>
</ul>
<pre><code>ALTER TABLE kuming.tableName DELETE WHERE toDate(insert_at_timestamp)='2020-07-21';
</code></pre>
<p>方法二：</p>
<pre><code>ALTER TABLE kuming.tableName DELETE WHERE insert_at_timestamp&lt;=1596470399;
</code></pre>
<p>方法三：（当前两种方法分区数据没有删除掉的时候可以用方法三）</p>
<pre><code>ALTER TABLE kuming.tableName DROP PARTITION '2020-08-03';
</code></pre>
<p>然后查询本地表分区是否删除</p>
<pre><code>SELECT      partition,     name, table     active FROM system.parts where table='tableName';
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[prometheus监控数据远程存储clickhouse集群]]></title>
        <id>https://lvelvis.github.io/post/prometheus-jian-kong-shu-ju-yuan-cheng-cun-chu-clickhouse-ji-qun/</id>
        <link href="https://lvelvis.github.io/post/prometheus-jian-kong-shu-ju-yuan-cheng-cun-chu-clickhouse-ji-qun/">
        </link>
        <updated>2020-10-28T07:36:32.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前提">前提</h2>
<pre><code>Prometheus的本地存储设计可以减少其自身运维和管理的复杂度，同时能够满足大部分用户监控规模的需求。但是本地存储也意味着Prometheus无法持久化数据，无法存储大量历史数据(TSDB存储大量数据，会导致程序启动加载占用过多内存)，同时也无法灵活扩展和迁移。
为了保持Prometheus的简单性，Prometheus并没有尝试在自身中解决以上问题，而是通过定义两个标准接口(remote_write/remote_read)，让用户可以基于这两个接口对接将数据保存到任意第三方的存储服务中，这种方式在Promthues中称为Remote Storage。
</code></pre>
<p>目前Prometheus社区也提供了部分对于第三方数据库的Remote Storage支持：</p>
<table>
<thead>
<tr>
<th>存储服务</th>
<th>支持模式</th>
</tr>
</thead>
<tbody>
<tr>
<td>AppOptics</td>
<td><code>write</code></td>
</tr>
<tr>
<td>Chronix</td>
<td><code>write</code></td>
</tr>
<tr>
<td>Cortex</td>
<td><code>read/write</code></td>
</tr>
<tr>
<td>CrateDB</td>
<td><code>read/write</code></td>
</tr>
<tr>
<td>Gnocchi</td>
<td><code>write</code></td>
</tr>
<tr>
<td>Graphite</td>
<td><code>write</code></td>
</tr>
<tr>
<td>InfluxDB</td>
<td><code>read/write</code></td>
</tr>
<tr>
<td>OpenTSDB</td>
<td><code>write</code></td>
</tr>
<tr>
<td>PostgreSQL/TimescaleDB</td>
<td><code>read/write</code></td>
</tr>
<tr>
<td>SignalFx</td>
<td><code>write</code></td>
</tr>
</tbody>
</table>
<h2 id="remote-write">Remote Write</h2>
<p><img src="https://lvelvis.github.io/post-images/1603871818190.png" alt="" loading="lazy"><br>
用户可以在Prometheus配置文件中指定Remote Write(远程写)的URL地址，一旦设置了该配置项，Prometheus将采集到的样本数据通过HTTP的形式发送给适配器(Adaptor)。而用户则可以在适配器中对接外部任意的服务。外部服务可以是真正的存储系统，公有云的存储服务，也可以是消息队列等任意形式。</p>
<ul>
<li>可同时启用配置TSDB存储1一天数据，其他数据使用远程存储；查询近1天数据，从tsdb获取，其他数据从远程存储获取</li>
</ul>
<h2 id="remote-read">Remote Read</h2>
<p><img src="https://lvelvis.github.io/post-images/1603872148839.png" alt="" loading="lazy"><br>
如下图所示，Promthues的Remote Read(远程读)也通过了一个适配器实现。在远程读的流程当中，当用户发起查询请求后，Promthues将向remote_read中配置的URL发起查询请求(matchers,ranges)，Adaptor根据请求条件从第三方存储服务中获取响应的数据。同时将数据转换为Promthues的原始样本数据返回给Prometheus Server。</p>
<p>当获取到样本数据后，Promthues在本地使用PromQL对样本数据进行二次处理。</p>
<h2 id="prom2click适配器">prom2click适配器</h2>
<pre><code>git clone https://github.com/lvelvis/prom2click.git
</code></pre>
<ul>
<li>prometheus-operator已测试，可正常获取数据</li>
<li>修改部分代码解决部分监控项无法获取数据</li>
<li>读写数据需指定为分布式表(官方不建议写入分布式表)，或者配置2个write地址，每个地址落在每个分片上</li>
<li>修改为按天分区</li>
</ul>
<h3 id="prom2click流程图">prom2click流程图</h3>
<p><img src="https://lvelvis.github.io/post-images/1603872379113.png" alt="" loading="lazy"><br>
*prom2click与clickhouse的连接使用的长链接；因此需要多配置几个prom2click服务(默认使用配置-ch.dsn第一个clickhoue节点)，均衡clickhouse查询压力。</p>
<h2 id="clickhouse-环境">clickhouse 环境</h2>
<pre><code>clickhouse-server --version
ClickHouse server version 20.3.8.53.
</code></pre>
<p>使用4个节点+3zk 2shards_2replicas</p>
<h2 id="配置方式">配置方式</h2>
<h3 id="官方promtheus部署方式promtheusyml增加以下参数">官方promtheus部署方式，promtheus.yml增加以下参数</h3>
<pre><code>remote_write:
    - url: &quot;http://localhost:9201/write&quot;
remote_read:
    - url: &quot;http://localhost:9202/read&quot;
    - url: &quot;http://localhost:9203/read&quot;
</code></pre>
<h3 id="promtheus-operatorkube-prometheus-manifests">promtheus-operator(kube-prometheus-manifests)</h3>
<p>prometheus/prometheus-prometheus.yaml</p>
<pre><code>apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  labels:
    prometheus: k8s
  name: k8s
  namespace: monitoring
spec:
  retention: 1d
  remoteWrite: 
    - url: &quot;http://192.168.101.240:9201/write&quot;
  remoteRead: 
    - url: &quot;http://192.168.101.241:9201/read&quot;
    - url: &quot;http://192.168.101.242:9201/read&quot;
    - url: &quot;http://192.168.101.243:9201/read&quot;
  alerting:
    alertmanagers:
    - name: alertmanager-main
      namespace: monitoring
      port: web
  image: lvcisco/prometheus:v2.15.2
  nodeSelector:
    kubernetes.io/os: linux
  podMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  replicas: 2
  resources:
    requests:
      cpu: &quot;2&quot;
      memory: 1Gi
    limits:
      cpu: &quot;4&quot;
      #memory: 8Gi
  ruleSelector:
    matchLabels:
      prometheus: k8s
      role: alert-rules
  securityContext:
    fsGroup: 2000
</code></pre>
<pre><code>kubectl replace -f  prometheus/prometheus-prometheus.yaml 
</code></pre>
<p>然后查看promtheus容器配置是否生效</p>
<pre><code>kubectl  -n monitoring exec  prometheus-k8s-0 tail  /etc/prometheus/config_out/prometheus.env.yaml

remote_write:
- url: http://192.168.101.240:9201/write
  remote_timeout: 30s
remote_read:
- url: http://192.168.101.241:9201/read
  remote_timeout: 30s
- url: http://192.168.101.242:9201/read
  remote_timeout: 30s
- url: http://192.168.101.243:9201/read
  remote_timeout: 30s
</code></pre>
<ul>
<li>更多参数配置查看<a href="https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotereadspec">远程存储配置</a></li>
<li><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write">其他配置</a></li>
</ul>
<h2 id="测试">测试</h2>
<p>可删除tsdb数据，需重启prometheus-k8s容器，再次查询grafana是否有历史数据</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka数据导入到Clickhouse]]></title>
        <id>https://lvelvis.github.io/post/kafka-shu-ju-dao-ru-dao-clickhouse/</id>
        <link href="https://lvelvis.github.io/post/kafka-shu-ju-dao-ru-dao-clickhouse/">
        </link>
        <updated>2020-10-28T03:26:38.000Z</updated>
        <content type="html"><![CDATA[<pre><code>Kafka 是目前应用非常广泛的开源消息中间件，一个常用的的场景就是做数据总线收集各个服务的消息日志，下游各种数据服务订阅消费数据，生成各种报表或数据应用等。Clickhouse 的自带了 Kafka Engine，使得 Clickhouse 和 Kafka 的集成变得非常容易。
</code></pre>
<h2 id="创建-kafka-表">创建 Kafka 表</h2>
<p>Clickhouse 的 Kafka Engine 可以将 Kafka 中的流映射成一个表，方便我们的后续处理。只要建表的时候制定</p>
<pre><code>Kafka(broker_list, topic_list, group_name, format[, schema])
broker_list: 逗号分隔的 Kafka broker 列表
topic_list: 消费的topic
group_name: consumer group 的id， 同一个 group_name 的 clickhouse 会在同一个 consumer group 消费数据
format: kafka 消息的格式
</code></pre>
<p>在前文的所述的3节点 clickhouse 集群上，在每一个节点都建一个 Kafka Engine 的表从 kafka 的events topic读数据。</p>
<pre><code>CREATE TABLE event_stream (ts UInt64, tag String, cnt Int64, val Double) 
ENGINE = Kafka('127.0.0.1:9092', 'events', 'group1', 'JSONEachRow');
</code></pre>
<h2 id="写入数据">写入数据</h2>
<p>现在我们试着往 kafka 写一点json数据</p>
<pre><code>bin/kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic events
&gt;{&quot;ts&quot;:1515897449,&quot;tag&quot;:&quot;aa&quot;,&quot;cnt&quot;:3,&quot;val&quot;:0.7}
&gt;{&quot;ts&quot;:1515897450,&quot;tag&quot;:&quot;bb&quot;,&quot;cnt&quot;:9,&quot;val&quot;:0.28}
&gt;{&quot;ts&quot;:1515897451,&quot;tag&quot;:&quot;cc&quot;,&quot;cnt&quot;:7,&quot;val&quot;:0.93}
&gt;{&quot;ts&quot;:1515897452,&quot;tag&quot;:&quot;dd&quot;,&quot;cnt&quot;:1,&quot;val&quot;:0.78}
</code></pre>
<p>然后在每个 clickhose 节点中查看数据。</p>
<pre><code>┌─────────ts─┬─tag─┬─cnt─┬────────────────val─┐
│ 1515897449 │ aa  │   3 │ 0.7000000000000001 │
└────────────┴─────┴─────┴────────────────────┘
┌─────────ts─┬─tag─┬─cnt─┬──val─┐
│ 1515897452 │ dd  │   1 │ 0.78 │
└────────────┴─────┴─────┴──────┘
</code></pre>
<p>注意的是由于一个kafka的partition 只能由一个 group consumer 消费，所以clickhouse 节点数需要大于 topic 的 partition 数。</p>
<p>由于 Kafka 表只是 kafka 流的一个视图而已，当数据被 select 了一次之后，这个数据就会被认为已经消费了，下次 select 就不会再出现。所以Kafka表单独使用是没什么用的，一般是用来和 MaterialView 配合，将Kafka表里面的数据自动导入到 MaterialView 里面。</p>
<h2 id="与-materialview-集成">与 MaterialView 集成</h2>
<p>我们现在每一节点建一个 MaterialView 保存 Kafka 里面的数据, 再顺手建一个全局的Distributed表。</p>
<pre><code>CREATE MATERIALIZED VIEW events ENGINE = MergeTree(day, (day,ts, tag, cnt, val), 8192) AS
SELECT toDate(toDateTime(ts)) AS day, ts, tag, cnt, val FROM event_stream;

CREATE TABLE events_all AS events
ENGINE = Distributed(perftest_3shards_1replicas, default, events, rand());
</code></pre>
<p>再往Kafka里面写些数据，就能在各个节点的 events 或 events_all 里面查出来了。</p>
<h2 id="总结">总结</h2>
<p>clichouse 和 Kafka的配合可以说是十分的便利，只有配置好，clickhouse 从 kafka 读数据和写入都是如此的方便。不过还是有相当的局限性，因为目前对 kafka 数据格式的支持还是有限。如果能通过插件之类的扩展方式自定义format就好了。另外，clickhouse 是否保证数据的一致性，如何去重？如何保证数据不丢失？也可以通过与flink结合，从而保证数据的一致性。详细的情况还需要进一步测试</p>
<h2 id="参考资料">参考资料</h2>
<p>https://clickhouse.yandex/tutorial.html<br>
https://clickhouse.yandex/docs/en/table_engines/kafka.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[clickhouse基本操作]]></title>
        <id>https://lvelvis.github.io/post/clickhouse-ji-ben-cao-zuo/</id>
        <link href="https://lvelvis.github.io/post/clickhouse-ji-ben-cao-zuo/">
        </link>
        <updated>2020-10-27T04:09:31.000Z</updated>
        <content type="html"><![CDATA[<h2 id="一-先来说一下clickhouse为啥快">一、先来说一下，ClickHouse为啥快</h2>
<figure data-type="image" tabindex="1"><img src="https://lvelvis.github.io/post-images/1603856102146.gif" alt="" loading="lazy"></figure>
<pre><code>ClickHouse有多少CPU，吃多少资源，所以飞快；
ClickHouse不支持事务，不存在隔离级别。这里要额外说一下，有人觉得，你一个数据库都不支持事务，不支持ACID还玩个毛。ClickHouse的定位是分析性数据库，而不是严格的关系型数据库。又有人要问了，数据都不一致，统计个毛。举个例子，汽车的油表是100%准确么？为了获得一个100%准确的值，难道每次测量你都要停车检查么？统计数据的意义在于用大量的数据看规律，看趋势，而不是100%准确。
IO方面，MySQL是行存储，ClickHouse是列存储，后者在count()这类操作天然有优势，同时，在IO方面，MySQL需要大量随机IO，ClickHouse基本是顺序IO。
有人可能觉得上面的数据导入的时候，数据肯定缓存在内存里了，这个的确，但是ClickHouse基本上是顺序IO，用过就知道了，对IO基本没有太高要求，当然，磁盘越快，上层处理越快，但是99%的情况是，CPU先跑满了（数据库里太少见了，大多数都是IO不够用）
</code></pre>
<h2 id="二-创建库">二、创建库</h2>
<p>CREATE/ATTACH DATABASE zabbix ENGINE = Ordinary;<br>
ATTACH 也可以建库，但是metadata目录下不会生成.sql文件，一般用于metadata元数据sql文件被删除后，恢复库表结构使用</p>
<h2 id="三-创建本地表">三、创建本地表</h2>
<p>CREATE TABLE test02( id UInt16,col1 String,col2 String,create_date date ) ENGINE = MergeTree(create_date, (id), 8192);<br>
ENGINE：是表的引擎类型，<br>
MergeTree：最常用的，MergeTree要求有一个日期字段，还有主键。<br>
Log引擎没有这个限制，也是比较常用。<br>
ReplicatedMergeTree：MergeTree的分支，表复制引擎。<br>
Distributed：分布式引擎。<br>
create_date：是表的日期字段，一个表必须要有一个日期字段。<br>
id：是表的主键，主键可以有多个字段，每个字段用逗号分隔。<br>
8192：是索引粒度，用默认值8192即可。</p>
<h2 id="四-创建分布式表">四、创建分布式表</h2>
<p>CREATE TABLE distributed_table AS table ENGINE = Distributed(cluster, db, table, rand());<br>
cluster：配置文件中的群集名称。<br>
db：库名。<br>
table：本地表名。<br>
rand()：分片方式：随机。<br>
intHash64():分片方式：指定字段做hash。<br>
Distribute引擎会选择每个分发到的Shard中的”健康的”副本执行SQL<br>
<img src="https://lvelvis.github.io/post-images/1603771933311.jpg" alt="" loading="lazy"></p>
<h2 id="五-ddl">五、DDL</h2>
<p>如果想按集群操作，需要借助zookeeper，在config.xml中添加配置<br>
&lt;distributed_ddl&gt;<br>
<path>/clickhouse/task_queue/ddl</path><br>
&lt;/distributed_ddl&gt;<br>
一个节点创建表，会同步到各个节点<br>
CREATE TABLE db.table [ON CLUSTER cluster] (...)<br>
添加、删除、修改列<br>
ALTER TABLE [db].table [ON CLUSTER cluster] ADD|DROP|MODIFY COLUMN ...<br>
rename 支持*MergeTree和Distributed<br>
rename table db.table1 to db.table2 [ON CLUSTER cluster]<br>
truncate table db.table;不支持Distributed引擎</p>
<h2 id="六-deleteupdate-不支持distributed引擎">六、delete/update 不支持Distributed引擎</h2>
<p>ALTER TABLE [db.]table DELETE WHERE filter_expr...<br>
ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE ...</p>
<h2 id="七-分区表">七、分区表</h2>
<p>按时间分区：<br>
toYYYYMM(EventDate)：按月分区<br>
toMonday(EventDate)：按周分区<br>
toDate(EventDate)：按天分区<br>
按指定列分区：<br>
PARTITION BY cloumn_name<br>
对分区的操作：<br>
alter table test1 DROP PARTITION [partition] #删除分区<br>
alter table test1 DETACH PARTITION [partition]#下线分区<br>
alter table test1 ATTACH PARTITION [partition]#恢复分区<br>
alter table .test1 FREEZE PARTITION [partition]#备份分区</p>
<h2 id="八-数据同步">八、数据同步</h2>
<ol>
<li>采用remote函数<br>
insert into db.table select * from remote('目标IP',db.table,'user','passwd')</li>
<li>csv文件导入clickhouse<br>
cat test.csv | clickhouse-client -u user --password password --query=&quot;INSERT INTO db.table FORMAT CSV&quot;</li>
<li>同步mysql库中表<br>
CREATE TABLE tmp ENGINE = MergeTree ORDER BY id AS SELECT * FROM mysql('hostip:3306', 'db', 'table', 'user', 'passwd') ;<br>
4） clickhouse-copier 工具</li>
</ol>
<h2 id="九-时间戳转换">九、时间戳转换</h2>
<p>select toUnixTimestamp('2018-11-25 00:00:02');<br>
select toDateTime(1543075202);</p>
<h2 id="十-其他事项">十、其他事项</h2>
<ol>
<li>clickhouse的cluster环境中，每台server的地位是等价的，即不存在master-slave之说，是multi-master模式。</li>
<li>各replicated表的宿主server上要在hosts里配置其他replicated表宿主server的ip和hostname的映射。</li>
<li>上面描述的在不同的server上建立全新的replicated模式的表，如果在某台server上已经存在一张replicated表，并且表中已经有数据，这时在另外的server上执行完replicated建表语句后，已有数据会自动同步到其他server上面。</li>
<li>如果zookeeper挂掉，replicated表会切换成read-only模式，不再进行数据同步，系统会周期性的尝试与zk重新建立连接。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka消费命令]]></title>
        <id>https://lvelvis.github.io/post/kafka-xiao-fei-ming-ling/</id>
        <link href="https://lvelvis.github.io/post/kafka-xiao-fei-ming-ling/">
        </link>
        <updated>2020-09-28T08:12:09.000Z</updated>
        <content type="html"><![CDATA[<p>模拟生产消息：</p>
<pre><code>bin/kafka-console-producer.sh --broker-list 192.168.101.100:9092 --topic flink-test
</code></pre>
<p>模拟消费数据(从开始位置消费)</p>
<pre><code>bin/kafka-console-consumer.sh --bootstrap-server  192.168.101.100:9092  --topic flink-test --from-beginning  |head
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[logstash output file to HDFS]]></title>
        <id>https://lvelvis.github.io/post/logstash-output-file-to-hdfs/</id>
        <link href="https://lvelvis.github.io/post/logstash-output-file-to-hdfs/">
        </link>
        <updated>2020-09-04T02:45:33.000Z</updated>
        <content type="html"><![CDATA[<p>logstash 直接把文件内容写入 hdfs 中， 并支持 hdfs 压缩格式。<br>
logstash 需要安装第三方插件，webhdfs插件，通过hdfs的web接口写入。<br>
即 http://namenode00:50070/webhdfs/v1/ 接口</p>
<p>新版本logstash已默认安装webhdfs插件<br>
官网地址及使用说明：<br>
https://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html<br>
检查hdfs的webhds接口</p>
<pre><code>curl -i  &quot;http://namenode:50070/webhdfs/v1/?user.name=hadoop&amp;op=LISTSTATUS&quot;   
HTTP/1.1 200 OK
Cache-Control: no-cache
Expires: Thu, 13 Jul 2017 04:53:39 GMT
Date: Thu, 13 Jul 2017 04:53:39 GMT
Pragma: no-cache
Expires: Thu, 13 Jul 2017 04:53:39 GMT
Date: Thu, 13 Jul 2017 04:53:39 GMT
Pragma: no-cache
Content-Type: application/json
Set-Cookie: hadoop.auth=&quot;u=hadoop&amp;p=hadoop&amp;t=simple&amp;e=1499957619679&amp;s=KSxdSAtjXAllhn73vh1MAurG9Bk=&quot;; Path=/; Expires=Thu, 13-Jul-2017 14:53:39 GMT; HttpOnly
Transfer-Encoding: chunked
Server: Jetty(6.1.26)
注释： active namenode 返回是200 ，standby namenode 返回是403.
</code></pre>
<p>测试hdfs是否正常通讯：</p>
<pre><code>#通过webhdfs接口创建test.conf
curl -i -X PUT &quot;http://hadoop-master:50070/webhdfs/v1/data/test.conf?user.name=hdfs&amp;op=CREATE&quot;
curl -i -T test.conf &quot;http://hadoop-slave1:50075/webhdfs/v1/data/test.conf?op=CREATE&amp;user.name=hdfs&amp;namenoderpcaddress=hadoop-master:9000&amp;overwrite=false&quot;
</code></pre>
<p>配置<br>
添加 logstash 一个配置文件</p>
<p>vim /home/mtime/logstash-2.3.1/conf/hdfs.conf</p>
<pre><code>input {
    kafka {
        bootstrap_servers =&gt; &quot;192.168.101.22:9092,192.168.101.23:9092,192.168.101.93:9092&quot;
        topics =&gt; &quot;test-logs&quot;
        group_id =&gt; &quot;hdfs-test-logs&quot;
        codec =&gt; json
	    consumer_threads =&gt; 15

    }
}

filter {
    date {
        match =&gt; [&quot;time&quot;,&quot;yyyy-MM-dd HH:mm:ss Z&quot;]
        target =&gt; &quot;@timestamp&quot;
        timezone =&gt; &quot;Asia/Shanghai&quot;
    }
    ruby {
        code =&gt; &quot;event.set('index.date', event.get('@timestamp').time.localtime.strftime('%Y%m%d'))&quot;
    }
    ruby {
        code =&gt; &quot;event.set('index.hour', event.get('@timestamp').time.localtime.strftime('%H'))&quot;
    }
}
output {            
    webhdfs {
           host =&gt; &quot;hadoop-master&quot;
           port =&gt; 50070
           path =&gt; &quot;/data/pt-collect-log/test-logs/%{index.date}/application-%{index.hour}.log&quot;
           user =&gt; &quot;hdfs&quot;
	   codec =&gt; line { format =&gt; &quot;%{message}&quot;}
           flush_size =&gt; 1000
           compression =&gt; &quot;gzip&quot;            
           idle_flush_time =&gt; 10
           retry_interval =&gt; 3
	   retry_times =&gt; 100
       }
}
</code></pre>
<p>关于hdfs部分配置，可以在 plugins-outputs-webhdfs 官网找到<br>
启动 logstart<br>
cd /home/mtime/logstash-2.3.1/bin/<br>
./logstash -f ../conf/hdfs.conf    # 为前台启动<br>
报错处理</p>
<pre><code>[WARN ][logstash.outputs.webhdfs ] Failed to flush outgoing items {:outgoing_count=&gt;160, :exception=&gt;&quot;WebHDFS::IOError&quot;,
我将hdfs端口 由原来的50070 改为 14000 端口，就在不报错了。
官方提供的例子中用的就是50070端口，一直没有尝试14000端口。

还有：
because this file lease is currently owned by DFSClient
hadoop 租约问题，后期正常就没有了。
执行recoverLease来释放文件的锁

$ hdfs debug recoverLease -path /logstash/2017/02/10/go-03.log
还有：
:message=&gt;&quot;webhdfs write caused an exception: {\&quot;RemoteException\&quot;:{\&quot;message\&quot;:\&quot;Failed to APPEND_FILE
当一个进程在读写这个文件的时候，另一个进程应该是不能同时写入的。
我们由原来3个logstash同时消费，改为了1个logstash消费，不在报错了。
这个应该也可以通过有话写入hdfs参数来解决。

还有：
Max write retries reached. Exception: initialize: name or service not known {:level=&gt;:error}
losgstash 需要能解析所有 hadoop 集群所有节点的主机名。
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ceph报错管理]]></title>
        <id>https://lvelvis.github.io/post/ceph-bao-cuo-guan-li/</id>
        <link href="https://lvelvis.github.io/post/ceph-bao-cuo-guan-li/">
        </link>
        <updated>2020-08-25T10:12:33.000Z</updated>
        <content type="html"><![CDATA[<p>使用ceph -s查看集群状态，发现一直有如下报错，且数量一直在增加</p>
<pre><code>daemons have recently crashed
</code></pre>
<p>经查当前系统运行状态正常，判断这里显示的应该是历史故障，处理方式如下：</p>
<p>查看历史crash</p>
<pre><code>ceph crash ls-new
</code></pre>
<p>根据ls出来的id查看详细信息</p>
<pre><code>ceph crash info &lt;crash-id&gt;
</code></pre>
<p>将历史crash信息进行归档，即不再显示</p>
<pre><code>ceph crash archive &lt;crash-id&gt;

</code></pre>
<p>归档所有信息</p>
<pre><code>ceph crash archive-all
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[golang笔记-string、int、int64互相转换]]></title>
        <id>https://lvelvis.github.io/post/golang-bi-ji-stringintint64-hu-xiang-zhuan-huan/</id>
        <link href="https://lvelvis.github.io/post/golang-bi-ji-stringintint64-hu-xiang-zhuan-huan/">
        </link>
        <updated>2020-08-10T08:35:04.000Z</updated>
        <content type="html"><![CDATA[<pre><code>#string到int  
int,err:=strconv.Atoi(string)  
#string到int64  
int64, err := strconv.ParseInt(string, 10, 64)  
#int到string  
string:=strconv.Itoa(int)  
#int64到string  
string:=strconv.FormatInt(int64,10)  ```

同类型之间转换，比如int64到int，直接int(int64)即可；
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[golang笔记-go-restful]]></title>
        <id>https://lvelvis.github.io/post/golang-bi-ji-go-restful/</id>
        <link href="https://lvelvis.github.io/post/golang-bi-ji-go-restful/">
        </link>
        <updated>2020-06-22T06:45:14.000Z</updated>
        <content type="html"><![CDATA[<p>用golang写一个restful api。如果您不知道什么是restful,可以看<a href="http://www.ruanyifeng.com/blog/2014/05/restful_api.html">阮一峰老师的教程</a></p>
<p>首先，我们需要解决的是路由的问题，也就是如何将不同的url映射到不同的处理函数。</p>
<pre><code>    router.GET(&quot;/api/todo/:todoid&quot;, getTodoById)
    router.POST(&quot;/api/todo/&quot;, addTodo)
    router.DELETE(&quot;/api/todo/:todoid&quot;, deleteTodo)
    router.PUT(&quot;/api/todo/:todoid&quot;, modifyTodo)
</code></pre>
<p>作为一个初学者，我马上打开github,找到了<a href="https://github.com/avelino/awesome-go">awesome-go</a>,经过一番调研，我感觉有几个http router的库比较适合：bone, httprouter, mux</p>
<h2 id="基本框架">基本框架</h2>
<p>首先，我们设计了四个路由，分别为根据Id获得todo，增加todo，修改todo，删除todo。这里关于解析路由参数，我们使用了httprouter.Params的ByName函数。</p>
<pre><code>package main

import (
  &quot;fmt&quot;
  &quot;github.com/julienschmidt/httprouter&quot;
  &quot;net/http&quot;
  &quot;log&quot;
  &quot;io&quot;
  &quot;io/ioutil&quot;
)

func getTodoById(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  fmt.Fprintf(w, &quot;getTodo %s\n&quot;, todoid)
}

func addTodo(w http.ResponseWriter, r *http.Request, _ httprouter.Params){
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  fmt.Fprintf(w, &quot;addTodo! %s\n&quot;,body)
}

func deleteTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  fmt.Fprintf(w, &quot;deleteTodo %s\n&quot;, todoid)
}

func modifyTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  fmt.Fprintf(w, &quot;modifyTodo %s to %s\n&quot;, todoid, body)
}

func main() {
    router := httprouter.New()
    router.GET(&quot;/api/todo/:todoid&quot;, getTodoById)
    router.POST(&quot;/api/todo/&quot;, addTodo)
    router.DELETE(&quot;/api/todo/:todoid&quot;, deleteTodo)
    router.PUT(&quot;/api/todo/:todoid&quot;, modifyTodo)
    log.Fatal(http.ListenAndServe(&quot;:8080&quot;, router))
}
</code></pre>
<p>我们可以用curl来测试一下我们的api,以put为例</p>
<pre><code>curl --data &quot;content=shopping&amp;time=tomorrow&quot; http://127.0.0.1:8080/api/todo/123 -X PUT

// modifyTodo 123 to content=shopping&amp;time=tomorrow
</code></pre>
<h2 id="json的解析">json的解析</h2>
<p>我们在使用restful api的时候，常常需要给后台传递数据。从上面可以看到，我们通过http.Request的Body属性可以获得数据</p>
<pre><code>body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
</code></pre>
<p>从上面，我们读出的数据是[]byte，但是我们希望将其解析为对象，那么在这之前，我们需要先定义我们的struct。假设我们的todo只有一个字段，就是Name</p>
<pre><code>type Todo struct {
    Name      string
}
</code></pre>
<p>现在我们可以这样解析</p>
<pre><code>var todo Todo;
json.Unmarshal(body, &amp;todo);
</code></pre>
<h2 id="model层设计">model层设计</h2>
<pre><code>package main

import (
  &quot;gopkg.in/mgo.v2&quot;
  &quot;fmt&quot;
  &quot;log&quot;
  &quot;gopkg.in/mgo.v2/bson&quot;
)

var session *mgo.Session

func init(){
  session,_ = mgo.Dial(&quot;mongodb://127.0.0.1&quot;)
}

type Todo struct {
    Name      string
}

func createTodo(t Todo){
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  c.Insert(&amp;t)
}

func queryTodoById(id string){
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  result := Todo{}

  err := c.Find(bson.M{&quot;_id&quot;: bson.ObjectIdHex(id)}).One(&amp;result)
  if err != nil {
    log.Fatal(err)
  }

  fmt.Println(&quot;Todo:&quot;, result.Name)
}

func removeTodo(id string){
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  err := c.Remove(bson.M{&quot;_id&quot;: bson.ObjectIdHex(id)})
  if err != nil{
    log.Fatal(err)
  }
}

func updateTodo(id string, update interface{}){
  //change := bson.M{&quot;$set&quot;: bson.M{&quot;name&quot;: &quot;hahaha&quot;}}
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  err := c.Update(bson.M{&quot;_id&quot;: bson.ObjectIdHex(id)}, update)
  if err != nil{
    log.Fatal(err)
  }
}
</code></pre>
<p>我们定义了Todo的struct,并添加了几种函数。</p>
<pre><code>package main

import (
  &quot;fmt&quot;
  &quot;github.com/julienschmidt/httprouter&quot;
  &quot;net/http&quot;
  &quot;log&quot;
  &quot;io&quot;
  &quot;io/ioutil&quot;
  &quot;encoding/json&quot;
  &quot;gopkg.in/mgo.v2/bson&quot;
)

func getTodoById(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  queryTodoById(todoid)
  fmt.Fprintf(w, &quot;getUser %s\n&quot;, todoid)
}

func addTodo(w http.ResponseWriter, r *http.Request, _ httprouter.Params){
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  var todo Todo;
  json.Unmarshal(body, &amp;todo);
  createTodo(todo)
  fmt.Fprintf(w, &quot;addUser! %s\n&quot;,body)
}

func deleteTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  removeTodo(todoid)
  fmt.Fprintf(w, &quot;deleteUser %s\n&quot;, todoid)
}

func modifyTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  var todo Todo
  json.Unmarshal(body, &amp;todo);
  change := bson.M{&quot;$set&quot;: bson.M{&quot;name&quot;: todo.Name}}
  updateTodo(todoid,change)
  fmt.Fprintf(w, &quot;modifyUser %s to %s\n&quot;, todoid, body)
}

func main() {
    router := httprouter.New()
    router.GET(&quot;/api/todo/:todoid&quot;, getTodoById)
    router.POST(&quot;/api/todo/&quot;, addTodo)
    router.DELETE(&quot;/api/todo/:todoid&quot;, deleteTodo)
    router.PUT(&quot;/api/todo/:todoid&quot;, modifyTodo)
    log.Fatal(http.ListenAndServe(&quot;:8080&quot;, router))
}</code></pre>
]]></content>
    </entry>
</feed>