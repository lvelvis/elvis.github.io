<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://lvelvis.github.io</id>
    <title>lvelvis</title>
    <updated>2020-05-01T05:22:29.944Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="http://lvelvis.github.io"/>
    <link rel="self" href="http://lvelvis.github.io/atom.xml"/>
    <subtitle>时光,浓淡相宜;人心,远近相安;这就是最好的生活</subtitle>
    <logo>http://lvelvis.github.io/images/avatar.png</logo>
    <icon>http://lvelvis.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, lvelvis</rights>
    <entry>
        <title type="html"><![CDATA[istio-客户端源地址如何显示]]></title>
        <id>http://lvelvis.github.io/post/istio-ke-hu-duan-yuan-di-zhi-ru-he-xian-shi/</id>
        <link href="http://lvelvis.github.io/post/istio-ke-hu-duan-yuan-di-zhi-ru-he-xian-shi/">
        </link>
        <updated>2020-04-30T07:32:39.000Z</updated>
        <content type="html"><![CDATA[<h3 id="前提">前提</h3>
<p>由于istio部署到IDC、非云环境无法获取客户端真实IP地址，web应用无法使用该组件，早期版本的istio做了很多测试，最终无法实现获取remote client address；</p>
<h3 id="环境">环境</h3>
<p>kubernetes版本：k8s-1.16.9<br>
istio版本：1.5</p>
<h3 id="方法">方法</h3>
<pre><code>kubectl -n istio-system edit  deployments. istio-pilot
添加如下：
       env:
       - name: PILOT_SIDECAR_USE_REMOTE_ADDRESS
          value: &quot;true&quot;
</code></pre>
<p>以下是github相应的issue<br>
<img src="http://lvelvis.github.io/post-images/1588232244321.png" alt="" loading="lazy"></p>
<h3 id="其他测试">其他测试</h3>
<p>istio-1.5版本回归单体，各个组件优化了很多，后期测试http链接与tcp链接应用</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[K8S通过helm 部署 ELK 7.3]]></title>
        <id>http://lvelvis.github.io/post/k8s-tong-guo-helm-bu-shu-elk-73/</id>
        <link href="http://lvelvis.github.io/post/k8s-tong-guo-helm-bu-shu-elk-73/">
        </link>
        <updated>2020-04-30T07:12:33.000Z</updated>
        <content type="html"><![CDATA[<h1 id="前提">前提：</h1>
<p>在kubernetes集群中部署elk组件，es集群部署3个节点，kibana部署一个，容器数据持久化需要storageclass</p>
<p>依赖：<br>
Helm<br>
Persistent Volumes</p>
<h1 id="准备配置">准备配置</h1>
<p>由于repo在线安装太慢，建议下载char本地修改参数后安装</p>
<pre><code>git clone https://github.com/elastic/helm-charts.git
</code></pre>
<h1 id="部署-elk">部署 ELK</h1>
<h2 id="创建elk命名空间">创建elk命名空间</h2>
<pre><code>#cat elk-ns.yml
apiVersion: v1
kind: Namespace
metadata:
  name: elk
</code></pre>
<h2 id="部署elasticsearch">部署elasticsearch</h2>
<p>cd helm-charts/elasticsearch</p>
<p>helm install --namespace=elk  --name=elasticsearch .</p>
<h2 id="部署-kibana">部署 Kibana</h2>
<p>cd helm-charts/kibana</p>
<p>helm install --namespace=elk --name=kibana .<br>
通过 kubectl get deploy 和 pod 了解部署状态；</p>
<h1 id="小知识">小知识</h1>
<p>Kibana 直接通过 K8S 内部 DNS 域名 访问 ES。</p>
<p>查看容器内的配置</p>
<pre><code>kubectl  exec kibana-kibana-7cbc5db55c-6qct7 -c kibana -- cat /usr/share/kibana/config/kibana.yml

# Default Kibana configuration for docker target
server.name: kibana
server.host: &quot;0&quot;
elasticsearch.hosts: [ &quot;http://elasticsearch:9200&quot; ]
xpack.monitoring.ui.container.elasticsearch.enabled: true
</code></pre>
<h1 id="kibana-添加-ingress">Kibana 添加 Ingress</h1>
<p>通过 Ingress 添加访问入口</p>
<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kibana
  namespace: default
spec:
  rules:
  - host: &lt;YourDomain&gt;  ## 访问 Kibana 的域名 
    http:
      paths:
      - backend:
          serviceName: kibana-kibana
          servicePort: 5601
        path: /
 status:
  loadBalancer:
    ingress:
    - ip: &lt;YourLoadBalancerIP&gt;  ## LB 的 IP
</code></pre>
<h1 id="访问测试">访问测试</h1>
<p>访问域名，即可打开 Kibana 7.3 版本；</p>
<figure data-type="image" tabindex="1"><img src="http://lvelvis.github.io/post-images/1588231324831.jpg" alt="" loading="lazy"></figure>
<p>查看集群的运行状态</p>
<figure data-type="image" tabindex="2"><img src="http://lvelvis.github.io/post-images/1588231313286.jpg" alt="" loading="lazy"></figure>
<p>也可以通过命令行查看</p>
<pre><code>~$ curl  -s &lt;YourESHost&gt;/_cluster/health | jq .
{
  &quot;cluster_name&quot;: &quot;elasticsearch&quot;,
  &quot;status&quot;: &quot;yellow&quot;,
  &quot;timed_out&quot;: false,
  &quot;number_of_nodes&quot;: 3,
  &quot;number_of_data_nodes&quot;: 3,
  &quot;active_primary_shards&quot;: 19,
  &quot;active_shards&quot;: 35,
  &quot;relocating_shards&quot;: 0,
  &quot;initializing_shards&quot;: 0,
  &quot;unassigned_shards&quot;: 3,
  &quot;delayed_unassigned_shards&quot;: 0,
  &quot;number_of_pending_tasks&quot;: 0,
  &quot;number_of_in_flight_fetch&quot;: 0,
  &quot;task_max_waiting_in_queue_millis&quot;: 0,
  &quot;active_shards_percent_as_number&quot;: 92.10526315789474
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubesphere安装使用体验]]></title>
        <id>http://lvelvis.github.io/post/kubesphere-an-zhuang-shi-yong-ti-yan/</id>
        <link href="http://lvelvis.github.io/post/kubesphere-an-zhuang-shi-yong-ti-yan/">
        </link>
        <updated>2020-04-29T09:33:05.000Z</updated>
        <content type="html"><![CDATA[<p>最近又出来个kubesphere的工具用来管理k8s，今天特意来安装体验下；<br>
github地址：https://github.com/pixiake/ks-installer</p>
<p>官方使用文档：https://kubesphere.io/docs/advanced-v2.0/zh-CN/installation/all-in-one/</p>
<p>先放上安装效果图，UI界面还是很清爽的：<br>
<img src="http://lvelvis.github.io/post-images/1588152934567.png" alt="" loading="lazy"></p>
<h3 id="当前环境">当前环境：</h3>
<pre><code>k8s集群已经安装完成，用kubesphere管理现有的k8s集群；

k8s版本为1.14  

系统为centos7.6

kubesphere使用要求：

kubernetes version &gt; 1.13.0

helm version &gt; 2.10.0

a default storage class must be in kubernetes cluster
</code></pre>
<p>安装完成后默认用户名密码：</p>
<p>用户名：admin</p>
<p>密码：P@88w0rd</p>
<h3 id="开始安装">开始安装</h3>
<p>安装步骤大概记录：</p>
<pre><code>kubectl create ns kubesphere-system
kubectl create ns kubesphere-monitoring-system

#访问etcd用到的secret
kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  --from-file=etcd-client-ca.crt=ca.pem  --from-file=etcd-client.crt=etcd-key.pem  --from-file=etcd-client.key=etcd.pem

#管理k8s用到的secret

kubectl -n kubesphere-system create secret generic kubesphere-ca  --from-file=ca.crt=ca.pem --from-file=ca.key=ca-key.pem

#clone好github项目，执行下面的这条命令

cd deploy
kubectl apply -f kubesphere-installer.yaml
</code></pre>
<p>执行完上面的命令，可以通过下面的命令，查看安装过程日志</p>
<pre><code>kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l job-name=kubesphere-installer -o jsonpath='{.items[0].metadata.name}') -f
</code></pre>
<p>查看安装结果，STATUS跟下面保持一致才说明安装成功</p>
<pre><code>[root@ks-allinone deploy]# kubectl get pods -n kubesphere-system
NAME                                     READY   STATUS      RESTARTS   AGE
ks-account-6db466d8dc-srrwj              1/1     Running     0          149m
ks-apigateway-7d77cb9495-jzmg6           1/1     Running     0          170m
ks-apiserver-f8469fd47-b58rm             1/1     Running     0          166m
ks-console-54c849bdc9-dfkbf              1/1     Running     0          168m
ks-console-54c849bdc9-z2d5q              1/1     Running     0          168m
ks-controller-manager-569456b4cd-gngm5   1/1     Running     0          170m
ks-docs-6bbdcc9bfb-6jldz                 1/1     Running     0          3h7m
kubesphere-installer-7ph6l               0/1     Completed   1          3h11m
openldap-5c986c5bff-rzqwv                1/1     Running     0          3h25m
redis-76dc4db5dd-lv6kg                   1/1     Running     0          149m
</code></pre>
<h3 id="安装过程出现的错误">安装过程出现的错误</h3>
<p>1.在安装的时提示metrics-server已经安装，导致安装中断；</p>
<p>解析办法：在kubesphere-installer.yaml的configMap增加配置：metrics_server_enable: False（默认是没有的）</p>
<pre><code>apiVersion: v1
data:
  ks-config.yaml: |
    kube_apiserver_host: 10.10.5.208:6443
    etcd_tls_enable: True
    etcd_endpoint_ips: 10.10.5.169,10.10.5.183,10.10.5.184
    disableMultiLogin: True
    elk_prefix: logstash
    metrics_server_enable: False
  #  local_registry: 192.168.1.2:5000
kind: ConfigMap
metadata:
  name: kubesphere-config
  namespace: kubesphere-system
</code></pre>
<p>增加Ingress配置：</p>
<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubesphere
  namespace: kubesphere-system
  annotations:
    #kubernetes.io/ingress.class: traefik
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: ks.staplescn.com
    http:
      paths:
      - path:
        backend:
          serviceName: ks-console
          servicePort: 80
</code></pre>
<p>访问界面：<br>
<img src="http://lvelvis.github.io/post-images/1588153130667.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes Tekton-CI/CD 持续集成流水线]]></title>
        <id>http://lvelvis.github.io/post/kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian/</id>
        <link href="http://lvelvis.github.io/post/kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian/">
        </link>
        <updated>2020-01-20T08:48:40.000Z</updated>
        <content type="html"><![CDATA[<h1 id="前言">前言</h1>
<p>我们通常的开发流程是，在本地开发完成应用之后，使用git作为版本管理工具，将本地代码提交到类似Github这样的仓库中做持久化存储，当我们可能来自多个仓库、可能涉及到多个中间件作为底层依赖一起部署到生产环境中时，相信不少在大中型企业工作的小伙伴都知道公司内部通常会有发布系统，那么云原生技术栈中有没有为我们提供类似的发布系统呢？档案是肯定的，而且不乏竞争者，业内知名的有knavtie Build/jekinsX/spinnaker/orgo/tekton，其中，tekon凭借其众多优良特性在一众竞争者中胜出，成为领域内的事实标准，今天我们就来揭开Tekton的神秘面纱。<br>
<img src="http://lvelvis.github.io/post-images/1588236885419.png" alt="" loading="lazy"></p>
<h1 id="正文">正文</h1>
<h3 id="什么是tekton">什么是Tekton</h3>
<p><img src="http://lvelvis.github.io/post-images/1588236902661.png" alt="" loading="lazy"><br>
那Tekton都提供了哪些CRD呢？<br>
•	Task：顾名思义，task表示一个构建任务，task里可以定义一系列的steps，例如编译代码、构建镜像、推送镜像等，每个step实际由一个Pod执行。<br>
•	TaskRun：task只是定义了一个模版，taskRun才真正代表了一次实际的运行，当然你也可以自己手动创建一个taskRun，taskRun创建出来之后，就会自动触发task描述的构建任务。<br>
•	Pipeline：一个或多个task、PipelineResource以及各种定义参数的集合。<br>
•	PipelineRun：类似task和taskRun的关系，pipelineRun也表示某一次实际运行的pipeline，下发一个pipelineRun CRD实例到kubernetes后，同样也会触发一次pipeline的构建。<br>
•	PipelineResource：表示pipeline input资源，比如github上的源码，或者pipeline output资源，例如一个容器镜像或者构建生成的jar包等。<br>
他们大概有如下图所示的关系：<br>
官方介绍：</p>
<pre><code>Tekton 是一个功能强大且灵活的 Kubernetes 原生开源框架，用于创建持续集成和交付（CI/CD）系统。通过抽象底层实现细节，用户可以跨多云平台和本地系统进行构建、测试和部署。
</code></pre>
<pre><code>个人理解：
•	以yaml文件编排应用构建及部署流程
•	knavtive build模块升级版，社区最终采用 Tekton 替代 knavtive Build作为云原生领域的CI/CD 解决方案
•	标准化CI/CD流水线构建、测试及部署流程的工具
</code></pre>
<p>Tekton在一众竞争对手的比拼中PK胜出：<br>
<img src="http://lvelvis.github.io/post-images/1588236947460.png" alt="" loading="lazy"></p>
<p>下面就让我们一起来深入详细了解下Tekton到底怎么玩。<br>
Tekton Pipeline中有5类对象，核心理念是通过定义yaml定义构建过程。<br>
•	Task：一个任务的执行模板，用于描述单个任务的构建过程<br>
•	TaskRun：需要通过定义TaskRun任务去运行Task。<br>
•	Pipeline：包含多个Task,并在此基础上定义input和output,input和output以PipelineResource作为交付。<br>
•	PipelineRun：需要定义PipelineRun才会运行Pipeline。<br>
•	PipelineResource：可用于input和output的对象集合。</p>
<h3 id="task">Task</h3>
<p>Task 就是一个任务执行模板，之所以说 Task 是一个模板是因为 Task 定义中可以包含变量，Task 在真正执行的时候需要给定变量的具体值。如果把 Tekton 的 Task 有点儿类似于定义一个函数，Task 通过 inputs.params 定义需要哪些入参，并且每一个入参还可以指定默认值。Task 的 steps 字段表示当前 Task 是有哪些步骤组成的，每一个步骤具体就是基于镜像启动一个 container 执行一些操作，container 的启动参数可以通过 Task 的入参使用模板语法进行配置。下面是一个Demo：<br>
<img src="http://lvelvis.github.io/post-images/1588236983683.png" alt="" loading="lazy"></p>
<h3 id="taskrun">TaskRun</h3>
<p>Task 定义好以后是不能执行的，就像一个函数定义好以后需要调用才能执行一样。所以需要再定义一个 TaskRun 去执行 Task。<br>
TaskRun 主要是负责设置 Task 需要的参数，并通过 taskRef 字段引用要执行的 Task。下面是一个Demo：<br>
<img src="http://lvelvis.github.io/post-images/1588237005091.png" alt="" loading="lazy"></p>
<p>但是在实际使用过程中，我们一般很少使用TaskRun，因为它只能给不一个Task 传参，Tekton提供了给多个Task同时传参的解决方案Pipeline和PipelineRun，且看下文详解，这里只是多嘴一下，这个TaskRun很少使用，稍微了解下就可以了。</p>
<h3 id="pipeline">Pipeline</h3>
<p>一个 TaskRun 只能执行一个 Task，当需要编排多个 Task 的时候就需要 Pipeline 出马了。Pipeline 是一个编排 Task 的模板。Pipeline 的 params 声明了执行时需要的入参。 Pipeline 的 spec.tasks 定义了需要编排的 Task。Tasks 是一个数组，数组中的 task 并不是通过数组声明的顺序去执行的，而是通过 runAfter 来声明 task 执行的顺序。Tekton controller 在解析 CRD 的时候会解析 Task 的顺序，然后根据 runAfter 设置生成的依次树依次去执行。Pipeline 在编排 Task 的时候需要给每一个 Task 传入必须的参数，这些参数的值可以来自 Pipeline 自身的 params 设置。下面是一个Demo：<br>
<img src="http://lvelvis.github.io/post-images/1588237033509.png" alt="" loading="lazy"><br>
<img src="http://lvelvis.github.io/post-images/1588237113343.png" alt="" loading="lazy"></p>
<h3 id="pipelinerun">PipelineRun</h3>
<p>和 Task 一样 Pipeline 定义完成以后也是不能直接执行的，需要 PipelineRun 才能执行 Pipeline。PipelineRun 的主要作用是给 Pipeline 传入必要的入参，并执行 Pipeline。下面是一个Demo：<br>
<img src="http://lvelvis.github.io/post-images/1588237145814.png" alt="" loading="lazy"><br>
<img src="http://lvelvis.github.io/post-images/1588237151158.png" alt="" loading="lazy"></p>
<h3 id="pipelineresource">PipelineResource</h3>
<p>可能你还想在 Task 之间共享资源，这就是 PipelineResource 的作用。比如我们可以把 git 仓库信息放在 PipelineResource 中。这样所有 Task 就可以共享这些信息了。<br>
<img src="http://lvelvis.github.io/post-images/1588237189249.png" alt="" loading="lazy"><br>
<img src="http://lvelvis.github.io/post-images/1588237194640.png" alt="" loading="lazy"><br>
<img src="http://lvelvis.github.io/post-images/1588237221271.png" alt="" loading="lazy"></p>
<h1 id="实战">实战</h1>
<p>关于Tekton的实战，可以参考Github里面的这个完整的Demo，里面是一个go语言吧编写的web服务，接口可以打印&quot;Hello world&quot;。时间有限，就不做演示了，感兴趣的可以在自己的k8s集群上面跑一下感受一下，相关的yaml文件也可以拷贝下来，作为后面改写的模板。<br>
准备 PIpeline 的资源<br>
kubectl apply -f tasks/source-to-image.yaml -f tasks/deploy-using-kubectl.yaml  -f resources/picalc-git.yaml -f image-secret.yaml -f pipeline-account.yaml -f pipeline/build-and-deploy-pipeline.yaml<br>
执行 create 把 pipelieRun 提交到 Kubernetes 集群。之所以这里使用 create 而不是使用 apply 是因为 PIpelineRun 每次都会创建一个新的，kubectl 的 create 指令会基于 generateName 创建新的 PIpelineRun 资源。<br>
kubectl create -f run/picalc-pipeline-run.yaml</p>
<h1 id="总结">总结</h1>
<p>Tekton以K8S为依托，成为云原生领域CI/CD的事实性标准，帮助我们提高云原生环境下的应用构建和部署效率。<br>
来一张图对全文做一个简单的总结：</p>
<figure data-type="image" tabindex="1"><img src="http://lvelvis.github.io/post-images/1588236724398.png" alt="" loading="lazy"></figure>
<h1 id="参考资料">参考资料</h1>
<ol>
<li><a href="https://cloud.google.com/tekton/">Tekton官网</a></li>
<li><a href="https://github.com/knative-sample/tekton-knative/tree/b1.0?spm=ata.13261165.0.0.21213a182xyMm5&amp;file=b1.0">Tekton Demo</a></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes搭建rook-ceph]]></title>
        <id>http://lvelvis.github.io/post/kubernetes搭建rook-ceph/</id>
        <link href="http://lvelvis.github.io/post/kubernetes搭建rook-ceph/">
        </link>
        <updated>2019-12-11T16:00:00.000Z</updated>
        <content type="html"><![CDATA[<h1 id="简介">简介</h1>
<p>Rook官网：https://rook.io<br>
Rook是云原生计算基金会(CNCF)的孵化级项目.<br>
Rook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。<br>
至于CEPH，官网在这：https://ceph.com/<br>
ceph官方提供的helm部署，至今我没成功过，所以转向使用rook提供的方案<br>
有道笔记原文：http://note.youdao.com/noteshare?id=281719f1f0374f787effc90067e0d5ad&amp;sub=0B59EA339D4A4769B55F008D72C1A4C0</p>
<h1 id="环境">环境</h1>
<pre><code>centos 7.5
kernel 4.18.7-1.el7.elrepo.x86_64
docker 18.06
kubernetes v1.12.2
    kubeadm部署：
        网络: canal
        DNS: coredns
    集群成员：    
    192.168.1.1 kube-master
    192.168.1.2 kube-node1
    192.168.1.3 kube-node2
    192.168.1.4 kube-node3
    192.168.1.5 kube-node4

所有node节点准备一块200G的磁盘：/dev/sdb
kubernetes搭建rook-ceph
</code></pre>
<figure data-type="image" tabindex="1"><img src="http://lvelvis.github.io/post-images/1588233297037.png" alt="" loading="lazy"></figure>
<h1 id="准备工作">准备工作</h1>
<p>所有节点开启ip_forward</p>
<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
</code></pre>
<p>sysctl -p</p>
<h1 id="开始部署operator">开始部署Operator</h1>
<h2 id="部署rook-operator">部署Rook Operator</h2>
<pre><code>cd $HOME
git clone https://github.com/rook/rook.git

cd rook
cd cluster/examples/kubernetes/ceph
kubectl apply -f operator.yaml
</code></pre>
<figure data-type="image" tabindex="2"><img src="http://lvelvis.github.io/post-images/1588233371696.png" alt="" loading="lazy"></figure>
<h2 id="查看operator的状态">查看Operator的状态</h2>
<p>执行apply之后稍等一会<br>
operator会在集群内的每个主机创建两个pod:rook-discover,rook-ceph-agent</p>
<p>kubectl -n rook-ceph-system get pod -o wide<br>
<img src="http://lvelvis.github.io/post-images/1588233523024.png" alt="" loading="lazy"></p>
<h2 id="给节点打标签">给节点打标签</h2>
<p>运行ceph-mon的节点打上：ceph-mon=enabled<br>
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-mon=enabled<br>
运行ceph-osd的节点，也就是存储节点，打上：ceph-osd=enabled<br>
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-osd=enabled<br>
运行ceph-mgr的节点，打上：ceph-mgr=enabled<br>
mgr只能支持一个节点运行，这是ceph跑k8s里的局限<br>
kubectl label nodes kube-node1 ceph-mgr=enabled</p>
<h2 id="配置clusteryaml文件">配置cluster.yaml文件</h2>
<p>官方配置文件详解：https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html<br>
文件中有几个地方要注意：</p>
<ul>
<li>dataDirHostPath: 这个路径是会在宿主机上生成的，保存的是ceph的相关的配置文件，再重新生成*集群的时候要确保这个目录为空，否则mon会无法启动</li>
<li>useAllDevices: 使用所有的设备，建议为false，否则会把宿主机所有可用的磁盘都干掉</li>
<li>useAllNodes：使用所有的node节点，建议为false，肯定不会用k8s集群内的所有node来搭建ceph的</li>
<li>databaseSizeMB和journalSizeMB：当磁盘大于100G的时候，就注释这俩项就行了<br>
本次实验用到的 cluster.yaml 文件内容如下：</li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;configmaps&quot;]
  verbs: [ &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;delete&quot; ]
---
# Allow the operator to create resources in this cluster's namespace
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster-mgmt
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-cluster-mgmt
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph-system
---
# Allow the pods in this namespace to work with configmaps
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-cluster
subjects:
- kind: ServiceAccount
  name: rook-ceph-cluster
  namespace: rook-ceph
---
apiVersion: ceph.rook.io/v1beta1
kind: Cluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
    # v12 is luminous, v13 is mimic, and v14 is nautilus.
    # RECOMMENDATION: In production, use a specific version tag instead of the general v13 flag, which pulls the latest release and could result in different
    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
    image: ceph/ceph:v13
    # Whether to allow unsupported versions of Ceph. Currently only luminous and mimic are supported.
    # After nautilus is released, Rook will be updated to support nautilus.
    # Do not set to true in production.
    allowUnsupported: false
  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).
  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
  # In Minikube, the '/data' directory is configured to persist across reboots. Use &quot;/data/rook&quot; in Minikube environment.
  dataDirHostPath: /var/lib/rook
  # The service account under which to run the daemon pods in this cluster if the default account is not sufficient (OSDs)
  serviceAccount: rook-ceph-cluster
  # set the amount of mons to be started
  # count可以定义ceph-mon运行的数量，这里默认三个就行了
  mon:
    count: 3
    allowMultiplePerNode: true
  # enable the ceph dashboard for viewing cluster status
  # 开启ceph资源面板
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
  network:
    # toggle to use hostNetwork
    # 使用宿主机的网络进行通讯
    # 使用宿主机的网络貌似可以让集群外的主机挂载ceph
    # 但是我没试过，有兴趣的兄弟可以试试改成true
    # 反正这里只是集群内用，我就不改了
    hostNetwork: false
  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
  # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
  # tolerate taints with a key of 'storage-node'.
  placement:
#    all:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: role
#              operator: In
#              values:
#              - storage-node
#      podAffinity:
#      podAntiAffinity:
#      tolerations:
#      - key: storage-node
#        operator: Exists
# The above placement information can also be specified for mon, osd, and mgr components
#    mon:
#    osd:
#    mgr:
# nodeAffinity：通过选择标签的方式，可以限制pod被调度到特定的节点上
# 建议限制一下，为了让这几个pod不乱跑
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mon
              operator: In
              values:
              - enabled
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-osd
              operator: In
              values:
              - enabled
    mgr:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mgr
              operator: In
              values:
              - enabled
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
#    mgr:
#      limits:
#        cpu: &quot;500m&quot;
#        memory: &quot;1024Mi&quot;
#      requests:
#        cpu: &quot;500m&quot;
#        memory: &quot;1024Mi&quot;
# The above example requests/limits can also be added to the mon and osd components
#    mon:
#    osd:
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    deviceFilter:
    location:
    config:
      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.
      # Set the storeType explicitly only if it is required not to use the default.
      # storeType: bluestore
      # databaseSizeMB: &quot;1024&quot; # this value can be removed for environments with normal sized disks (100 GB or larger)
      # journalSizeMB: &quot;1024&quot;  # this value can be removed for environments with normal sized disks (20 GB or larger)
# Cluster level list of directories to use for storage. These values will be set for all nodes that have no `directories` set.
#    directories:
#    - path: /rook/storage-dir
# Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
# nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
#建议磁盘配置方式如下：
#name: 选择一个节点，节点名字为kubernetes.io/hostname的标签，也就是kubectl get nodes看到的名字
#devices: 选择磁盘设置为OSD
# - name: &quot;sdb&quot;:将/dev/sdb设置为osd
    nodes:
    - name: &quot;kube-node1&quot;
      devices:
      - name: &quot;sdb&quot;
    - name: &quot;kube-node2&quot;
      devices:
      - name: &quot;sdb&quot;
    - name: &quot;kube-node3&quot;
      devices:
      - name: &quot;sdb&quot;

#      directories: # specific directories to use for storage can be specified for each node
#      - path: &quot;/rook/storage-dir&quot;
#      resources:
#        limits:
#          cpu: &quot;500m&quot;
#          memory: &quot;1024Mi&quot;
#        requests:
#          cpu: &quot;500m&quot;
#          memory: &quot;1024Mi&quot;
#    - name: &quot;172.17.4.201&quot;
#      devices: # specific devices to use for storage can be specified for each node
#      - name: &quot;sdb&quot;
#      - name: &quot;sdc&quot;
#      config: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: filestore
#    - name: &quot;172.17.4.301&quot;
#      deviceFilter: &quot;^sd.&quot;
</code></pre>
<h2 id="开始部署ceph">开始部署ceph</h2>
<p>部署ceph<br>
kubectl apply -f cluster.yaml<br>
cluster会在rook-ceph这个namesapce创建资源<br>
看到所有的pod都Running就行了<br>
注意看一下pod分布的宿主机，跟我们打标签的主机是一致的<br>
kubectl -n rook-ceph get pod -o wide<br>
<img src="http://lvelvis.github.io/post-images/1588233756856.png" alt="" loading="lazy"></p>
<p>切换到其他主机看一下磁盘</p>
<p>切换到kube-node1<br>
lsblk<br>
<img src="http://lvelvis.github.io/post-images/1588233803458.png" alt="" loading="lazy"></p>
<h2 id="配置ceph-dashboard">配置ceph dashboard</h2>
<p>看一眼dashboard在哪个service上<br>
kubectl -n rook-ceph get service<br>
可以看到dashboard监听了8443端口<br>
<img src="http://lvelvis.github.io/post-images/1588233853544.png" alt="" loading="lazy"></p>
<p>创建个nodeport类型的service以便集群外部访问<br>
kubectl apply -f dashboard-external-https.yaml</p>
<p>查看一下nodeport在哪个端口<br>
kubectl -n rook-ceph get service<br>
<img src="http://lvelvis.github.io/post-images/1588233909644.png" alt="" loading="lazy"></p>
<p>找出Dashboard的登陆账号和密码<br>
MGR_POD=<code>kubectl get pod -n rook-ceph | grep mgr | awk '{print $1}'</code><br>
kubectl -n rook-ceph logs $MGR_POD | grep password<br>
<img src="http://lvelvis.github.io/post-images/1588233955731.png" alt="" loading="lazy"></p>
<p>打开浏览器输入任意一个Node的IP+nodeport端口<br>
这里我的就是：https://192.168.1.2:30290<br>
<img src="http://lvelvis.github.io/post-images/1588234024005.png" alt="" loading="lazy"><br>
<img src="http://lvelvis.github.io/post-images/1588234065656.png" alt="" loading="lazy"></p>
<h2 id="配置ceph为storageclass">配置ceph为storageclass</h2>
<p>官方给了一个样本文件：storageclass.yaml<br>
这个文件使用的是 RBD 块存储<br>
pool创建详解：https://rook.io/docs/rook/v0.8/ceph-pool-crd.html</p>
<pre><code>apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  #这个name就是创建成ceph pool之后的pool名字
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 1
  # size 池中数据的副本数,1就是不保存任何副本
  failureDomain: osd
  #  failureDomain：数据块的故障域，
  #  值为host时，每个数据块将放置在不同的主机上
  #  值为osd时，每个数据块将放置在不同的osd上
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: ceph
   # StorageClass的名字，pvc调用时填的名字
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use `rook` as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: xfs
# 设置回收策略默认为：Retain
reclaimPolicy: Retain
</code></pre>
<h2 id="创建storageclass">创建StorageClass</h2>
<p>kubectl apply -f storageclass.yaml<br>
kubectl get storageclasses.storage.k8s.io  -n rook-ceph<br>
kubectl describe storageclasses.storage.k8s.io  -n rook-ceph<br>
kubernetes搭建rook-ceph</p>
<p>创建个nginx pod尝试挂载</p>
<pre><code>cat &lt;&lt; EOF &gt; nginx.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: ceph

---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports: 
  - port: 80
    name: nginx-port
    targetPort: 80
    protocol: TCP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /html
          name: http-file
      volumes:
      - name: http-file
        persistentVolumeClaim:
          claimName: nginx-pvc
EOF
</code></pre>
<p>kubectl apply -f nginx.yaml<br>
查看pv,pvc是否创建了<br>
kubectl get pv,pvc</p>
<p>看一下nginx这个pod也运行了<br>
kubectl get pod</p>
<p>删除这个pod,看pv是否还存在<br>
kubectl delete -f nginx.yaml</p>
<p>kubectl get pv,pvc<br>
可以看到，pod和pvc都已经被删除了，但是pv还在！！！</p>
<h2 id="添加新节点进入集群">添加新节点进入集群</h2>
<p>这次我们要把node4添加进集群，先打标签<br>
kubectl label nodes kube-node4 ceph-osd=enabled<br>
重新编辑cluster.yaml文件<br>
原来的基础上添加node4的信息</p>
<p>cd $HOME/rook/cluster/examples/kubernetes/ceph/<br>
vi cluster.yam<br>
<img src="http://lvelvis.github.io/post-images/1588234207475.png" alt="" loading="lazy"></p>
<p>apply一下cluster.yaml文件<br>
kubectl apply -f cluster.yaml</p>
<p>盯着rook-ceph名称空间,集群会自动添加node4进来<br>
kubectl -n rook-ceph get pod -o wide -w<br>
kubectl -n rook-ceph get pod -o wide<br>
<img src="http://lvelvis.github.io/post-images/1588234283568.png" alt="" loading="lazy"><br>
<img src="http://lvelvis.github.io/post-images/1588234307875.png" alt="" loading="lazy"><br>
去node4节点看一下磁盘<br>
lsblk<br>
<img src="http://lvelvis.github.io/post-images/1588234334204.png" alt="" loading="lazy"></p>
<h2 id="删除一个节点">删除一个节点</h2>
<p>去掉node3的标签<br>
kubectl label nodes kube-node3 ceph-osd-<br>
重新编辑cluster.yaml文件<br>
删除node3的信息<br>
cd $HOME/rook/cluster/examples/kubernetes/ceph/<br>
vi cluster.yam<br>
<img src="http://lvelvis.github.io/post-images/1588234380826.png" alt="" loading="lazy"></p>
<p>apply一下cluster.yaml文件<br>
kubectl apply -f cluster.yaml<br>
<img src="http://lvelvis.github.io/post-images/1588234493936.png" alt="" loading="lazy"></p>
<p>kubectl -n rook-ceph get pod -o wide<br>
<img src="http://lvelvis.github.io/post-images/1588234548709.png" alt="" loading="lazy"><br>
<img src="http://lvelvis.github.io/post-images/1588234579591.png" alt="" loading="lazy"><br>
最后记得删除宿主机的/var/lib/rook文件夹</p>
]]></content>
    </entry>
</feed>