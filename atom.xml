<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://lvelvis.github.io</id>
    <title>lvelvis</title>
    <updated>2020-11-04T02:22:02.840Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="http://lvelvis.github.io"/>
    <link rel="self" href="http://lvelvis.github.io/atom.xml"/>
    <subtitle>时光,浓淡相宜;人心,远近相安;这就是最好的生活</subtitle>
    <logo>http://lvelvis.github.io/images/avatar.png</logo>
    <icon>http://lvelvis.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, lvelvis</rights>
    <entry>
        <title type="html"><![CDATA[Clickhouse删除表分区]]></title>
        <id>http://lvelvis.github.io/post/clickhouse-shan-chu-biao-fen-qu/</id>
        <link href="http://lvelvis.github.io/post/clickhouse-shan-chu-biao-fen-qu/">
        </link>
        <updated>2020-11-04T02:19:49.000Z</updated>
        <content type="html"><![CDATA[<p>方法一：</p>
<pre><code>ALTER TABLE kuming.tableName DELETE WHERE toDate(insert_at_timestamp)='2020-07-21';
</code></pre>
<p>方法二：</p>
<pre><code>ALTER TABLE kuming.tableName DELETE WHERE insert_at_timestamp&lt;=1596470399;
</code></pre>
<p>方法三：（当前两种方法分区数据没有删除掉的时候可以用方法三）</p>
<pre><code>ALTER TABLE kuming.tableName DROP PARTITION '2020-08-03';
</code></pre>
<p>然后查询本地表分区是否删除</p>
<pre><code>SELECT      partition,     name, table     active FROM system.parts where table='tableName';
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[prometheus监控数据远程存储clickhouse集群]]></title>
        <id>http://lvelvis.github.io/post/prometheus-jian-kong-shu-ju-yuan-cheng-cun-chu-clickhouse-ji-qun/</id>
        <link href="http://lvelvis.github.io/post/prometheus-jian-kong-shu-ju-yuan-cheng-cun-chu-clickhouse-ji-qun/">
        </link>
        <updated>2020-10-28T07:36:32.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前提">前提</h2>
<pre><code>Prometheus的本地存储设计可以减少其自身运维和管理的复杂度，同时能够满足大部分用户监控规模的需求。但是本地存储也意味着Prometheus无法持久化数据，无法存储大量历史数据(TSDB存储大量数据，会导致程序启动加载占用过多内存)，同时也无法灵活扩展和迁移。
为了保持Prometheus的简单性，Prometheus并没有尝试在自身中解决以上问题，而是通过定义两个标准接口(remote_write/remote_read)，让用户可以基于这两个接口对接将数据保存到任意第三方的存储服务中，这种方式在Promthues中称为Remote Storage。
</code></pre>
<p>目前Prometheus社区也提供了部分对于第三方数据库的Remote Storage支持：</p>
<table>
<thead>
<tr>
<th>存储服务</th>
<th>支持模式</th>
</tr>
</thead>
<tbody>
<tr>
<td>AppOptics</td>
<td><code>write</code></td>
</tr>
<tr>
<td>Chronix</td>
<td><code>write</code></td>
</tr>
<tr>
<td>Cortex</td>
<td><code>read/write</code></td>
</tr>
<tr>
<td>CrateDB</td>
<td><code>read/write</code></td>
</tr>
<tr>
<td>Gnocchi</td>
<td><code>write</code></td>
</tr>
<tr>
<td>Graphite</td>
<td><code>write</code></td>
</tr>
<tr>
<td>InfluxDB</td>
<td><code>read/write</code></td>
</tr>
<tr>
<td>OpenTSDB</td>
<td><code>write</code></td>
</tr>
<tr>
<td>PostgreSQL/TimescaleDB</td>
<td><code>read/write</code></td>
</tr>
<tr>
<td>SignalFx</td>
<td><code>write</code></td>
</tr>
</tbody>
</table>
<h2 id="remote-write">Remote Write</h2>
<p><img src="http://lvelvis.github.io/post-images/1603871818190.png" alt="" loading="lazy"><br>
用户可以在Prometheus配置文件中指定Remote Write(远程写)的URL地址，一旦设置了该配置项，Prometheus将采集到的样本数据通过HTTP的形式发送给适配器(Adaptor)。而用户则可以在适配器中对接外部任意的服务。外部服务可以是真正的存储系统，公有云的存储服务，也可以是消息队列等任意形式。</p>
<ul>
<li>可同时启用配置TSDB存储1一天数据，其他数据使用远程存储；查询近1天数据，从tsdb获取，其他数据从远程存储获取</li>
</ul>
<h2 id="remote-read">Remote Read</h2>
<p><img src="http://lvelvis.github.io/post-images/1603872148839.png" alt="" loading="lazy"><br>
如下图所示，Promthues的Remote Read(远程读)也通过了一个适配器实现。在远程读的流程当中，当用户发起查询请求后，Promthues将向remote_read中配置的URL发起查询请求(matchers,ranges)，Adaptor根据请求条件从第三方存储服务中获取响应的数据。同时将数据转换为Promthues的原始样本数据返回给Prometheus Server。</p>
<p>当获取到样本数据后，Promthues在本地使用PromQL对样本数据进行二次处理。</p>
<h2 id="prom2click适配器">prom2click适配器</h2>
<pre><code>git clone https://github.com/lvelvis/prom2click.git
</code></pre>
<ul>
<li>prometheus-operator已测试，可正常获取数据</li>
<li>修改部分代码解决部分监控项无法获取数据</li>
<li>读写数据需指定为分布式表(官方不建议写入分布式表)，或者配置2个write地址，每个地址落在每个分片上</li>
<li>修改为按天分区</li>
</ul>
<h3 id="prom2click流程图">prom2click流程图</h3>
<p><img src="http://lvelvis.github.io/post-images/1603872379113.png" alt="" loading="lazy"><br>
*prom2click与clickhouse的连接使用的长链接；因此需要多配置几个prom2click服务(默认使用配置-ch.dsn第一个clickhoue节点)，均衡clickhouse查询压力。</p>
<h2 id="clickhouse-环境">clickhouse 环境</h2>
<pre><code>clickhouse-server --version
ClickHouse server version 20.3.8.53.
</code></pre>
<p>使用4个节点+3zk 2shards_2replicas</p>
<h2 id="配置方式">配置方式</h2>
<h3 id="官方promtheus部署方式promtheusyml增加以下参数">官方promtheus部署方式，promtheus.yml增加以下参数</h3>
<pre><code>remote_write:
    - url: &quot;http://localhost:9201/write&quot;
remote_read:
    - url: &quot;http://localhost:9202/read&quot;
    - url: &quot;http://localhost:9203/read&quot;
</code></pre>
<h3 id="promtheus-operatorkube-prometheus-manifests">promtheus-operator(kube-prometheus-manifests)</h3>
<p>prometheus/prometheus-prometheus.yaml</p>
<pre><code>apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  labels:
    prometheus: k8s
  name: k8s
  namespace: monitoring
spec:
  retention: 1d
  remoteWrite: 
    - url: &quot;http://192.168.101.240:9201/write&quot;
  remoteRead: 
    - url: &quot;http://192.168.101.241:9201/read&quot;
    - url: &quot;http://192.168.101.242:9201/read&quot;
    - url: &quot;http://192.168.101.243:9201/read&quot;
  alerting:
    alertmanagers:
    - name: alertmanager-main
      namespace: monitoring
      port: web
  image: lvcisco/prometheus:v2.15.2
  nodeSelector:
    kubernetes.io/os: linux
  podMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  replicas: 2
  resources:
    requests:
      cpu: &quot;2&quot;
      memory: 1Gi
    limits:
      cpu: &quot;4&quot;
      #memory: 8Gi
  ruleSelector:
    matchLabels:
      prometheus: k8s
      role: alert-rules
  securityContext:
    fsGroup: 2000
</code></pre>
<pre><code>kubectl replace -f  prometheus/prometheus-prometheus.yaml 
</code></pre>
<p>然后查看promtheus容器配置是否生效</p>
<pre><code>kubectl  -n monitoring exec  prometheus-k8s-0 tail  /etc/prometheus/config_out/prometheus.env.yaml

remote_write:
- url: http://192.168.101.240:9201/write
  remote_timeout: 30s
remote_read:
- url: http://192.168.101.241:9201/read
  remote_timeout: 30s
- url: http://192.168.101.242:9201/read
  remote_timeout: 30s
- url: http://192.168.101.243:9201/read
  remote_timeout: 30s
</code></pre>
<ul>
<li>更多参数配置查看<a href="https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotereadspec">远程存储配置</a></li>
<li><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write">其他配置</a></li>
</ul>
<h2 id="测试">测试</h2>
<p>可删除tsdb数据，需重启prometheus-k8s容器，再次查询grafana是否有历史数据</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka数据导入到Clickhouse]]></title>
        <id>http://lvelvis.github.io/post/kafka-shu-ju-dao-ru-dao-clickhouse/</id>
        <link href="http://lvelvis.github.io/post/kafka-shu-ju-dao-ru-dao-clickhouse/">
        </link>
        <updated>2020-10-28T03:26:38.000Z</updated>
        <content type="html"><![CDATA[<pre><code>Kafka 是目前应用非常广泛的开源消息中间件，一个常用的的场景就是做数据总线收集各个服务的消息日志，下游各种数据服务订阅消费数据，生成各种报表或数据应用等。Clickhouse 的自带了 Kafka Engine，使得 Clickhouse 和 Kafka 的集成变得非常容易。
</code></pre>
<h2 id="创建-kafka-表">创建 Kafka 表</h2>
<p>Clickhouse 的 Kafka Engine 可以将 Kafka 中的流映射成一个表，方便我们的后续处理。只要建表的时候制定</p>
<pre><code>Kafka(broker_list, topic_list, group_name, format[, schema])
broker_list: 逗号分隔的 Kafka broker 列表
topic_list: 消费的topic
group_name: consumer group 的id， 同一个 group_name 的 clickhouse 会在同一个 consumer group 消费数据
format: kafka 消息的格式
</code></pre>
<p>在前文的所述的3节点 clickhouse 集群上，在每一个节点都建一个 Kafka Engine 的表从 kafka 的events topic读数据。</p>
<pre><code>CREATE TABLE event_stream (ts UInt64, tag String, cnt Int64, val Double) 
ENGINE = Kafka('127.0.0.1:9092', 'events', 'group1', 'JSONEachRow');
</code></pre>
<h2 id="写入数据">写入数据</h2>
<p>现在我们试着往 kafka 写一点json数据</p>
<pre><code>bin/kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic events
&gt;{&quot;ts&quot;:1515897449,&quot;tag&quot;:&quot;aa&quot;,&quot;cnt&quot;:3,&quot;val&quot;:0.7}
&gt;{&quot;ts&quot;:1515897450,&quot;tag&quot;:&quot;bb&quot;,&quot;cnt&quot;:9,&quot;val&quot;:0.28}
&gt;{&quot;ts&quot;:1515897451,&quot;tag&quot;:&quot;cc&quot;,&quot;cnt&quot;:7,&quot;val&quot;:0.93}
&gt;{&quot;ts&quot;:1515897452,&quot;tag&quot;:&quot;dd&quot;,&quot;cnt&quot;:1,&quot;val&quot;:0.78}
</code></pre>
<p>然后在每个 clickhose 节点中查看数据。</p>
<pre><code>┌─────────ts─┬─tag─┬─cnt─┬────────────────val─┐
│ 1515897449 │ aa  │   3 │ 0.7000000000000001 │
└────────────┴─────┴─────┴────────────────────┘
┌─────────ts─┬─tag─┬─cnt─┬──val─┐
│ 1515897452 │ dd  │   1 │ 0.78 │
└────────────┴─────┴─────┴──────┘
</code></pre>
<p>注意的是由于一个kafka的partition 只能由一个 group consumer 消费，所以clickhouse 节点数需要大于 topic 的 partition 数。</p>
<p>由于 Kafka 表只是 kafka 流的一个视图而已，当数据被 select 了一次之后，这个数据就会被认为已经消费了，下次 select 就不会再出现。所以Kafka表单独使用是没什么用的，一般是用来和 MaterialView 配合，将Kafka表里面的数据自动导入到 MaterialView 里面。</p>
<h2 id="与-materialview-集成">与 MaterialView 集成</h2>
<p>我们现在每一节点建一个 MaterialView 保存 Kafka 里面的数据, 再顺手建一个全局的Distributed表。</p>
<pre><code>CREATE MATERIALIZED VIEW events ENGINE = MergeTree(day, (day,ts, tag, cnt, val), 8192) AS
SELECT toDate(toDateTime(ts)) AS day, ts, tag, cnt, val FROM event_stream;

CREATE TABLE events_all AS events
ENGINE = Distributed(perftest_3shards_1replicas, default, events, rand());
</code></pre>
<p>再往Kafka里面写些数据，就能在各个节点的 events 或 events_all 里面查出来了。</p>
<h2 id="总结">总结</h2>
<p>clichouse 和 Kafka的配合可以说是十分的便利，只有配置好，clickhouse 从 kafka 读数据和写入都是如此的方便。不过还是有相当的局限性，因为目前对 kafka 数据格式的支持还是有限。如果能通过插件之类的扩展方式自定义format就好了。另外，clickhouse 是否保证数据的一致性，如何去重？如何保证数据不丢失？也可以通过与flink结合，从而保证数据的一致性。详细的情况还需要进一步测试</p>
<h2 id="参考资料">参考资料</h2>
<p>https://clickhouse.yandex/tutorial.html<br>
https://clickhouse.yandex/docs/en/table_engines/kafka.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[clickhouse基本操作]]></title>
        <id>http://lvelvis.github.io/post/clickhouse-ji-ben-cao-zuo/</id>
        <link href="http://lvelvis.github.io/post/clickhouse-ji-ben-cao-zuo/">
        </link>
        <updated>2020-10-27T04:09:31.000Z</updated>
        <content type="html"><![CDATA[<h2 id="一-先来说一下clickhouse为啥快">一、先来说一下，ClickHouse为啥快</h2>
<figure data-type="image" tabindex="1"><img src="http://lvelvis.github.io/post-images/1603856102146.gif" alt="" loading="lazy"></figure>
<pre><code>ClickHouse有多少CPU，吃多少资源，所以飞快；
ClickHouse不支持事务，不存在隔离级别。这里要额外说一下，有人觉得，你一个数据库都不支持事务，不支持ACID还玩个毛。ClickHouse的定位是分析性数据库，而不是严格的关系型数据库。又有人要问了，数据都不一致，统计个毛。举个例子，汽车的油表是100%准确么？为了获得一个100%准确的值，难道每次测量你都要停车检查么？统计数据的意义在于用大量的数据看规律，看趋势，而不是100%准确。
IO方面，MySQL是行存储，ClickHouse是列存储，后者在count()这类操作天然有优势，同时，在IO方面，MySQL需要大量随机IO，ClickHouse基本是顺序IO。
有人可能觉得上面的数据导入的时候，数据肯定缓存在内存里了，这个的确，但是ClickHouse基本上是顺序IO，用过就知道了，对IO基本没有太高要求，当然，磁盘越快，上层处理越快，但是99%的情况是，CPU先跑满了（数据库里太少见了，大多数都是IO不够用）
</code></pre>
<h2 id="二-创建库">二、创建库</h2>
<p>CREATE/ATTACH DATABASE zabbix ENGINE = Ordinary;<br>
ATTACH 也可以建库，但是metadata目录下不会生成.sql文件，一般用于metadata元数据sql文件被删除后，恢复库表结构使用</p>
<h2 id="三-创建本地表">三、创建本地表</h2>
<p>CREATE TABLE test02( id UInt16,col1 String,col2 String,create_date date ) ENGINE = MergeTree(create_date, (id), 8192);<br>
ENGINE：是表的引擎类型，<br>
MergeTree：最常用的，MergeTree要求有一个日期字段，还有主键。<br>
Log引擎没有这个限制，也是比较常用。<br>
ReplicatedMergeTree：MergeTree的分支，表复制引擎。<br>
Distributed：分布式引擎。<br>
create_date：是表的日期字段，一个表必须要有一个日期字段。<br>
id：是表的主键，主键可以有多个字段，每个字段用逗号分隔。<br>
8192：是索引粒度，用默认值8192即可。</p>
<h2 id="四-创建分布式表">四、创建分布式表</h2>
<p>CREATE TABLE distributed_table AS table ENGINE = Distributed(cluster, db, table, rand());<br>
cluster：配置文件中的群集名称。<br>
db：库名。<br>
table：本地表名。<br>
rand()：分片方式：随机。<br>
intHash64():分片方式：指定字段做hash。<br>
Distribute引擎会选择每个分发到的Shard中的”健康的”副本执行SQL<br>
<img src="http://lvelvis.github.io/post-images/1603771933311.jpg" alt="" loading="lazy"></p>
<h2 id="五-ddl">五、DDL</h2>
<p>如果想按集群操作，需要借助zookeeper，在config.xml中添加配置<br>
&lt;distributed_ddl&gt;<br>
<path>/clickhouse/task_queue/ddl</path><br>
&lt;/distributed_ddl&gt;<br>
一个节点创建表，会同步到各个节点<br>
CREATE TABLE db.table [ON CLUSTER cluster] (...)<br>
添加、删除、修改列<br>
ALTER TABLE [db].table [ON CLUSTER cluster] ADD|DROP|MODIFY COLUMN ...<br>
rename 支持*MergeTree和Distributed<br>
rename table db.table1 to db.table2 [ON CLUSTER cluster]<br>
truncate table db.table;不支持Distributed引擎</p>
<h2 id="六-deleteupdate-不支持distributed引擎">六、delete/update 不支持Distributed引擎</h2>
<p>ALTER TABLE [db.]table DELETE WHERE filter_expr...<br>
ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE ...</p>
<h2 id="七-分区表">七、分区表</h2>
<p>按时间分区：<br>
toYYYYMM(EventDate)：按月分区<br>
toMonday(EventDate)：按周分区<br>
toDate(EventDate)：按天分区<br>
按指定列分区：<br>
PARTITION BY cloumn_name<br>
对分区的操作：<br>
alter table test1 DROP PARTITION [partition] #删除分区<br>
alter table test1 DETACH PARTITION [partition]#下线分区<br>
alter table test1 ATTACH PARTITION [partition]#恢复分区<br>
alter table .test1 FREEZE PARTITION [partition]#备份分区</p>
<h2 id="八-数据同步">八、数据同步</h2>
<ol>
<li>采用remote函数<br>
insert into db.table select * from remote('目标IP',db.table,'user','passwd')</li>
<li>csv文件导入clickhouse<br>
cat test.csv | clickhouse-client -u user --password password --query=&quot;INSERT INTO db.table FORMAT CSV&quot;</li>
<li>同步mysql库中表<br>
CREATE TABLE tmp ENGINE = MergeTree ORDER BY id AS SELECT * FROM mysql('hostip:3306', 'db', 'table', 'user', 'passwd') ;<br>
4） clickhouse-copier 工具</li>
</ol>
<h2 id="九-时间戳转换">九、时间戳转换</h2>
<p>select toUnixTimestamp('2018-11-25 00:00:02');<br>
select toDateTime(1543075202);</p>
<h2 id="十-其他事项">十、其他事项</h2>
<ol>
<li>clickhouse的cluster环境中，每台server的地位是等价的，即不存在master-slave之说，是multi-master模式。</li>
<li>各replicated表的宿主server上要在hosts里配置其他replicated表宿主server的ip和hostname的映射。</li>
<li>上面描述的在不同的server上建立全新的replicated模式的表，如果在某台server上已经存在一张replicated表，并且表中已经有数据，这时在另外的server上执行完replicated建表语句后，已有数据会自动同步到其他server上面。</li>
<li>如果zookeeper挂掉，replicated表会切换成read-only模式，不再进行数据同步，系统会周期性的尝试与zk重新建立连接。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka消费命令]]></title>
        <id>http://lvelvis.github.io/post/kafka-xiao-fei-ming-ling/</id>
        <link href="http://lvelvis.github.io/post/kafka-xiao-fei-ming-ling/">
        </link>
        <updated>2020-09-28T08:12:09.000Z</updated>
        <content type="html"><![CDATA[<p>模拟生产消息：</p>
<pre><code>bin/kafka-console-producer.sh --broker-list 192.168.101.100:9092 --topic flink-test
</code></pre>
<p>模拟消费数据(从开始位置消费)</p>
<pre><code>bin/kafka-console-consumer.sh --bootstrap-server  192.168.101.100:9092  --topic flink-test --from-beginning  |head
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[logstash output file to HDFS]]></title>
        <id>http://lvelvis.github.io/post/logstash-output-file-to-hdfs/</id>
        <link href="http://lvelvis.github.io/post/logstash-output-file-to-hdfs/">
        </link>
        <updated>2020-09-04T02:45:33.000Z</updated>
        <content type="html"><![CDATA[<p>logstash 直接把文件内容写入 hdfs 中， 并支持 hdfs 压缩格式。<br>
logstash 需要安装第三方插件，webhdfs插件，通过hdfs的web接口写入。<br>
即 http://namenode00:50070/webhdfs/v1/ 接口</p>
<p>新版本logstash已默认安装webhdfs插件<br>
官网地址及使用说明：<br>
https://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html<br>
检查hdfs的webhds接口</p>
<pre><code>curl -i  &quot;http://namenode:50070/webhdfs/v1/?user.name=hadoop&amp;op=LISTSTATUS&quot;   
HTTP/1.1 200 OK
Cache-Control: no-cache
Expires: Thu, 13 Jul 2017 04:53:39 GMT
Date: Thu, 13 Jul 2017 04:53:39 GMT
Pragma: no-cache
Expires: Thu, 13 Jul 2017 04:53:39 GMT
Date: Thu, 13 Jul 2017 04:53:39 GMT
Pragma: no-cache
Content-Type: application/json
Set-Cookie: hadoop.auth=&quot;u=hadoop&amp;p=hadoop&amp;t=simple&amp;e=1499957619679&amp;s=KSxdSAtjXAllhn73vh1MAurG9Bk=&quot;; Path=/; Expires=Thu, 13-Jul-2017 14:53:39 GMT; HttpOnly
Transfer-Encoding: chunked
Server: Jetty(6.1.26)
注释： active namenode 返回是200 ，standby namenode 返回是403.
</code></pre>
<p>测试hdfs是否正常通讯：</p>
<pre><code>#通过webhdfs接口创建test.conf
curl -i -X PUT &quot;http://hadoop-master:50070/webhdfs/v1/data/test.conf?user.name=hdfs&amp;op=CREATE&quot;
curl -i -T test.conf &quot;http://hadoop-slave1:50075/webhdfs/v1/data/test.conf?op=CREATE&amp;user.name=hdfs&amp;namenoderpcaddress=hadoop-master:9000&amp;overwrite=false&quot;
</code></pre>
<p>配置<br>
添加 logstash 一个配置文件</p>
<p>vim /home/mtime/logstash-2.3.1/conf/hdfs.conf</p>
<pre><code>input {
    kafka {
        bootstrap_servers =&gt; &quot;192.168.101.22:9092,192.168.101.23:9092,192.168.101.93:9092&quot;
        topics =&gt; &quot;test-logs&quot;
        group_id =&gt; &quot;hdfs-test-logs&quot;
        codec =&gt; json
	    consumer_threads =&gt; 15

    }
}

filter {
    date {
        match =&gt; [&quot;time&quot;,&quot;yyyy-MM-dd HH:mm:ss Z&quot;]
        target =&gt; &quot;@timestamp&quot;
        timezone =&gt; &quot;Asia/Shanghai&quot;
    }
    ruby {
        code =&gt; &quot;event.set('index.date', event.get('@timestamp').time.localtime.strftime('%Y%m%d'))&quot;
    }
    ruby {
        code =&gt; &quot;event.set('index.hour', event.get('@timestamp').time.localtime.strftime('%H'))&quot;
    }
}
output {            
    webhdfs {
           host =&gt; &quot;hadoop-master&quot;
           port =&gt; 50070
           path =&gt; &quot;/data/pt-collect-log/test-logs/%{index.date}/application-%{index.hour}.log&quot;
           user =&gt; &quot;hdfs&quot;
	   codec =&gt; line { format =&gt; &quot;%{message}&quot;}
           flush_size =&gt; 1000
           compression =&gt; &quot;gzip&quot;            
           idle_flush_time =&gt; 10
           retry_interval =&gt; 3
	   retry_times =&gt; 100
       }
}
</code></pre>
<p>关于hdfs部分配置，可以在 plugins-outputs-webhdfs 官网找到<br>
启动 logstart<br>
cd /home/mtime/logstash-2.3.1/bin/<br>
./logstash -f ../conf/hdfs.conf    # 为前台启动<br>
报错处理</p>
<pre><code>[WARN ][logstash.outputs.webhdfs ] Failed to flush outgoing items {:outgoing_count=&gt;160, :exception=&gt;&quot;WebHDFS::IOError&quot;,
我将hdfs端口 由原来的50070 改为 14000 端口，就在不报错了。
官方提供的例子中用的就是50070端口，一直没有尝试14000端口。

还有：
because this file lease is currently owned by DFSClient
hadoop 租约问题，后期正常就没有了。
执行recoverLease来释放文件的锁

$ hdfs debug recoverLease -path /logstash/2017/02/10/go-03.log
还有：
:message=&gt;&quot;webhdfs write caused an exception: {\&quot;RemoteException\&quot;:{\&quot;message\&quot;:\&quot;Failed to APPEND_FILE
当一个进程在读写这个文件的时候，另一个进程应该是不能同时写入的。
我们由原来3个logstash同时消费，改为了1个logstash消费，不在报错了。
这个应该也可以通过有话写入hdfs参数来解决。

还有：
Max write retries reached. Exception: initialize: name or service not known {:level=&gt;:error}
losgstash 需要能解析所有 hadoop 集群所有节点的主机名。
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ceph报错管理]]></title>
        <id>http://lvelvis.github.io/post/ceph-bao-cuo-guan-li/</id>
        <link href="http://lvelvis.github.io/post/ceph-bao-cuo-guan-li/">
        </link>
        <updated>2020-08-25T10:12:33.000Z</updated>
        <content type="html"><![CDATA[<p>使用ceph -s查看集群状态，发现一直有如下报错，且数量一直在增加</p>
<pre><code>daemons have recently crashed
</code></pre>
<p>经查当前系统运行状态正常，判断这里显示的应该是历史故障，处理方式如下：</p>
<p>查看历史crash</p>
<pre><code>ceph crash ls-new
</code></pre>
<p>根据ls出来的id查看详细信息</p>
<pre><code>ceph crash info &lt;crash-id&gt;
</code></pre>
<p>将历史crash信息进行归档，即不再显示</p>
<pre><code>ceph crash archive &lt;crash-id&gt;

</code></pre>
<p>归档所有信息</p>
<pre><code>ceph crash archive-all
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[golang笔记-string、int、int64互相转换]]></title>
        <id>http://lvelvis.github.io/post/golang-bi-ji-stringintint64-hu-xiang-zhuan-huan/</id>
        <link href="http://lvelvis.github.io/post/golang-bi-ji-stringintint64-hu-xiang-zhuan-huan/">
        </link>
        <updated>2020-08-10T08:35:04.000Z</updated>
        <content type="html"><![CDATA[<pre><code>#string到int  
int,err:=strconv.Atoi(string)  
#string到int64  
int64, err := strconv.ParseInt(string, 10, 64)  
#int到string  
string:=strconv.Itoa(int)  
#int64到string  
string:=strconv.FormatInt(int64,10)  ```

同类型之间转换，比如int64到int，直接int(int64)即可；
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[golang笔记-go-restful]]></title>
        <id>http://lvelvis.github.io/post/golang-bi-ji-go-restful/</id>
        <link href="http://lvelvis.github.io/post/golang-bi-ji-go-restful/">
        </link>
        <updated>2020-06-22T06:45:14.000Z</updated>
        <content type="html"><![CDATA[<p>用golang写一个restful api。如果您不知道什么是restful,可以看<a href="http://www.ruanyifeng.com/blog/2014/05/restful_api.html">阮一峰老师的教程</a></p>
<p>首先，我们需要解决的是路由的问题，也就是如何将不同的url映射到不同的处理函数。</p>
<pre><code>    router.GET(&quot;/api/todo/:todoid&quot;, getTodoById)
    router.POST(&quot;/api/todo/&quot;, addTodo)
    router.DELETE(&quot;/api/todo/:todoid&quot;, deleteTodo)
    router.PUT(&quot;/api/todo/:todoid&quot;, modifyTodo)
</code></pre>
<p>作为一个初学者，我马上打开github,找到了<a href="https://github.com/avelino/awesome-go">awesome-go</a>,经过一番调研，我感觉有几个http router的库比较适合：bone, httprouter, mux</p>
<h2 id="基本框架">基本框架</h2>
<p>首先，我们设计了四个路由，分别为根据Id获得todo，增加todo，修改todo，删除todo。这里关于解析路由参数，我们使用了httprouter.Params的ByName函数。</p>
<pre><code>package main

import (
  &quot;fmt&quot;
  &quot;github.com/julienschmidt/httprouter&quot;
  &quot;net/http&quot;
  &quot;log&quot;
  &quot;io&quot;
  &quot;io/ioutil&quot;
)

func getTodoById(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  fmt.Fprintf(w, &quot;getTodo %s\n&quot;, todoid)
}

func addTodo(w http.ResponseWriter, r *http.Request, _ httprouter.Params){
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  fmt.Fprintf(w, &quot;addTodo! %s\n&quot;,body)
}

func deleteTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  fmt.Fprintf(w, &quot;deleteTodo %s\n&quot;, todoid)
}

func modifyTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  fmt.Fprintf(w, &quot;modifyTodo %s to %s\n&quot;, todoid, body)
}

func main() {
    router := httprouter.New()
    router.GET(&quot;/api/todo/:todoid&quot;, getTodoById)
    router.POST(&quot;/api/todo/&quot;, addTodo)
    router.DELETE(&quot;/api/todo/:todoid&quot;, deleteTodo)
    router.PUT(&quot;/api/todo/:todoid&quot;, modifyTodo)
    log.Fatal(http.ListenAndServe(&quot;:8080&quot;, router))
}
</code></pre>
<p>我们可以用curl来测试一下我们的api,以put为例</p>
<pre><code>curl --data &quot;content=shopping&amp;time=tomorrow&quot; http://127.0.0.1:8080/api/todo/123 -X PUT

// modifyTodo 123 to content=shopping&amp;time=tomorrow
</code></pre>
<h2 id="json的解析">json的解析</h2>
<p>我们在使用restful api的时候，常常需要给后台传递数据。从上面可以看到，我们通过http.Request的Body属性可以获得数据</p>
<pre><code>body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
</code></pre>
<p>从上面，我们读出的数据是[]byte，但是我们希望将其解析为对象，那么在这之前，我们需要先定义我们的struct。假设我们的todo只有一个字段，就是Name</p>
<pre><code>type Todo struct {
    Name      string
}
</code></pre>
<p>现在我们可以这样解析</p>
<pre><code>var todo Todo;
json.Unmarshal(body, &amp;todo);
</code></pre>
<h2 id="model层设计">model层设计</h2>
<pre><code>package main

import (
  &quot;gopkg.in/mgo.v2&quot;
  &quot;fmt&quot;
  &quot;log&quot;
  &quot;gopkg.in/mgo.v2/bson&quot;
)

var session *mgo.Session

func init(){
  session,_ = mgo.Dial(&quot;mongodb://127.0.0.1&quot;)
}

type Todo struct {
    Name      string
}

func createTodo(t Todo){
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  c.Insert(&amp;t)
}

func queryTodoById(id string){
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  result := Todo{}

  err := c.Find(bson.M{&quot;_id&quot;: bson.ObjectIdHex(id)}).One(&amp;result)
  if err != nil {
    log.Fatal(err)
  }

  fmt.Println(&quot;Todo:&quot;, result.Name)
}

func removeTodo(id string){
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  err := c.Remove(bson.M{&quot;_id&quot;: bson.ObjectIdHex(id)})
  if err != nil{
    log.Fatal(err)
  }
}

func updateTodo(id string, update interface{}){
  //change := bson.M{&quot;$set&quot;: bson.M{&quot;name&quot;: &quot;hahaha&quot;}}
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  err := c.Update(bson.M{&quot;_id&quot;: bson.ObjectIdHex(id)}, update)
  if err != nil{
    log.Fatal(err)
  }
}
</code></pre>
<p>我们定义了Todo的struct,并添加了几种函数。</p>
<pre><code>package main

import (
  &quot;fmt&quot;
  &quot;github.com/julienschmidt/httprouter&quot;
  &quot;net/http&quot;
  &quot;log&quot;
  &quot;io&quot;
  &quot;io/ioutil&quot;
  &quot;encoding/json&quot;
  &quot;gopkg.in/mgo.v2/bson&quot;
)

func getTodoById(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  queryTodoById(todoid)
  fmt.Fprintf(w, &quot;getUser %s\n&quot;, todoid)
}

func addTodo(w http.ResponseWriter, r *http.Request, _ httprouter.Params){
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  var todo Todo;
  json.Unmarshal(body, &amp;todo);
  createTodo(todo)
  fmt.Fprintf(w, &quot;addUser! %s\n&quot;,body)
}

func deleteTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  removeTodo(todoid)
  fmt.Fprintf(w, &quot;deleteUser %s\n&quot;, todoid)
}

func modifyTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  var todo Todo
  json.Unmarshal(body, &amp;todo);
  change := bson.M{&quot;$set&quot;: bson.M{&quot;name&quot;: todo.Name}}
  updateTodo(todoid,change)
  fmt.Fprintf(w, &quot;modifyUser %s to %s\n&quot;, todoid, body)
}

func main() {
    router := httprouter.New()
    router.GET(&quot;/api/todo/:todoid&quot;, getTodoById)
    router.POST(&quot;/api/todo/&quot;, addTodo)
    router.DELETE(&quot;/api/todo/:todoid&quot;, deleteTodo)
    router.PUT(&quot;/api/todo/:todoid&quot;, modifyTodo)
    log.Fatal(http.ListenAndServe(&quot;:8080&quot;, router))
}</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[jenkins x on kubernetes实践(支持多主)]]></title>
        <id>http://lvelvis.github.io/post/jenkins-x-on-kubernetes-shi-jian-zhi-chi-duo-zhu/</id>
        <link href="http://lvelvis.github.io/post/jenkins-x-on-kubernetes-shi-jian-zhi-chi-duo-zhu/">
        </link>
        <updated>2020-06-18T02:26:17.000Z</updated>
        <content type="html"><![CDATA[<h2 id="jenkins是什么">jenkins是什么？</h2>
<p>Jenkins是一个开源的持续集成工具，可用于自动化的执行与构建，测试和交付或部署软件有关的各种任务,有非常丰富的插件支持。</p>
<h2 id="kubernetes是什么">kubernetes是什么？</h2>
<p>Kubernetes是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。这个视频生动地介绍了k8s</p>
<h2 id="jenkins-on-k8s-有什么好处">jenkins on k8s 有什么好处？</h2>
<p>jenkins通过单Master多个Slave的方式提供服务，Master保存了任务的配置信息，安装的插件等等，而slave主要负责执行任务，在使用中存在以下几个问题：</p>
<ol>
<li>当存在多个slave时，运行slave的机器难以统一管理，每次添加新节点时总要做大量的重复工作。</li>
<li>由于不同业务的构建频率并不相同，在使用会发现有很多slave大多数时间都处于空闲状态，造成资源浪费</li>
<li>jenkins默认采取保守的调度方式，造成某些slave的负载过高，任务不能平均分配</li>
</ol>
<h2 id="jenkins架构">jenkins架构</h2>
<p><img src="http://lvelvis.github.io/post-images/1592447693022.png" alt="" loading="lazy"><br>
使用k8s管理jenkins具有以下优势：</p>
<ol>
<li>使用docker运行jenkins保证环境的一致性，可以根据不同业务选择合适的镜像</li>
<li>k8s对抽象后的资源（pods）进行统一的管理调度，提供资源隔离和共享，使机器计算资源变得弹性可扩展,避免资源浪费。</li>
<li>k8s提供容器的自愈功能，能够保证始终有一定数量的容器是可用的</li>
<li>k8s默认的调度器提供了针对节点当前资源分配容器的调度策略，调度器支持插件化部署便于自定义。</li>
</ol>
<h2 id="一搭建环境">一，搭建环境</h2>
<h3 id="工具准备">工具准备</h3>
<pre><code>kubernetes v1.8.4
docker v1.12.6
jenkins master镜像 jenkins/jenkins:lts（v2.73.3）
slave镜像 jenkinsci/jnlp-slave
Kubernetes plugin (v1.1)
</code></pre>
<h3 id="安装kubernetes集群">安装kubernetes集群</h3>
<p>中文教程：https://www.kubernetes.org.cn/2906.html<br>
省略.....</p>
<h2 id="二创建statefulset">二，创建StatefulSet</h2>
<p>StatefulSet(有状态副本集)：Deployments适用于运行无状态应用，StatefulSet则为有状态的应用提供了支持，可以为应用提供有序的部署和扩展，稳定的持久化存储，我们使用SS来运行jenkins master。</p>
<p>创建完整的Stateful Set需要依次创建一下对象：<br>
1、Persistent Volume<br>
2、Persistent Volume Claim<br>
3、StatefulSet<br>
4、Service</p>
<p>创建PersistentVolume：<br>
为了保存应用运行时的数据需要先创建k8s的卷文件，K8s中存在Volume和PersistentVolume两种类型：</p>
<ol>
<li>Volume：与docker中的volume不同在于Volume生命周期是和pod绑定的，与pod中的container无关。k8s为Volume提供了多种类型文件系统（cephfs,nfs…,简单起见我直接选择了hostPath，使用的node节点本地的存储系统）</li>
<li>PersistentVolume:从名字可以看出来，PV的生命周期独立于使用它的pod，不会像volume随pod消失而消失，而是做为一个集群中的资源存在（像node节点一样），同时PV屏蔽了使用具体存储系统的细节。<br>
k8s中的对象都是通过yaml文件来定义的，首先创建名为jenkins-volume.yml的文件:</li>
</ol>
<p>❣️❣️注意：PV的创建有静态，动态两种方式，动态创建可以减少管理员的操作步骤，需要提供指定的StorageClass。为了测试方便，所以我们直接选择静态创建，manual是一个不存在的storage class</p>
<pre><code>kind: PersistentVolume
apiVersion: v1
metadata:
  name: jenkins-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/tmp/data&quot;
</code></pre>
<p>master节点执行下面的命令，PV就手动创建完了</p>
<pre><code>kubectl create -f jenkins-volume1.yaml
</code></pre>
<p>创建PersistentVolumeClaim：</p>
<pre><code>PersistentVolumeClaim(PVC):
持久化存储卷索取，如果说PV是集群中的资源，PVC就是资源的消费者，PVC可以指定需要的资源大小和访问方式,pod不会和PV直接接触，而是通过PVC来请求资源，PV的生成阶段叫做provision,生成PV后会绑定到PVC对象，然后才能被其他对象使用。
</code></pre>
<p>PV和PVC的生命周期如下图：<br>
<img src="http://lvelvis.github.io/post-images/1592447954632.png" alt="" loading="lazy">pv life<br>
创建文件jenkins-claim.yaml<br>
注意： name必须为jenkins-home-jenkins-0否则会绑定失败</p>
<pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: jenkins-home-jenkins-0
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
</code></pre>
<p>执行命令kubectl create -f jenkins-claim.yaml<br>
然后查看PVC是否创建成功，status为bound说明PVC已经绑定</p>
<pre><code>[root@master ~]# kubectl describe pvc jenkins-home-jenkins-0
Name:          jenkins-home-jenkins-0
Namespace:     kubernetes-plugin
StorageClass:  manual
Status:        Bound
Volume:        jenkins-volume
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed=yes
               pv.kubernetes.io/bound-by-controller=yes
Capacity:      10Gi
Access Modes:  RWO
Events:        &lt;none&gt;
</code></pre>
<p>创建StatefulSet和Service：<br>
从kubernetes-plugin github仓库下载jenkins.yml文件</p>
<pre><code>wget https://raw.githubusercontent.com/jenkinsci/kubernetes-plugin/master/src/main/kubernetes/jenkins.yml
修改jenkins.yml：
去掉87行externalTrafficPolicy: Local（这是GKE使用的）
修改83行type: LoadBalancer改为type: NodePort
</code></pre>
<p>注意：<br>
service type=ClusterIP时只允许从集群内部访问， type设置为NodePort是为了从集群外的机器访问jenkins,请谨慎使用，开启NodePort会在所有节点（含master）的统一固定端口开放服务。</p>
<p>执行命令</p>
<pre><code>[root@master ~]# kubectl create -f jenkins.yml 
statefulset &quot;jenkins&quot; created
service &quot;jenkins&quot; created
访问jenkins master,地址为masterip:32058
</code></pre>
<p>#查看映射的端口</p>
<pre><code>[root@master ~]# kubectl get service jenkins
NAME      TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)                        AGE
jenkins   NodePort   10.96.82.68   &lt;none&gt;        80:32058/TCP,50000:30345/TCP   1m

</code></pre>
<p>查看pod : jenkins-0的容器日志，粘贴下面的密码进入jenkins,jenkins安装完成。</p>
<pre><code>Jenkins initial setup is required. An admin user has been created and a password generated.
Please use the following password to proceed to installation:
70aa7b41ba894855abccd09306625b8a
</code></pre>
<h3 id="问题分析">问题分析</h3>
<p>1.创建stateful set时失败，提示”PersistentVolumeClaim is not bound: “jenkins-home-jenkins-0”：”<br>
因为采用静态创建PV时，StatefulSet会按照固定名称查找PVC，PVC的名字要满足</p>
<p>PVC_name == volumeClaimTemplates_name + “-“ + pod_name</p>
<p>这里的名字就是jenkins-home-jenkins-0</p>
<p>2.pod启动失败，jenkins用户没有目录权限<br>
错误提示”touch: cannot touch ‘/var/jenkins_home/copy_reference_file.log’: Permission denied<br>
Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions?”<br>
要确保节点目录开放权限,在node上执行命令：</p>
<pre><code>sudo chown -R 1000:1000 /var/jenkins_home/
sudo chown -R 1000:1000 /tmp/data
##如果仍然失败，尝试在node上重启docker
systemctl restart docker
</code></pre>
<p>注意pv指定的hostPath权限也要修改，否则是无效的</p>
<h2 id="三-配置jenkins">三 ，配置jenkins</h2>
<p>创建jenkins服务账号</p>
<pre><code>wget https://raw.githubusercontent.com/jenkinsci/kubernetes-plugin/master/src/main/kubernetes/service-account.yml
kubectl create -f service-account.yml
</code></pre>
<p>配置插件<br>
访问http://masterip:32058/pluginManager/,搜索插件Kubernetes plugin安装；<br>
访问 http://masterip:32058/configure<br>
选择新建云–kubernetes,在URl填写api server地址，<br>
执行kubectl describe命令，复制output中的token，填入到 ‘Kubernetes server certificate key’</p>
<p>[root@master ~]# kubectl get secret<br>
NAME                  TYPE                                  DATA      AGE<br>
default-token-4kb54   kubernetes.io/service-account-token   3         1d<br>
jenkins-token-wzbsx   kubernetes.io/service-account-token   3         1d<br>
[root@master ~]# kubectl describe secret/jenkins-token-wzbsx<br>
...<br>
jenkins url,tunnel填写service的CLUSTER-IP即可，结果如图：<br>
<img src="http://lvelvis.github.io/post-images/1592448124617.png" alt="" loading="lazy">peizhi1<br>
选择add pod template，填写下面的内容，retain slave可以设置运行jenkins slave 的container空闲后能存活多久。<br>
<img src="http://lvelvis.github.io/post-images/1592448148171.png" alt="" loading="lazy">content<br>
插件配置完成。</p>
<h2 id="四-测试">四 ，测试</h2>
<ol>
<li>扩容测试<br>
StatefulSet扩容：<br>
首先需要手动创建PV，PVC(见第二步),然后执行扩容命令</li>
</ol>
<pre><code>kubectl scale statefulset/jenkins --replicas=２
</code></pre>
<p>查看StatefulSet,此时已经拥有两个master节点，访问service时会随机将请求发送给后端的master。</p>
<pre><code>[root@master ~]# kubectl get statefulset/jenkins 
NAME      DESIRED   CURRENT   AGE
jenkins   2         2         5d
</code></pre>
<p>虽然通过k8s可以轻松实现jenkins master节点的拓展，但是由于jenkins存储数据的方式通过本地文件存储，master之间的数据同步还是一个麻烦的问题，参考jenkins存储模型。</p>
<p>jenkins master上保存的文件：</p>
<pre><code>ls /temp/data
jenkins.CLI.xml
jenkins.install.InstallUtil.lastExecVersion
jenkins.install.UpgradeWizard.state
jenkins.model.ArtifactManagerConfiguration.xml
jenkins.model.JenkinsLocationConfiguration.xml
jobs
logs
nodeMonitors.xml
nodes
</code></pre>
<ol start="2">
<li>高可用测试<br>
现在stateful set中已经有两个pod,在jenkins-1所在的节点执行docker stop停止运行jenkins-master的容器，同时在命令行查看pod的状态，可以看到jenkins-1异常（Error状态）之后慢慢恢复了运行状态（Running）：</li>
</ol>
<pre><code>[root@master ~]# kubectl get pods -w
NAME        READY     STATUS    RESTARTS   AGE
jenkins-0   1/1       Running   0          1d
jenkins-1   0/1       Running   1         20h
jenkins-1   1/1       Running   1         20h
jenkins-1   0/1       Error     1         20h
jenkins-1   0/1       CrashLoopBackOff   1         20h
jenkins-1   0/1       Running   2         20h
jenkins-1   1/1       Running   2         20h
</code></pre>
<p>kubectl describe pod jenkins-1查看pod的事件日志，k8s通过探针(probe)接口检测到服务停止之后自动执行了拉取镜像，重启container的操作。</p>
<pre><code>Events:
  Type     Reason      Age                From                              Message
  ----     ------      ----               ----                              -------
  Warning  Unhealthy   27m (x2 over 27m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Readiness probe failed: HTTP probe failed with statuscode: 503
  Warning  Unhealthy   27m (x2 over 27m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Liveness probe failed: HTTP probe failed with statuscode: 503
  Warning  Unhealthy   24m                kubelet, iz8pscwd1fv6kprs1zw21kz  Readiness probe failed: Get http://192.168.24.4:8080/login: dial tcp 192.168.24.4:8080: getsockopt: connection refused
  Warning  BackOff     20m (x2 over 20m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Back-off restarting failed container
  Warning  FailedSync  20m (x2 over 20m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Error syncing pod
  Normal   Pulling     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  pulling image &quot;jenkins/jenkins:lts-alpine&quot;
  Normal   Started     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Started container
  Normal   Pulled      19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Successfully pulled image &quot;jenkins/jenkins:lts-alpine&quot;
  Normal   Created     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Created container
</code></pre>
<ol start="3">
<li>jenkins构建测试<br>
当前集群中使用的jenkins slave镜像只包含一个java运行环境来运行jenkins-slave.jar,在实际使用中需要自定义合适的镜像。选择自定义镜像之后需要修改插件的配置，同样name命名为jnlp替换默认镜像，arguments安装工具提示填写即可。<br>
<img src="http://lvelvis.github.io/post-images/1592448208433.png" alt="" loading="lazy"><br>
创建job，同时开始构建,k8s会在不同节点上创建pod来运行任务</li>
</ol>
<p>jenkins默认调度策略</p>
<ol>
<li>尝试在上次构建的节点上构建，指定某台slave之后会一直使用。</li>
<li>当队列有2个构建时，不会立刻创建两个executor,而是先创建一个executor然后尝试等待executor空闲，目的是保证每个executor被充分利用。<br>
k8s调度策略<br>
使用Pod.spec.nodeSelector根据label为pod选择node<br>
3 .调度器scheduler有Predicates，Priorities两个阶段，分别负责节点过滤和评分排序，各个阶段都有k8s提供的检查项，我们可以自由组合。<br>
（比如PodFitsResources检查cpu内存等资源，PodFitsHostPorts检查端口占用，SelectorSpreadPriority要求一个服务尽量分散分布）自定义schduler参考<br>
资源不足时会发生什么<br>
当前集群中有3个节点，我在node2运行一个CPU占用限制在80%的程序,然后设置jenkins插件ContainerTemplate的request和limit均为cpu 500m,内存500Mi,（500m代表单核CPU的50%）看一下pod会怎么调度<br>
k8s仍然尝试在node2分配节点（为什么其他节点不行），结果POD处于pending状态：</li>
</ol>
<pre><code>{
&quot;phase&quot;: &quot;Pending&quot;,
&quot;conditions&quot;: [
  {
    &quot;type&quot;: &quot;PodScheduled&quot;,
    &quot;status&quot;: &quot;False&quot;,
    &quot;lastProbeTime&quot;: null,
    &quot;lastTransitionTime&quot;: &quot;2017-12-09T08:29:10Z&quot;,
    &quot;reason&quot;: &quot;Unschedulable&quot;,
    &quot;message&quot;: &quot;No nodes are available that match all of the predicates: Insufficient cpu (4), PodToleratesNodeTaints (1).&quot;
  }
],
&quot;qosClass&quot;: &quot;Guaranteed&quot;
</code></pre>
<p>最后pod被删除，而jenkins任务会阻塞一直到有其他空闲的slave出现。</p>
<h2 id="五总结">五，总结</h2>
<p>本文介绍了在k8s集群部署jenkins服务的方式和k8s带来的资源管理便捷，由于我也是刚开始接触k8s,所用的实例只是搭建了用于测试的实验环境，离在实际生产环境中使用还有问题需要验证。</p>
]]></content>
    </entry>
</feed>